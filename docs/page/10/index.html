<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <link rel="canonical" href="http://yut.hatenablog.com" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.55.4" />

  <title>Y&#39;s note</title>

  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pure/1.0.0/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://yutakikuchi.github.io/blog/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://yutakikuchi.github.io/blog/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="https://yutakikuchi.github.io/blog/css/blackburn.css">

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">

  
  <link href="https://fonts.googleapis.com/css?family=Raleway" rel="stylesheet" type="text/css">

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

 
  
  <link rel="alternate" type="application/rss+xml" title="Y&#39;s note" href="https://yutakikuchi.github.io/blog/index.xml" />
  

  
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/androidstudio.min.css">
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
  
  <script>hljs.initHighlightingOnLoad();</script>
  

  <link rel="shortcut icon" href="https://yutakikuchi.github.io/blog/img/favicon.ico" type="image/x-icon" />

  
    
        <link rel="stylesheet" href="https://yutakikuchi.github.io/blog/css/my.css">
    
  
  
    
        <script src="https://yutakikuchi.github.io/blog/js/my.js"></script>
    
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="https://yutakikuchi.github.io/blog/">Y's note</a>


  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://yutakikuchi.github.io/blog/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://yutakikuchi.github.io/blog/post/"><i class='fa fa-list fa-fw'></i>Posts</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="https://yutakikuchi.github.io/blog/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://yutakikuchi.github.io/blog/index.xml"><i class="fa fa-rss fa-fw"></i>RSS</a>
    </li>
    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/yutakikuchi_" target="_blank"><i class="fa fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://facebook.com/yuta.kikuchi.007" target="_blank"><i class="fa fa-facebook-square fa-fw"></i>Facebook</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="http://slideshare.net/https://www.slideshare.net/yutakikuchi58/" target="_blank"><i class="fa fa-slideshare fa-fw"></i>SlideShare</a>
    </li>
    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/https://www.linkedin.com/in/%E4%BD%91%E5%A4%AA-%E8%8F%8A%E6%B1%A0-36291a44/" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
    </li>
    

    

    

    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://github.com/yutakikuchi" target="_blank"><i class="fa fa-github-square fa-fw"></i>GitHub</a>
    </li>
    

    

    

    

    

    

    

    

    

    

    

    

    

  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2019. All rights reserved.</small>
  </div>
  <div class="small-print">
    <small>Built with&nbsp;<a href="https://gohugo.io/" target="_blank">Hugo</a></small>
    <small>Theme&nbsp;<a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a></small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Y&#39;s note</h1>
  <h2></h2>
</div>

<div class="content">
  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201301070827/">10分でHadoop-Pigの基本文法を理解する</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2013 Jan 07, 08:27</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [Hadoop] : 10分でHadoop-Pigの基本文法を理解する Hadoop MapReduce デザインパターン ―MapReduceによる大規模テキストデータ処理
作者: Jimmy Lin,Chris Dyer,神林飛志,野村直之,玉川竜司出版社/メーカー: オライリージャパン発売日: 2011/10/01メディア: 大型本購入: 4人 クリック: 254回この商品を含むブログ (16件) を見る
はじめに  年末から使い続けているPigについて勉強した事をまとめていきます。主に以下のDocumentを参照しています。PigのDocumentでLatinを日本語で詳しく紹介しているものが見当たらなかったので、そういった目的でこの記事を参照されている方のお役に立てれば光栄です。
 Getting Started  Pig Latin Basics  PigTutorial - Apache Pig - Apache Software Foundation  Pig Latin の基本  Hadoop Oozie設定からPigのPythonUDFを利用するまでのまとめ - Yuta.Kikuchiの日記  PigでHadoopをより便利に使う！PigでのMapReduceまとめ - Yuta.Kikuchiの日記     Pig Tutorial  tutorial.tar.gzのdownloadとscriptの実行 tutorialのサンプルファイルが入っているtutorial.tar.gzをdownloadして解凍します。解凍したら作成されるディレクトリに移動します。試しにlocalmodeでscript1-local.pigを実行してみます。実行すると結果のディレクトリがlocalに作成されて中身を確認する事ができます。実行したサンプルはexecite-small.logという検索Queryログをtab区切りでparseして、Queryのngramを抽出します。抽出したngramの頻度に対してscoreを付けて、頻度の高いngramを出力しています。では以下では文法を確認して行きます。
$ wget -O tutorial.tar.gz "https://cwiki.apache.org/confluence/download/attachments/27822259/pigtutorial.tar.gz?version=1&modificationDate=1311188529000" $ tar xzf tutorial.tar.gz $ cd pigtmp $ ls -rw-r--r--.
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201301070827/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201301030718/">無駄無駄無駄無駄無駄ァ！ - The Lean StartUpから学ぶ無駄の無い起業プロセス - </a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2013 Jan 03, 07:18</time>
  </div>

  

  

  

</div>

  </header>

  <p>
   [開発手法] : 無駄無駄無駄無駄無駄ァ！ - The Lean StartUpから学ぶ無駄の無い起業プロセス - Running Lean ―実践リーンスタートアップ (THE LEAN SERIES)
作者: アッシュ・マウリャ,渡辺千賀,エリック・リース,角征典出版社/メーカー: オライリージャパン発売日: 2012/12/21メディア: 単行本（ソフトカバー）購入: 3人 クリック: 14回この商品を含むブログ (18件) を見る
The Lean StartUp  
すみません、タイトルは釣りです。正月で時間があったのでThe Lean StarUpを読みました。StartUpにおける構築-計測-学習のプロセスを短く回す事に寄って時間の無駄を無くす事を目的とした本です。量は360ページありますが分かりやすい話なので、1日あればスラスラ読む事が出来ると思います。私個人が凄く忘れやすい人間であり、本を毎回捲るのが面倒であるため要点を箇条書きでまとめておきます。
リーンスタートアップまとめ - NAVER まとめ 
  はじめに   スタートアップのほとんどが失敗する。 正しいやり方で進めるからこそ成功する。 IMVU開発ではユーザの意見を取り入れなかった。 顧客から望みを聞くわけではなく、顧客の望みを中心とする意思決定を科学的に行う。 リーンスタートアップの5原則  アントレプレナーはあらゆるところにいる 企業とはマネジメント 検証による学び 構築-計測-学習 革新会計     スタート   とにかくやってみようという方針では混乱を招くことが多い。 リーンスタートアップでは検証による学びを単位として進歩を計測する。 スタートアップの目標はできるかぎり早く、作るべきモノ(顧客が欲しがりお金を払ってくれるモノ)を突き止めること。 スタートアップに目的が存在する。  目的地 = vision visionを実現するためのstrategyを採用する strategyから生み出される成果物がproduct  visionはめったに変えないが、strategyとproductはチューニングを行う。 スタートアップとはとてつもなく不確実な状態で新しい製品やサービスを作り出さなければならない人的組織である。 大企業は製品を少しずつ改良し顧客を満足させる持続的イノベーションを得意とし、画期的な新製品を出す破壊的なイノベーションは不得意な場合がある。 企業の経営陣が従業員がイノベーションを進めることができるような環境を作らなければならない。    学び   検証による学びは進捗を的確に図るための方法。 無駄を発見し、それを体系的に無くしていく方法を学べばトヨタのようにリーンな企業となり、業界をリードする立場に立てる。 リーンにおける価値とは顧客にとってのメリットを指し、それ以外は全て無駄だと考える。 スタートアップの場合は顧客が誰か、顧客の価値が何かを見出すことができない。 顧客はこう望んでいるはずと自分の考えを正当化することは簡単。 実験から新しいことに気づきそれを元に当たらし実験をまた考える。 実験は最初の製品である。    舵取り   構築-計測-学習のトータルサイクルを最小にすることが重要。 実用最小限の製品を作り、革新会計による計測を行う。 トータルサイクルを終えたときに方向転換するか、当初戦略を維持するかの問いに直面し、仮説に一つでも誤りがある場合は新しい戦略的仮説に方向転換する必要がある。 類例と反例から答えが得られていない問いが明らかになる。 成功と失敗を分ける鍵は計画のうまく行っている部分と道を誤っている部分を見つける先見性と能力、ツールをアントレプレナーが持っていて、戦略を状況に順応させられるか。 持続的イノベーションの場合、現地・現物主義で顧客のニーズを確認できるがスタートアップの場合はどの仮説から検証すべきかぐらいしか分からない。 実用最小限の製品は事業仮説を検証するためのもの。 誰が顧客なのかが分からなければ、何が品質なのかも分からない。 顧客が気にするのは自分にとって良いか悪いか。 実用最小限の商品を作るときは求める学びに貢献しない機能やプロセス、労力は全て削除すること。 スタートアップは競合他社に直面するので、勝つためには他よりも速いスピードで学ぶしか道は無い。 スタートアップがやらなければならないこと  現状を的確に計測し評価で明らかになった厳しい現実を直視する 理想に現実の数字を近づける方法が学べる実権を考案する  革新会計は以下の3段階  実用最小限の製品を作成 ベースライン状態から理想状態へのエンジンチューニング 方向転換するか辛抱するか  独立した顧客グループの数値を比較するコホート分析の導入。 異なるバージョンの製品を同時に顧客に試してもらうスプリットテストの導入。 虚栄の評価基準は捨てて行動につながる評価基準を採用する。 虚栄の評価基準は人の弱さにつけ込む。数値が上がると行動が改善をもたらしたと考え、数値が下がると自分以外の誰かのせいにしたがる。 ヴォディズンは実用最小限の製品作成のスピードアップを図り、方向転換時にはレガシーな商品を捨てた。 スタートアップに残された時間は資金から資本年少率を出すことではなく、ピボットを後何回できるかで測る。 ピボットには勇気が必要。ピボットに遅れてしまうのは以下の3つの理由  虚栄の評価基準から偽の認識を導き出してしまう 仮説が曖昧だと完全な失敗が無くなり、完全な失敗が無いから根本的な見直しが無い 失敗を認めると士気が下がる  アントレプレナーは失敗する気概を持たないといけない。 スタートアップの場合は定期的に会議を開いて方向転換か辛抱かを検討すべき。 ピボットとは作ったものや学んだものを再利用してもっとも優れた方向を見つけること。 アーリーアダプタとメインストリームで求めるものが異なる。 ズームイン型ピボット  一機能と考えていたものがメインになる。  ズームアウト型ピボット  ズームインの逆で製品全体を一機能と考える。  顧客セグメント型ピボット  顧客層を変更する  顧客ニーズ型ピボット  顧客と自分たちが問題解決したいことを別に発見し、方向転換する  プラットフォーム型ピボット  アプリケーション-プラットフォーム間の方向転換  事業構造型ピボット  企業自体が高利益率・少量の複合システムか低利益率・大量の大量操業に分けられ、これらの事業構造を切り替えること  価値補足方ピボット  企業が生み出す価値を貨幣化や収益モデルとして捕らえる  成長エンジン型ピボット  スピードアップや利益向上を実現するため  チャネル型ピボット  ソリューションを他のチャネルで提供する  技術型ピボット  同じソリューションを他の技術で実現する     スピードアップ   一つ一つのプロセスに要する時間が全く同じ場合でもバッチサイズが小さいほうが効率的になる。 バッチサイズを小さくすれば構築-計画-学習のフィードバックループを競合他社より短いサイクルで回せる。 プッシュ方式をプル方式にする効果とバッチサイズを縮小する効果を持つ。 構築-計測-学習のトータルサイクルを計画するときは逆順で考える。 トヨタのすごいところは史上最高レベルの学ぶ組織を作り上げた点。 持続的な成長とは過去の顧客行動が新しい顧客を呼び込むこと。 過去の顧客が新しい顧客を呼び込む形式は以下の4つ  口コミ 利用効果 有料広告 リピート購入・利用  成長エンジンには3つ存在する。 粘着型成長エンジン  新規顧客の契約速度が解約速度を上まれば成長する  ウィルス型成長エンジン  顧客一人が何人の顧客を連れてくるかが係数となる  支出型成長エンジン  顧客の獲得に再投資できる金額の売り上げに占める割合が一致している場合は成長速度は同じ。 成長速度を上げたければ顧客あたりの売り上げを増やすか顧客の獲得コストを下げる。  3種類の全ての成長エンジンを並行してモデル化するのはややこしい作業。 時間のために品質を犠牲にしてはならない。 問題に対して「なぜ？」という疑問を5回ブレークダウンして真因を見つけて正すことができる。 5回のなぜを繰り返して5回の誰になって悪者を探してしまうのは良くない。 人間ではなくプロセスに問題があったことを見つける。 5回のなぜの学び促進には分野ごとに責任者を置くと良い。 スタートアップは守備範囲において自由に開発、マーケティングする権限が無くてはならない。 チームは全部門をカバーするメンバで構成する。 チームは小さくなくてはならない。 成果にはアントレプレナーの利害がかかっていなければならない。 組織メンバが自己防衛に走る環境ではイノベーションは生まれない。 イノベーションが自由に行えるsandboxを用意する。 新しい製品を開発した創造性のあるマネージャーが部門や資源の管理を行うケースが多く、新しいイノベーションが生み出しづらい。 社員一人一人の得意分野を理解して、製品の段階にあわせて担当者をバトンタッチする。 イノベーションサンドボックスはチームと親組織を守るための仕組み。 人間の時間を乱用するのは創造性と可能性の過失無駄罪。 定量的な目標を設定することではなく、目標を達成するための方法を整えること。    
  </p>

  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201212291432/">Hadoop Oozie設定からPigのPythonUDFを利用するまでのまとめ</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Dec 29, 14:32</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [Hadoop] : Hadoop Oozie設定からPigのPythonUDFを利用するまでのまとめ Hadoop 第2版
作者: Tom White,玉川竜司,兼田聖士出版社/メーカー: オライリージャパン発売日: 2011/07/23メディア: 大型本購入: 9人 クリック: 182回この商品を含むブログ (24件) を見る
Oozie  OozieとはHadoop MapReduceのジョブ管理システムの事でMapReduceの定期処理化や複数のMapReduceの実行し結果を一つにまとめるなど一連の処理フローとして定義することができる優れものです。Oozie自体はJava/Tomcatで作られているようです。Oozieを動かすために開発者は以下のものを用意しなければなりません。
   ファイル   必須   記述方式   用途   設置場所     MapReduceプログラム   必須  Java,Streaming,Pig/Hive等   MapReduce実行   HDFS     workflow   必須   xml   OozieJobの実行   HDFS     coordinator   定期化する場合必須  xml   Ooziejobの定期化   HDFS     properties   必須  設定ファイル   Ooziejobの実行パラメータファイル   Oozie client   Oozieを動かすためのパラメータファイル以外は全てHDFSのパスに設置する必要があります。workflowがMapReduceの一連処理の流れを定義し、coordinatorは定期処理をしたい場合に定義するxmlファイルです。propertiesはJobTracker,NameNodeやOozieのjobに渡すパラメータの設定ファイルです。
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201212291432/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201212170830/">PigでHadoopをより便利に使う！PigでのMapReduceまとめ</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Dec 17, 08:30</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [Hadoop] : PigでHadoopをより便利に使う！PigでのMapReduceまとめ Hadoop Hacks ―プロフェッショナルが使う実践テクニック
作者: 中野猛,山下真一,猿田浩輔,上新卓也,小林隆出版社/メーカー: オライリージャパン発売日: 2012/04/25メディア: 単行本（ソフトカバー）購入: 3人 クリック: 156回この商品を含むブログ (8件) を見る
Pig  HadoopのMapReduceを独自で記述するのは手間が掛かります。それらの手間を出来るだけ緩和させるための便利なツールとしてDSL形式の処理フローを定義する事でMapReduceを実行するHiveやPIgというものが存在します。HiveとPigはライバルブロジェクトのようで、本日紹介するPigはYahoo!が開発しているミドルウェアになります。Hiveについては以前簡単に紹介をしたので以下のリンクを参考にしてください。PigLatinという手続き型の文法でDataのload/filter/join/sort/group/join/limit/storeなどの処理を組み合わせ、inputからoutputまでの一連のMapReduceを定義する事ができます。JavaやStreamingでのMapReduceでは目的のデータを抽出するために多段MapReduce処理を記述する事もありますが、PigやHiveを使うと1回の実行で済んだりします。
Hadoopをより便利に使う！HiveでのMapReduceまとめ - Yuta.Kikuchiの日記 
  Pigの実行  Install CentosでInstallする内容については以前まとめたので、詳しくはそちらを参照してください。
CentOSでHadoopを使ってみる - Yuta.Kikuchiの日記 
必要なyum installは以下のものになります。
$ yum install hadoop-0.20 -y $ yum install hadoop-0.20-conf-pseudo -y $ yum install hadoop-pig -y  PigLatin Pig Latin Basics 
PigLatinとはPigの便利な文法のことです。ここでは代表的なLatinをいくつか紹介します。
   文法   役割       LOAD   データのファイルシステムから読み込み     STORE   データをファイルシステムに保存する     DUMP   結果をコンソールに出力する     FILTER   条件を指定しデータのフィルタリングを行う       MATCHES   データを正規表現の条件でフィルタリングする     FOREACH   繰り返しデータ変換を行う     SPLIT   データのを分割する     JOIN   データの結合を行う     GROUP   データのグループ化を行う     ORDER   データをソートする     DISTINCT   データの重複を排除する     LIMIT   データの出力件数を制限する     SAMPLE   データのサンプリングを行う     LocalModeで実行 Pigはlocalmodeとmapreducemodeをそれぞれ実行で切り分ける事が出来ます。localmodeはjobを投げるサーバの1台で、データのLoad/Storeもローカルサーバのものに対して行います。mapreducemodeはHDFS上のデータを利用し、全Hadoopクラスタに対して分散処理を行うモードになります。localで実行したい場合は-x localというオプションを起動時に設定します。pigコマンドを実行すると対話コンソールが出力されます。
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201212170830/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201211261210/">3ヶ月間Hadoopを使ってみて学んだ事</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Nov 26, 12:10</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [Hadoop] : 3ヶ月間Hadoopを使ってみて学んだ事 Hadoop 第2版
作者: Tom White,玉川竜司,兼田聖士出版社/メーカー: オライリージャパン発売日: 2011/07/23メディア: 大型本購入: 9人 クリック: 182回この商品を含むブログ (24件) を見る
Overture  BigData解析という仕事をやり始めて半年、Hadoopを業務で使い始めて3ヶ月以上が経過したのでここで今までの業務での知識をまとめてみたいと思います。先日参加したWebDBForum2012でも各種企業がBigData(主にログ)からユーザの趣味思考や特徴などを解析して表示システムへのFeedBackや企業戦略などに活かしている報告があり、Hadoopなどの分散処理技術や今後は更にリアルタイムでBigDataを使うためのミドルウェアが出てくることが予想され、そこに精通した人間が求められるようになってくると思います。
第5回 Webとデータベースに関するフォーラム (WebDB Forum 2012) 
  Hadoop  Hadoop - Wikipedia 
Hadoopの説明を簡単に。大量のデータを扱うために従来のスタンドアロンで処理を行うのではなく、大量のマシンに処理を分散させて解析スピードを劇的に改善することを目的としたフレームワークです。例えば200GほどのApacheのAccessLogからUserのCookie、Refererや利用しているUserAgent/Deviceなどをデータを1日毎に集計しようとした場合、1台スクリプトで計算していたら翌日の集計に間に合いません。しかしこれをHadoopの分散処理を使えば1-2時間で処理を完了させることが出来ると思います。
merit / demerit Hadoopを利用するmerit/demeritを箇条書きで書きます。
 merit  大量のデータを複数台で分散し、解析処理を高速化。 スケールアウトが比較的楽にできる。 面倒な分散処理は全てHadoopが行ってくれる。Hadoopユーザは解析処理に専念できる。 導入実績が豊富で信頼ができる。  JAVAが苦手なユーザでもStreamingを使う事でPerl/Python/Rubyなどの言語でMapReduceを記述する事が可能。    demerit  逐次処理を行う場合には不向き。 リアルタイム性を求める処理には不向き。 多段のMapReduce処理を重ねないと集計が出来ないケースがある。 MapReduceのテクニックを学ばなければならない。    ServerStructure Hadoopのクラスタはmaster/slaveの二つに分けられます。masterとなるのは1台のサーバでそこを起点にslaveノードに対してTask,Dataが割り当てられます。master/slaveのサーバの内部でもMapReduce処理を行うサーバ、HDFSのファイルを管理するサーバに分けられ、それぞれに対してJobTracker/TaskTracker/NameNode/DataNodeと名前が定義されています。
 JobTracker  HadoopClientからjobを受け取る。Slaveサーバに対してTaskを分割する。  TaskTracker  Masterサーバから受け取ったTaskを実行する。  NameNode  実際のデータがDataNodeのどこに格納されているのかを管理するメタデータサーバ。  DataNode  NameNodeによって管理されている実際のデータ。ブロックと呼ばれる単位で格納されている。ブロックはデフォルトで64MByte。ファイルは設定に応じてreplication管理されている。  以下は概要図となりますが、Clientから投げられたHadoopのJobをMasterのJobTrackerが受け付けてSlaveのTaskTrackerに流します。Clientから投げられたHadoopのJobのINPUT_PATHはNameNodeに問い合わせを行い、どのDataNodeにデータが格納されているのかを取得します。JobTrackerはTaskTrackerに対してDataNodeのファイルを指定し、TaskTrackerが実際のファイルを取りに行くような仕組みです。
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201211261210/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201211130837/">Mahoutを使ったNaiveBayesによる機械学習</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Nov 13, 08:37</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [機械学習] : Mahoutを使ったNaiveBayesによる機械学習 入門 ソーシャルデータ ―データマイニング、分析、可視化のテクニック
作者: Matthew A. Russell,奥野陽（監訳）,佐藤敏紀（監訳）,瀬戸口光宏（監訳）,原川浩一（監訳）,水野貴明（監訳）,長尾高弘出版社/メーカー: オライリージャパン発売日: 2011/11/26メディア: 大型本購入: 18人 クリック: 779回この商品を含むブログ (42件) を見る
BigDataでの機械学習  膨大なデータに対して機械学習を行いたい時にlocalの端末一台では処理の時間が掛かりすぎてしまいます。学習、モデル作成、予測のそれぞれの処理を高速で行うための一つのSolutionがHadoop上で機械学習をしてしまうことだと思います。Hadoop上で機械学習をするための便利なライブラリとしてJAVAベースのMahoutがあります。この記事ではMahoutによるNaiveBayes分類学習を中心としたMahoutデータの生成と使い方について紹介します。
Machine Learning With Hadoop - Yuta.Kikuchiの日記 
  NLTKによる分かち書き  NLTK Install Pythonの自然言語処理ライブラリのnltkを利用して分かち書きを行います。nltkのセットアップは非常に簡単で以下のコマンドを実行するだけです。Installing NLTK ― NLTK 2.0 documentation 
$ python -V Python 2.6.6 $ wget "http://pypi.python.org/packages/2.6/s/setuptools/setuptools-0.6c11-py2.6.egg#md5=bfa92100bd772d5a213eedd356d64086" $ sudo sh setuptools-0.6c11-py2.6.egg --prefix=/usr/ $ sudo easy_install pip $ sudo pip install -U numpy $ sudo pip install -U pyyaml nltk  Mecab Install 形態素解析器で有名なMecabをPythonから利用できるようにします。Mecab本体、Mecab-ipadic、mecab-pythonの3つをinstallします。mecab-pythonのinstallの時に"
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201211130837/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201210290836/">lookコマンドによる二分探索が速すぎて見えない</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Oct 29, 08:36</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [Linux] : lookコマンドによる二分探索が速すぎて見えない Linuxコマンドブック ビギナーズ 第2版 コマンドブックシリーズ
作者: 田谷文彦,三澤明出版社/メーカー: ソフトバンク クリエイティブ発売日: 2007/04/11メディア: 単行本 クリック: 3回この商品を含むブログ (2件) を見る
grep vs look  数GByte容量の圧縮ファイルから特定の文字列を検索したい場合があります。一度きりのgrep検索処理であればそれほど気にする事はありませんが、System処理で何度も検索をするようなケースでは処理に時間がかかってしまいます。今日はsortされたファイルに対してlookという2分探索コマンドを利用するとgrepより高速に検索が可能ということを調べたいと思います。
  lookコマンドの活用  lookは通常の場合辞書ファイルからスペルを確認するために利用されます。例えばmorpholoと先頭一致する単語一覧を取得したい場合は$ look morpholoと実行します。単語一覧の辞書データは/usr/share/dict/wordsに配置されています。
$ look morpholo morphologic morphological morphologically morphologies morphologist morphologists morphology morpholoical辞書ファイルを見るとわかりますがlookを使うには予めデータをsortしておく必要があります。これはlookの内部処理で2分探索を行っているためだと思われます。lookは上の辞書だけでなくgrepと同じように引数に指定したファイルから検索を行う事ができます。lookのusageは以下のようになっています。
look 使い方: look [-dfa] [-t キャラクタ] 文字列 [ファイル]   圧縮ファイルへの検索  検索手段 圧縮ファイルに対する文字列検索の手段で思い当たるものは以下のものです。以下のそれぞれを検証して行きます。
 zgrepで検索 zcatで出力 + grepで検索 ファイル解凍 + grepで検索 ファイル解凍 + sort + lookで検索   検索対象ファイル作成 手元に手頃なgzファイルが無かったので16進数の文字列を改行で出力するスクリプトで作成します。以下のスクリプトで生成されるログファイルは5.
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201210290836/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201210150838/">Machine Learning With Hadoop</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Oct 15, 08:38</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [機械学習] : Machine Learning With Hadoop Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series)
作者: Kevin P. Murphy出版社/メーカー: The MIT Press発売日: 2012/08/24メディア: ハードカバー購入: 1人 クリック: 26回この商品を含むブログを見る
Big DataのMachine Learning  Daily数百ギガバイトのAccessLogからDataMiningに必要なFeatureをかき集めるのにスタンドアロンの端末で処理を行うには時間が掛かりすぎます。バッチ処理で1日以内にUserのAccessLogを整形、必要な部分を取り出してDataMining/Machine Learningに掛けて、Userが利用するSystemに反映して行こうと考えると最初のバッチ処理で利用できる時間はそれほど多くありません。処理時間改善のためにHadoopを使い複数台のマシンに大量のログデータとバッチ処理を分散させる仕組みはBig Dataを扱う人の中では常識として利用されています。今日はBigDataをMachineLearningさせたいときの方法について調べた内容を載せます。Apache Mahout、PigのUDF、独自MapReduceの3つのうちどれかを使う事になりそうです。
Apache Mahout  Hadoop上で実行可能な機械学習ライブラリ。ファイルをDownloadして展開するだけで利用可能。一番お手軽だが未実装のAlgorithmもあり、公開されているPatchファイルを当てるなどの対応が必要になる場合がある。 Apache Mahout: Scalable machine learning and data mining  Algorithms   Classification Clustering Pattern Mining Regression Dimension reduction Evolutionary Algorithms Recommenders / Collaborative Filtering Vector Similarity    Pig  Pig専用のScriptを書く事でMap/Reduceが可能。データのjoinなどもできる。Hiveのライバルプロジェクト。MachineLearningを行う場合はuser-defined functions (UDFs)を書く必要がある。MachineLearningのUDFsはほとんどWeb上に公開されていない。TwitterはPigを使って機械学習している。 Welcome to Apache Pig!
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201210150838/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201210120820/">Pythonのscikit-learnでRandomForest vs SVMを比較してみた</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Oct 12, 08:20</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [機械学習] : Pythonのscikit-learnでRandomForest vs SVMを比較してみた Random Forest
メディア: ペーパーバック クリック: 27回この商品を含むブログ (1件) を見る
Random Forest  Random Forestとは  Random forest - Wikipedia  Random forests - classification description  機械学習の方法論の一つで決定木ベースの集団学習アルゴリズムを取り入れたものです。説明変数の依存が少ないことや学習が高速であることが特徴として挙げられています。英語サイトの方で特徴として紹介されているRFの内容について記述します。
 Features  大きなデータに対して効率よく処理される。 変数の削除をすることなく入力した数千の変数を扱う事ができる。 どの変数が分類に対して重要なのかを計算して与えてくれる。 木の構築処理中に一般的なエラーの偏りの無い計算を生成する。 高い割合でデータが誤っている時に誤りのデータを計算し、精度を保つ効果的な手法を持っている。 アンバランスなデータが与えられたクラス群の中でエラーのバランスに対する手法を持っている。 生成された木は今後他のデータに適用させるために保存する事ができる。 変数とクラスタリング間の関係性に関する情報を計算する。 クラスタリングに利用される隣接するケースを計算する。 上の特性はラベリングやクラスタリングされていないデータやはずれ値に対しても拡張する事ができる。 変数の相互作用を発見するための実験的な方法を推薦する。   Remarks  RFではoverfitは存在しない。 RFは複数の木として処理できるし、それは処理速度が速い。 5万のデータと100の変数を持ったデータに対して、100個の木に割り当て、800Mhzのマシンで11分で処理が終わる。 RFではCross-Validationをする必要がない。out-of-bag (oob) エラー計算がその代わりとなる。   テキストデータ分類器の比較  http://mjin.doshisha.ac.jp/R/200905_70.pdf このPFD-PaperによるとRFがマクロ平均のF値において最も精度が高いと言われていますが、どんなデータに対しても精度が高いとは言えないと思います。画像引用 : 図6 : 10のテーマにおけるマクロ平均のF1 値の平均プロット

 SVMについて SVMには以前に記事を書いたので以下を参照してください。
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201210120820/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  
    <article>
  <header>
    <h2><a href="https://yutakikuchi.github.io/blog/post/201210090830/">線形予測の機械学習ツールliblinearで効果最大化のための最適な定数Cを探る</a></h2>

    <div class="post-meta">

  <div>
    <i class="fa fa-calendar fa-fw"></i>
    <time>2012 Oct 09, 08:30</time>
  </div>

  

  

  

</div>

  </header>

  <p>
  [機械学習] : 線形予測の機械学習ツールliblinearで効果最大化のための最適な定数Cを探る Machine Learning for Hackers
作者: Drew Conway,John Myles White出版社/メーカー: Oreilly & Associates Inc発売日: 2012/02/28メディア: ペーパーバック クリック: 63回この商品を含むブログを見る
liblinear   LIBLINEAR -- A Library for Large Linear Classification  10秒で設定可能なlibsvmで機械学習を行う - Yuta.Kikuchiの日記  R言語でSVM(Support Vector Machine)による分類学習 - Yuta.Kikuchiの日記  今日はliblinearを用いた機会学習の話です。今まではSVMを利用するときはkernelオプション付きのR言語のSVM/libsvm/svm-lightを利用していましたが、学習データが多い時に計算時間が何時間も掛かる事に不便を感じていました。そこでSVMのツールについて色々と調べてみたところ、線形予測に特化したliblinearの存在を知りました。公式のDocumentにもlibsvmとliblinearでの線形予測での処理時間が桁違いにliblinearの方が優れていることが記述されています。以下にliblinearの特徴を記述します。
 liblinearはinstanceや特徴が100万桁のデータを線形分離するためのtoolであり以下をサポートしています。  L2正則化の分類  L2-loss linear SVM, L1-loss linear SVM, and logistic regression (LR)  L1正則化の分類  L2-loss linear SVM and logistic regression (LR)  Support Vechtor RegressionのL2正則化  L2-loss linear SVR and L1-loss linear SVR   正則化とはOverfittingを回避するために罰則項を与える事です。種類としてはL1,L2,L1L2の3つが良く利用されるもので精度とスパース性によって異なります。L1は精度が低くスパース性が高い、L2は精度が高くスパース性が低い、L1L2は両方を取り入れ精度を高く保ちながらスパース性を高くすることです。
  </p>

  
  <footer>
    <a href="https://yutakikuchi.github.io/blog/post/201210090830/">Read more<i class="fa fa-angle-double-right fa-fw"></i></a>
  </footer>
  
</article>

  

  


<nav class="pagination" role="pagination">
  
  <a href="https://yutakikuchi.github.io/blog/page/9/"><i class="fa fa-chevron-left"></i></a>
  
  <span>&nbsp;10 / 21&nbsp;</span>
  
  <a href="https://yutakikuchi.github.io/blog/page/11/"><i class="fa fa-chevron-right"></i></a>
  
</nav>



</div>

</div>
</div>
<script src="https://yutakikuchi.github.io/blog/js/ui.js"></script>
<script src="https://yutakikuchi.github.io/blog/js/menus.js"></script>


<script>
  
  if (window.location.hostname != "localhost") {
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-20616165-3', 'auto');
    ga('send', 'pageview');
  }
</script>





</body>
</html>

