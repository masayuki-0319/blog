<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Y&#39;s note</title>
    <link>https://yutakikuchi.github.io/</link>
    <description>Recent content on Y&#39;s note</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <copyright>&amp;copy; 2019. All rights reserved.</copyright>
    <lastBuildDate>Mon, 29 Apr 2019 21:03:52 +0000</lastBuildDate>
    
	<atom:link href="https://yutakikuchi.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Docker for Macのメモリ制限の調整</title>
      <link>https://yutakikuchi.github.io/post/201904292103/</link>
      <pubDate>Mon, 29 Apr 2019 21:03:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201904292103/</guid>
      <description>[etc] : Docker for Macのメモリ制限の調整  Get started with Docker Desktop for Mac | Docker Documentation
 Install Docker Desktop for Mac | Docker Documentation
  Memory: By default, Docker Desktop for Mac is set to use 2 GB runtime memory, allocated from the total available memory on your Mac. To increase RAM, set this to a higher number; to decrease it, lower the number.
 Docker for Macを使ってDocker runする際に --memory(-m) ではメモリの制限が指定できない。 Defaultの制限は2Gになっている。下記を実行しても10Gに反映されない</description>
    </item>
    
    <item>
      <title>Kerasでお試しCNN</title>
      <link>https://yutakikuchi.github.io/post/201809241706/</link>
      <pubDate>Mon, 24 Sep 2018 17:06:50 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201809241706/</guid>
      <description>[etc] : Kerasでお試しCNN ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装
作者: 斎藤康毅出版社/メーカー: オライリージャパン発売日: 2016/09/24メディア: 単行本（ソフトカバー）この商品を含むブログ (18件) を見る
30分でDeepLearningを実行できるようにお試しするキット。手っ取り早く始めるためにkeras(Tensorflow backend)をinstall。kerasについては下記のページで紹介されている。 尚、下にinstallのlogを残しているがkerasの前にBackendとなるTensorflowをinstallすると良い。
引用 : Kerasは，Pythonで書かれた，TensorFlowまたはCNTK，Theano上で実行可能な高水準のニューラルネットワークライブラリです． Kerasは，迅速な実験を可能にすることに重点を置いて開発されました． アイデアから結果に到達するまでのリードタイムをできるだけ小さくすることが，良い研究をするための鍵になります． Keras Documentation
$ sudo pip install keras Cannot uninstall &#39;six&#39;. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall. // sixを再度install $ sudo pip install keras --ignore-installed six Installing collected packages: six, numpy, h5py, keras-applications, scipy, keras-preprocessing, pyyaml, keras Running setup.</description>
    </item>
    
    <item>
      <title>tmux : powerlineの表示ズレを解消する</title>
      <link>https://yutakikuchi.github.io/post/201809180049/</link>
      <pubDate>Tue, 18 Sep 2018 00:49:43 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201809180049/</guid>
      <description> [etc] : tmux : powerlineの表示ズレを解消する 表示ズレの解消 
ref : tmux 2.5 以降において East Asian Ambiguous Character を全角文字の幅で表示する &amp;middot; GitHub file-tmux-2-7-fix-diff : https://gist.github.com/z80oolong/e65baf0d590f62fab8f4f7c358cbcc34#file-tmux-2-7-fix-diff
上図のようにtmuxのpowerline行がずっと増え続ける問題を解消する。対応方針としてはpatchを当てる。PC環境はMac、tmuxのversionは2.7を想定。patchは上記gistにversion毎にpatchが用意されている。brew edit tmuxコマンドで下記内容を追記し、brew reinstallにてpatchを適用し再度install。
// コマンドは下記を実行 $ tmux -V tmux 2.7 $ brew edit tmux // 下記を追記 def patches [ &#34;https://gist.githubusercontent.com/z80oolong/e65baf0d590f62fab8f4f7c358cbcc34/raw/d478a099aa5074e932e3323e9b16033e13919cdf/tmux-2.7-fix.diff&#34; ] end $ brew reinstall --build-from-source tmux == Summary 🍺 /usr/local/Cellar/tmux/2.7: 10 files, 705.2KB, built in 29 seconds  </description>
    </item>
    
    <item>
      <title>DeepLearningによる画像解析</title>
      <link>https://yutakikuchi.github.io/post/201809170141/</link>
      <pubDate>Mon, 17 Sep 2018 01:41:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201809170141/</guid>
      <description>[etc] : DeepLearningによる画像解析 概要 http://www.image-net.org/challenges/LSVRC/ http://www.image-net.org/challenges/LSVRC/2012/
ILSVRC(ImageNet Large Scale Visual Recognition Challenge)はImageNetが毎年主催するコンピュータを利用した画像解析による物体認識・検出のコンペ。2012年にDeepLearningの手法が登場し、物体認識・検出の技術として3位以降のMachineLearningチームとError率で圧倒的な差をつけて優勝したことから注目を集めた。DeepLearningによる画像解析タスクといっても目的が複数存在するため、言葉の定義を下記にまとめる。
 物体認識(Object Recognition・Classification) : 1枚ずつの画像毎に何の物体であるかを認識する。(1枚の画像に対して1つの物体のラベルを付与する。) 物体位置特定(Object Localization) : 1枚の画像の中に物体が何処に映っているかの領域を認識する。 物体検出(Object Detection) : 1枚の画像の中に何が何処に映っているかを検出する。(1枚の画像に対して複数の物体のラベルと領域を認識する。) セグメンテーション(Segmentation)  : 1枚の画像の中に何が何処に映っているかをピクセル単位で分離する。  Object Recognition: which object is depicted in the image? Object detection: where is this object in the image?
Ref : image processing - Object detection versus object recognition - Signal Processing Stack Exchange
画像解析アルゴリズム   DeepLearningの画像解析アルゴリズムは目的により多数あり、それぞれで使用目的が異なる。
 物体認識(Object Recognition・Classification)  VGG(Visual Geometry Group : team)  Visual Geometry Group Home Page 畳み込みとプーリング層で構成される基本的なCNN。層の数でVGG16、VGG19がある。   ResNet(Residual Network)  https://arxiv.</description>
    </item>
    
    <item>
      <title>Computer Vision : Visual Importance Mapの研究</title>
      <link>https://yutakikuchi.github.io/post/201809090609/</link>
      <pubDate>Sun, 09 Sep 2018 06:09:43 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201809090609/</guid>
      <description> [etc] : Computer Vision : Visual Importance Mapの研究 Youtube : 
Paper :  
Ref: http://web.mit.edu/zoya/www/docs/predImportance_final.pdf http://www.dgp.toronto.edu/~donovan/layout/designLayout.pdf
MIT・Toronto大・Adobeなど様々な企業において、特定のGraphic Designに対して、どこが人が注目しているかをVisualizationするVisual Importance Mapという仕組みが研究されている。可視化の方法としてはGraphic Designの上にHeat Mapとして注目される領域をAIにより予測してOverlayしている。
一般的なCreative作成においてはGraphic Designerの経験と勘に基づいて作られているが、DesignのAudienceが注目されやすいポイントをVisualizationすることで新しいInsightをDesignerに与える。更にGraphic DesignerがInteractiveにCreativeを操作可能な画面の中で効果を予測しながら編集しリアルタイムに新しい価値を気づくことが可能な環境を提供することで、従来必要としたA/BテストによるAudienceの反応を計測する必要が無く、Operationコストと事前に効果を最大化するための施策を実行できることが魅力となる。

Designの効果の予測はどうやっているのか。PaperによるとAIへの入力となるデータについてはflickrのDesign、Mturkを利用して多人数にAnnotationをさせている。 Amazon Mechanical Turk
Annotationの方法としてはBubble ViewというCreativeをぼやかした状態において、Annotatorがどこをより見たいかを選択してもらう。これよりGround truth(教師データ)を集める。Ground truthデータをFully Convolutional Networks (FCNs)にかけてModelを生成し、新しいデータをModelに適用することでCreative上の注目ポイントを画像のPixelレベルで表示を可能としている。類似のResearchとしてSaliency Mapという人間の脳へのbottom-up性注意を予測するModelがある。(Saliencyの場合は視覚としての刺激を抽出することで、例えば暗い夜に月が明るく照らされると対比によって月の効果を認識する心理学的理論を示す)
Ref : github.com
既にAdobe senseiなどのApplicationでは上のようなGraphic Designの最適化、自動生成の世界が発表されており、今後ますます注目されてくる領域である。 
www.adobe.com
 </description>
    </item>
    
    <item>
      <title>製造業のAI導入</title>
      <link>https://yutakikuchi.github.io/post/201809080344/</link>
      <pubDate>Sat, 08 Sep 2018 03:44:02 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201809080344/</guid>
      <description> [AI] : 製造業のAI導入 Industory4.0、SmartFactory Industory4.0, SmartFactoryという言葉があるように製造業の工場ラインに対してIoT・AIを導入し生産工程をデジタル化する計画がある。生産工程のデジタル化の先に人が担っていた作業を補助する目的でのAI・IoT導入検討が進められている。
https://ja.wikipedia.org/wiki/%E3%82%A4%E3%83%B3%E3%83%80%E3%82%B9%E3%83%88%E3%83%AA%E3%83%BC4.0
製造業といってもIoT・AIの活用検証は多岐にわたる。 1. 製造ライン・プロセスでの検品作業の自動化 2. 製造機器の故障発生をセンサーログデータなどから予測する予防保全 3. 製造ラインを効率化するための生産計画の効率化

引用 : https://www.projectdesign.jp/201704/ai-business-model/003521.php
上記以外にも様々な検証が進んでおり、2030年にはAI活用業界のTopとして名を連ねることが予想されている。製造物に異常が発生したときの予算ロスはビジネス的なインパクトとして非常に大きいので、今後AI導入の注目業界であることは確かである。ただし、現状の製造業はAI導入のための課題はたくさん存在する。下記は製造ラインの異常検知の課題を列挙している。
データ収集の課題  紙などで情報を記録、デジタルデータを保存していないケースが存在する 製造ラインに対してIoTデバイスの設定が物理的に難しく、デジタルデータを収集するのに時間を要する。現状の製造ラインがIoTを導入することを前提とした設計になっていない。 熟練者に依存するタスクが多く、その人でないと判断ができない。また基準化、言語化がされていない。作業者の暗黙知、形式知のそれぞれのナレッジメントとデータが明確化されていない。 検品作業自動化のための異常データが発生する確率が少なく、これからデータを貯めるフェーズだとAI導入に時間が掛かる。 各社のデータ管理ポリシーが強固である。製造ラインのオンプレミスな環境から、クラウドなど外部にデータを預けることはポリシーに反することがある。  AI導入の課題  検品作業の人が担っているプロセスにおいては異常を検知する精度が高い。またAIが出力可能な精度はまだ100%にないため、作業すべてを置き換えることができない。 IoT・AIの両方をセットで新規導入するケースについては、IoTで撮像条件を良い状態にしつつ、AIの精度を上げる必要があるので、対応の時間が長期化するケースが多い。仮に精度問題が発生したときに、初期においてはどちらに原因があるかを都度分解する必要がある。 業務プロセス側のドメインを知らない作業者がAIのモデル運用を続けることで、実運用と乖離した最適化をしてしまうリスクがある。 AIによる異常検知が目的となってしまい、本質的な製造機やパーツが不良となる原因分析の方にフィードバックができない。  新しい動き 一方で最近の取り組みとしては良い材料も多い。工場の老朽化により製造ラインを新しく設計するケースも有り、その場合はIoT・AIの導入を見据えた形でリプレイスが検討されている。また製造ラインの担当者も異常検知が主目的ではなく、異常が発生する原因を特定するための製造物の正しい状態把握をデジタルデータで蓄積し、可視化から分かる機械工学の改善シフトに軸を移そうという動きも見受けられる。
製造業のAI導入の課題はまだまだ山積みではあるが、技術的な取り組みとしては非常にチャレンジングな領域であり、今後もこの方面には継続してトライをしていきたい。
 </description>
    </item>
    
    <item>
      <title>運のコントロール</title>
      <link>https://yutakikuchi.github.io/post/201808121805/</link>
      <pubDate>Sun, 12 Aug 2018 18:05:34 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201808121805/</guid>
      <description> [etc] : 運のコントロール 逆説のスタートアップ思考 (中公新書ラクレ 578)
作者: 馬田隆明出版社/メーカー: 中央公論新社発売日: 2017/03/08メディア: 新書この商品を含むブログ (1件) を見る
馬田隆明さんの逆説のスタートアップを読んだ。アイディア、戦略、プロダクト、運についてそれぞれの章ごとに書かれており、スタートアップに携わる人は一読することをおすすめする。起業の科学などアイディア〜プロダクトについては説明がある他の本もあるが、運についてはなかなか見ることがなかったコンテンツなので、読んでいて非常に面白かった。なお、馬田さんはスタートアップについて様々なコンテンツをslideshareで共有している。
※ 第四章、運についての言及をメモとして共有。ただし、内容は意訳とし記載しています。 「運もある程度コントロール可能である」。①成功している起業家こそ、リスクを嫌う傾向があり、リスクを管理するためのポートフォリオを作成する。スタートアップの一つの行動がリスクであるのだとすると、それ以外の分野では慎重に行動をする。アインシュタインは特許庁に勤めながら相対性理論を、カフカは保険局員として働きながら返信を書き上げた。②成功している起業家はタイミングを伺ってる。一番乗りで市場に入ることが正しくもなく、適切なタイミングまでじっくりと待っている。盛り上がっている市場に焦って参入することにより失敗する確率が高くなっているという報告もある。加熱が過ぎた、もしくは悪い時期に参入している方が成功確率が高くなる。③成功している起業家はチャンスを失うリスクのほうがリスクであると認識をしているので、今あるもののリスクではなく、将来を見て判断をしている。特にチャンスを失うというリスクの方を危惧する傾向がある。
ブラックスワンのように予期せぬリスクが発生する事も当然有り得て、そこの対しての回避戦略がいくつかある。一つの例としては「バーベル戦略」とい言われるもので、ハイリスクな投資を10〜15%、それ以外には健全な投資として85%〜90%を確保しておく。中間な投資は一切持たないという手法であるが、相対的にはミドルなリスクを取っていることになる。ミドルなリスクとして全てを保有していると予期せぬブラックスワンに耐えられず、全てが吹っ飛んでしまう可能性もある。
アンチフラジャイルという脆弱性を逆にうまく活かすことこそがスタートアップである。現実世界にあるボラティリティによる非対称な良いブラックスワンこそがイノベーションであるという説があるように、良いリスクに張ることがスタートアップであるという考え方。
挑戦の量が質を生みだす。 とある事例として、あるグループを量で評価する試験、質で評価する試験を同じ時間で回したときに、最終的には量で評価するグループの方が良い質を生み出せた。量による試行錯誤の回数を増やすことが非常に効果的であることを裏付ける例。アインシュタインは248、ダーウィンは119、フロイトは330の論文を書いている、エジソンは1093の特許、バッハは1000曲以上を作曲、ピカソは2万以上の作品を残しているように、天才も数多くの挑戦をしており、彼らの成功している時期は失敗を重ねていた時期と一致するという内容もあり、如何に試行錯誤が重要であるかが分かる。
以上が意訳内容。運もある程度コントロール可能であるという背景は、失敗のリスクも合わせてコントロールすることで、大きな失敗をする前に成長を持続させるという話。第四章だけでも時間があったら読み返したいと思える内容でした。
 www.slideshare.net
そこに張る勇気逆説のスタートアップ思考 (中公新書ラクレ 578)
作者: 馬田隆明出版社/メーカー: 中央公論新社発売日: 2017/03/08メディア: 新書この商品を含むブログ (1件) を見る
未来を正しく予測すること、どんなに優秀な人物が未来を考えても非常に難しいという事実がある。数多くの発明家や起業家が自分が描く未来へ投資を行い、失敗し続けてきた歴史もある。「未来は予期せぬところからやってくる」Y Comibinatorの設立者でもあるポール・グレアムも未来を予測することの難しさから、未来へのアイディアではなく考える人に注目すべきという話もある。
正しく予測できなかったとしても、現在〜未来を目指す中で「失敗の確率を減らすこと」は可能である。一見ダメそうなアイディアでも、隠れた良いアイディアを見つけることがスタートアップでは重要な要素とされているが、隠れた良いアイディアを実現する際に、検証というプロセスを入れることである程度のリスクを回避することができる。検証は最大の効果としては、その領域に携わる人間に対してアイディアを投げかけてみたり、課題となる一次情報を集めることで、自分のアイディアに対して客観的な感覚を注入することである。
検証されたアイディアに対して実施すべきかどうかの判断に必要なもの、それは経営者としてそこに張る勇気である。小さな市場にまずは目を向けて、そこの独占的なプレイヤーとなること。それを基に次は大きな市場を狙う。小さく成功を作って、大きく展開する。仮にそこで失敗、やり方を変えるためにピボットしても、バッターボックスに立ち続けてバットを振り続ければ、おそらく立ち直れる。そこに張る勇気。これが本当に重要。
 </description>
    </item>
    
    <item>
      <title>RPAとAIの違い</title>
      <link>https://yutakikuchi.github.io/post/201808051306/</link>
      <pubDate>Sun, 05 Aug 2018 13:06:54 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201808051306/</guid>
      <description> [AI] : RPAとAIの違い RPAとAI  RPA = Robotic Process Automation AI = Artificial Intelligence  これらの違いは？ なるほど、言葉の定義だけでは違いが分からん。
RPAとは狭義の意味では、ルールベースをロジックとした人間の作業を簡易化・もしくは自動化する仕組みで、Webツールなどで提供される。狭義の中では人間の作業が全てルールに落として、それをシステムの機能で再現させることである。RPAの定義では人の簡易的な脳を機械にコピーをしていくため、機械が自ら学ぶような世界観は含まれない。RPAを広義に捉えると簡易的なAIも含めて人間の作業をルール化していく仕組みを示すが、AIとの境界面が明確化されないので、ここでは狭義のRPAについて記載している。狭義のRAPにおいては金融系会社などで人の作業を代わりに対応させる実施、例えば伝票への自動入力等が多く進んでいる。
AIは、過去のデータを利用した様々な判断をするための予測をする仕組みを提供する。人間が予測のロジックを事前に提供することで、データからの読み取れる判断ポイントや特徴については機械が解釈をする。例えば写真の中にどのような物体が写っているかを自動的に判別する場合、RPAのように人間が物体を見たときの判断ポイントの全てをルールとして機械に与えるのではなく、それらを機械が自動的に判断するために過去のデータから解釈をしてく。
簡単なまとめ  RPA(Robotic Process Automation) : 人間の作業を機械が解釈可能なルールに落とし、ルールに基づいた作業を機械が自動化するような取り組み。 AI(Artificial Intelligence) : 人によってルール化されないポイントも機械が自動的に解釈し、自動的に物事を判断や予測するための仕組み。  参考URL www.itmedia.co.jp thefinance.jp winactor.com
 </description>
    </item>
    
    <item>
      <title>見えない人工知能を売ることの難しさ</title>
      <link>https://yutakikuchi.github.io/post/201808040812/</link>
      <pubDate>Sat, 04 Aug 2018 08:12:23 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201808040812/</guid>
      <description> [etc] : 見えない人工知能を売ることの難しさ 結果が見えない人工知能 人工知能をCustomerに販売することは難しいとされる。その要因は何か。
一番のポイントはCustomerへの販売を担当するSales担当者も人工知能を開発することにより、Customerの課題が解決できるかが受注のタイミングでは分からないということである。結果が見えないというのは受注のタイミングの話である。
Machine Learning・DeepLearningのどちらの手法を基にしたとしても、Customerの課題に対してデータを集め、人工知能のモデルを作り、その後に評価を行う。このプロセスを踏まえないと、そもそもCustomerが求めるKPIに対して成功・失敗するということが分かりにくい点である。
経験を持った技術者であれば先行研究の内容から特定の課題に対して、どういったデータ量と質、更にはMachine Learning・Deeplearningの手法を採用、モデルのチューニングをすると予測精度◯◯%ぐらいは出るかも、というざっくりした見積もりは可能である。ただし、この経験を持った技術者が人工知能を提供する側にはいたとしても、Customerの中に存在するとは限らない。むしろ存在する可能性は低い。提供側が精度◯◯%出ますのでご安心をという説明が出来たとしても、Customer側は何故そのような結果になるか、という点が理解しづらいはずである。
対して、一般的なWebシステムを開発する場合は、Inputのデータを与えてApplicationのlayerで演算をし、DBに格納・結果の可視化しOutputする一連の流れは、このシステムの業務手続きのフローを明確化したり、結果の出力サンプルをSales担当でもおそらく作成はできるであろうし、そしてこれらの内容を受注の前にイメージの共有がCustomerにでき、おそらく理解も可能なはずである。人工知能のように上記評価プロセスを実施しなくても、結果までがある程度見えてしまうという点が大きい。
最近は人工知能の評価プロセスを回す前、具体的には受注の前のタイミングで簡易的に評価プロセスを回すツールなども多く出てきている。今後これらのニーズは高まっていくであろうが、下記にまとめるように売ることを難しくしている要因全てを解消できるわけではないので、これらは業界の課題としてまだ残り続ける。
難しい要因のまとめ  売ることが難しい要因① : Sales担当者が受注の前にどれほど顧客課題を解決できるかの見積もりが難しい。 売ることが難しい要因② : データを集めて、モデルを構築し、その後に評価を行うプロセスを入れないとプロジェクトの成功・失敗が正確に分からない。 売ることが難しい要因③ : 一般的なシステムと異なり、InputとOutputまでの業務ロジック、更には演算(途中結果を含む)の可視化が難しい。 売ることが難しい要因④ : 人工知能が得意とする、もしくは課題解決可能なものは日々広がりを見せているが、まだまだ人間の能力を大きく超えることが出来ていない。それをCustomerの期待値に合わせて説明するということも難しい。  売る側の人間からのレポートは以上です。
 </description>
    </item>
    
    <item>
      <title>何を問題とし、どのように解くか</title>
      <link>https://yutakikuchi.github.io/post/201807230022/</link>
      <pubDate>Mon, 23 Jul 2018 00:22:44 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201807230022/</guid>
      <description> [etc] : 何を問題とし、どのように解くか ビジネスの立場の違う人間同士だと、向き合っている問題が世界情勢、市場、顧客課題詳細など自分が一番解くべき問題についてはレイヤーが異なることが多い。それが故に一緒に問題解決の話をしていても、プロトコルが合わずに、意見の食い違いが発生する。
巷には問題の定義より、課題に対するHowtoのノウハウが溜まっており、方法論ってそんなに重要なんだっけという思いが増してくる。それらにはなぜその問題を解こうとしたのかが書かれていないからだ。重要なのは「何を問題とするか」、その次にどのように解くかの流れである。立場の違う人通しでも、問題の定義がレイヤーごとに明確化され、なぜその問を選定したのかの背景が伝わるだけでも価値があり、そのステップが事前にあれば立場の違う人同士の中でも、意見の食い違いは緩和されるであろう。
 </description>
    </item>
    
    <item>
      <title>ソリューション型とプロダクト型</title>
      <link>https://yutakikuchi.github.io/post/201807150608/</link>
      <pubDate>Sun, 15 Jul 2018 06:08:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201807150608/</guid>
      <description> [etc] : ソリューション型とプロダクト型 Inspired: 顧客の心を捉える製品の創り方
作者: マーティケイガン出版社/メーカー: 株式会社　マーレアッズーロ発売日: 2015/02/07メディア: Kindle版この商品を含むブログ (1件) を見る
経営者が考えるビジネスのロジック 会社の経営は一言で言ってしまえばビジネス(営利・非営利の事業)を作ることであり、そのために人・モノ・金を動かす権限を持つ。ビジネスには様々なモデルが存在し、どのようなエコシステムを目指すかはその会社の経営者が自身で判断しなければならない。最近良く話す内容としては会社経営も成功ロジックを書くことであり、システムエンジニアが処理手続きを書く事と非常に似ている。成功までの最短となるプロセスをロジックを積んだ言語によって書き起こしているだけである。
ソリューション型 成功ロジックの中身をどう書くか。経営でよく上がる話として、ソリューション型、プロダクト型のどちらのビジネスを目指すべきかという点。または両社を目指す場合のコミット度合いの配分をどのように線を引くか。ソリューションは&#34;顧客の課題を解決すること&#34;であり、コンサル、SIなどはここでは含まれる。顧客のビジネスの深い業務知識を保有し、そこに対して他の誰も届けることが出来ない課題解決のコミットメントが継続できるのであれば、顧客の成功体験サポートをし満足度を得ることで、安定的な収益を重ねることができる非常に有効なビジネスだ。その一方で顧客に向き合う人数と時間を必要とする人工型になってしまう。
プロダクト型 対してプロダクト型とは&#34;自社が開発した製品・商品を必要とされる市場に提供する事&#34;とここでは定義する。モノやサービスを中心として顧客の需要に応えていく方針を持ち、自発的なプロセスを回していく。プロダクト型でも製品の提供結果として顧客の課題を解決する(上記のソリューション)ことは可能なので、ソリューション型の定義の中にプロダクト型も含まれるという定義がより近いであろう。自身たちが作るプロダクトの定義や展開先の市場特定もプロアクティブな意思決定が可能であり、課題に対する時間のコントロールが可能。その一方で開発初期においては本当に市場を取れるかどうかの不安が常に残る。
決定的な違いはなにか ソリューション型、プロダクト型の決定的な違いはなにか。それは顧客課題解決に対しての時間の使い方とキャッシュの発生である。どちらを会社の中心ビジネスとして採用するかによって対応する人の姿勢とスキル要件が大きくことなってくるので、会社全体としての顧客課題への向き合い方というものが自然と決定される。ソリューション型は役務ベースでのコミット成果に対する収益であり、プロダクト型は市場浸透に対しての収益となる。両方の型をビジネス目指す会社も多く、特にソリューション型で得られた具体的な知見をヒントに汎用化プロダクト型にシフトしていくやり方だ。ソリューション型の具体例を基に共通化して使える部分を抽象化した形で汎用型のテンプレートを開発し、ビジネスや業務の効率を目指すスタンスはソフトウェアの開発業務でもよく行う。会社内でビジネス変換を個別ソリューション型 = 汎用型プロダクト型に行う場合は全社にその戦略転換を伝達し共通理解と認識を持ち、採用要件が異なる中で集められたメンバーで役割を担う組織を作り、実行していく必要がある。
 </description>
    </item>
    
    <item>
      <title>久しぶりの投稿</title>
      <link>https://yutakikuchi.github.io/post/201807130219/</link>
      <pubDate>Fri, 13 Jul 2018 02:19:14 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201807130219/</guid>
      <description>[etc] : 久しぶりの投稿 1年以上日記を書くことをサボっていました。日記を再開し、これからは仕事やプライベートで感じたこと、調査したこと、考えたことの整理として投稿を再開していきます。最近はメタ思考を意識しながら生きています。できる限り早く成長したいです。</description>
    </item>
    
    <item>
      <title>新しい環境で得た学び</title>
      <link>https://yutakikuchi.github.io/post/201706120053/</link>
      <pubDate>Mon, 12 Jun 2017 00:53:23 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201706120053/</guid>
      <description> [etc] : 新しい環境で得た学び イノベーションのジレンマ―技術革新が巨大企業を滅ぼすとき (Harvard business school press)
作者: クレイトン・クリステンセン,玉田俊平太,伊豆原弓出版社/メーカー: 翔泳社発売日: 2001/07/01メディア: 単行本購入: 59人 クリック: 811回この商品を含むブログ (393件) を見る
2017年4月末から「Innovationで世界を変える」というビジョンを掲げる会社で、ゼロから物を作り顕在的な市場に当てはめることを楽しんでいる状況で、過去の環境では得られなかった 組織体制、採用 について新しい学びがあり、それをメモとして記載します。
Managerという職位をあえて作らない 今の会社は50人程の規模でチーム数は8個程ですが、チーム組織長やプロダクト管理職などの所謂Managerを作らず、全員がフラットな関係で業務を担当していくという姿勢を保っています。チームメンバーに求める事として 全員が何でも自分で考え、自走する ことを行っており、個々のメンバーが重要なポイントであるチームOKRを忘れなければ、意外と成り立つ世界であることが分かります。前提としては、採用のハードルで誰も通らない程の経験や学力レベルでフィルタリングをしている事もありますが、タスクを与えられる事無く自分で考えて行動できる素養をそもそも身につけているメンバーが多くいる(可能性のある人を採用し、中で意識付ける)ということがうまく回っている要因で、このような環境の場合管理者は不要であると感じます。
私も過去には幾つものManager職を兼務しておりましたが、その職位に全く固執していなかったので、常に廃止しても良いと考えていました。Managerがいることによって個々のメンバーは自分の得意専門領域に注力しがちになり、事業や会社を見る視線が低くなってしまう。逆にManager側はメンバーの専門タスクをどううまくコントロールするかという、規模が小さなフェーズの会社にとって使うべきではない時間に追われるなど&amp;hellip; 様々な問題があったように感じていました。
まとめると、今の環境は自分がもともと考えていた理想体制にすごく近い。人数が増えてきた時はまた新しい問題が見つかると思いますが、そこまでは今のまま走り続けるべきと思います。
海外からデータサイエンティストを採用する 私のチームは ビジネスを研究し、データサイエンスの価値を提供する ことを行い、機械学習の実務や最新研究への精通、クライアントのニーズを理解して解決方法を提案するコンサルティング能力、潜在顧客のリードから顕在顧客を導き出すマーケティング施策実行など、幅広い業務知識を必要とします。上でも書きましたが、それが故に採用ハードルが高く、人手を必要とするタイミングでも人が取れないという問題が長くあったようです。
採用のハードルを下げることはせず、そして日本のみの人材市場で上の能力を持つ人を探すのはもはや無理というのが会社の結論で、英語を利用した採用に枠を広げることによって、数少ない候補者をなんとか探そうというのが短期的な方針のようです。
私も何名か面接対応をさせていただきましたが、スーパーハイエンドな海外の人材でも日本で働くことに強い動機や意思があり、会社のビジョンや小規模フェーズの体制への理解があるような方からの注目は意外とあり、そういった方が積極的に入社をしてくれることは喜ばしいことです。
当然コミュニケーションが必然的に英語になるわけですが、細かい表現やちょっとしたニュアンス全て伝えきれているかという不安は常にあるので、英語での伝え方を勉強せねばという思いでおります。
最後に 上記以外にも数多くの気付きがありますが、また機会を見つけて書こうと思います。会社全体として自分より何でもできる人が多くいるというのは、知らない経験や世界を人から自身の体験として聞けることは環境として非常に恵まれていると感じますね。
 </description>
    </item>
    
    <item>
      <title>Consumer Monopolyの基準</title>
      <link>https://yutakikuchi.github.io/post/201705050204/</link>
      <pubDate>Fri, 05 May 2017 02:04:11 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201705050204/</guid>
      <description>[etc] : Consumer Monopolyの基準 億万長者をめざすバフェットの銘柄選択術
作者: メアリー・バフェット,デビッド・クラーク,井手正介,中熊靖和出版社/メーカー: 日本経済新聞出版社発売日: 2002/05/20メディア: 単行本購入: 6人 クリック: 110回この商品を含むブログ (66件) を見る
GWに部屋の掃除をしたら収納の奥から出てきた「バフェットの銘柄選択術」を十数年ぶりに読んでみた。以前は本の内容をConsumer Monoploy(消費者独占)の性質を企業に対する投資の視点として読み取っていたが、読み直してみると事業を作る企業としてもMonoplolyの対極にあるCommodity企業に染まらないようPositionを確立することが重要であるという内容として感じる。Commodity化された事業についてはMarketへの参入が競争という形であるため市場が存在することは明確であるが、激しい価格競争に打ち勝ち、他社製品との差別化のため圧倒する技術革新を起こす事が求められる。対してMonopoly事業では独占資本市場への参入であるため明確な市場定義は無く、独自ブランド(選択の余地が無い)を確立して企業の価値をZeroから作っていく必要がある。
話を本の内容に戻して、銘柄選択としてのMonopolyの基準としては下記のものがある。 1. 独自ブランド、2.EPSの増加、3.多額負債が無い、4.ROE、5.内部保留利益の再投資状況、6.インフレを価格に転嫁可能 。独自ブランドについては上で書いた通りだが、EPS(Earnings Per Share)、ROE(Return On Equity)といったFundamentals分析も重要であるということ。Fundamentals分析の指標は四則演算で全て求められる単純なものだが、何を意味するかを理解するのが難しいのと指標が分からないまま書を読み進めるのも難しいのでMemoとして下に記載。(計算式を理解することで不足変数を脳内で算出することができる。)
ちなみにバフェットはFundamentals分析も利用した上でMonopoly企業を事前に見定め、そのStockが悪材料によって下がった時に買い妥当な水準に戻った時に売るという手法により成功している。投資の姿勢としてはリスクが高い逆張りをしているわけだが、本当は優秀かつ優良な企業に目を向けて支える投資家としては正しいあり方なんだろうなぁと思う。
 時価総額  時価総額 = 株価 * 発行済み株式総数   EPS(Earnings Per Share) : 1株あたりの利益  EPS = 当期純利益 / 発行済み株式総数   BPS(Book-Value Per Share) : 1株あたりの純資産  BPS = 純資産 / 発行済み株式総数   PER(Price Earnings Ratio. P/E) : 株価収益率  PER = 株価 / EPS   PBR(Price Book-Value Ratio.</description>
    </item>
    
    <item>
      <title>Team Geekを読んだ</title>
      <link>https://yutakikuchi.github.io/post/201605020218/</link>
      <pubDate>Mon, 02 May 2016 02:18:27 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201605020218/</guid>
      <description>[etc] : Team Geekを読んだ Team Geek ―Googleのギークたちはいかにしてチームを作るのか
作者: Brian W. Fitzpatrick,Ben Collins-Sussman,及川卓也,角征典出版社/メーカー: オライリージャパン発売日: 2013/07/20メディア: 単行本（ソフトカバー）この商品を含むブログ (20件) を見る
 GWを利用して積読の消化をを実践中。Googleのギークたちはいかにしてチームを作るのかというサブタイトルが印象的な一冊。主にチームの文化形成やリーダー(マネージャー)としての在り方の部分を重点に自分なりに感じた言葉として纏める。
2章 素晴らしいチーム文化を作る 
 チーム文化 = エンジニアリングチームが共有する経験、価値、目標。 優秀なエンジニアに働いてもらうためには優秀なエンジニアを採用しなければならない。優秀なエンジニアトップダウンのマネジメントではなく合意ベースのマネジメントを好み、意思決定に参加したいと思う。 建設的な批判はチームの発展や成長には欠かせない。 MTGの数を減らして非同期コミュニケーションを増やすべき。 ミッションステートメントを作成し、注力すること、やらないことを明確化し、プロダクトの方向性の合意を得る。ただしプロダクトのピボットに合わせて修正可能なものとする。 MTGを行う際のルールを作る。  絶対に必要な人だけを呼ぶ。新しいものを設計するMTGには5人以上呼んではいけない。 アジェンダをMTG前に配布。 ゴールを達成したらそこで終了。 MTGを順調に進める。 お昼、終業の前に開始時間を設定する。  Face to FaceのMTGを過小評価してはいけない。 全てのコードはコードレビューされるべき。品質を保つと同時に品質への誇りを持たせる。   3章 船にはキャプテンが必要 
 Googleではチームリーダーは二つの役割に分かれている。  TL(チームリード) : プロダクトの技術的なものに責任を持つ。 TLM(テクニカルリードマネージャ) : エンジニアのキャリアに責任を持つ。  多くのエンジニアはマネージャーになりたくないと思う。 マネージャーになることは、自身をスケールさせたり、より多くのエンジニアと協力してより多くのコードを書くことができるし、もしくは自分がマネージャーに向いているかもしれない。 必要以上のマネージは行わないこと。例:マイクロマネジメントなど サーヴァントリーダーシップのようにエンジニアが働きやすい環境を整えるために、あらゆる障害を排除することも必要。 採用においては自分の言いなりになる人ではなく、自分より優秀な人を採用すべき。自分より優秀な人は自分の変わりになってくれるだろうし、新しい刺激を与えてくれるかもしれない。 1〜2人ほどのパフォーマンスが低い人がいるだけでチームはうまく回らなくなってしまう。そういった人をチームに残すことはその人のためにもならない。今の場所で成長させるか退席させるかを判断する。 パフォーマンスが低い人へのコーチングは2-3ヶ月で達成する目標を定めてあげる。小さい目標から入って小さい成功体験を何個か積み重ね、次第に目標を大きくしていく。 採用を妥協してはいけない。イマイチな人を採用した結果、採用を妥協したことのコストより大きくなってしまう恐れがある。 チームメンバーの幸せを追い求める。また面談では必要なものを聞く。 チームメンバーにタスクを委譲するものと、自分で手を汚して対応する必要がある。リーダーになって間もない頃であればタスクをメンバーに渡してチームが学習する機会とする。しばらくリードした後であれば他の人がやらないような領域で自ら手を動かし尊敬を集める。 事を荒立たせる必要がある時もあり、その場合は直ぐに着手する。自然解決を待ってはいけない。 人を幸せで生産的にするのは外発的動機づけではなく、内発的動機づけであり、内発的動機づけには自律/熟達/目的の3つが必要。</description>
    </item>
    
    <item>
      <title>The Data Management Platform: Foundation for Right-Time Customer Engagement [DMPに関する欧米の調査内容(2012.12)]</title>
      <link>https://yutakikuchi.github.io/post/201603062332/</link>
      <pubDate>Sun, 06 Mar 2016 23:32:27 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201603062332/</guid>
      <description>[DMP] : The Data Management Platform: Foundation for Right-Time Customer Engagement [DMPに関する欧米の調査内容(2012.12)] The Data Management Platform: Foundation for Right-Time Customer Engagement  調査資料
http://www.iab.net/media/file/Winterberry_Group_White_Paper-Data_Management_Platforms-November_2012.pdf
2012年12月にIABから出されたDMPに関する実態調査的な資料を見つけたので目を通してみた。※尚下記で引用される英文説明や図は全て上のリンクからの出典となる。
DMPの概要

出典元の表現としては以下のように書かれている。
 the data management platform (DMP), an emerging technology solution that supports “Big Data” implementation for advertisers, marketers, publishers and others looking to aggregate, integrate, manage and deploy disparate sources of data.  data management platform(DMP)は分断されたデータソースを集約/結合/管理/配布したい広告主、マーケータ、パブリッシャー、その他...の人たちのための&#34;Big Data&#34;の実装をサポートするテクノロジーソリューションとして用いられる。DMPとしては上図にあるように
 AGGREGATE(DATA SOURCES)、INTEGRATE AND MANAGE(DMP APPLICATIONS)、DEPLOY(USE CASES) の3パートに分かれており、データの収集、解析、利用という流れがある。AGGREGATEのフェーズにおいてデータソースとして中核となるのはFirst-Party(クライアントだけが持つデータ)、Thrid-Party(データ公開をしている外部クライアントが持つデータ)としてのデジタル化された行動履歴のデータであり、その他よく使われるものとしてはOfflineのデータ、商品の購買データ、新しいデータソースなどがある。収集したデータソースに対してデータの記録と保管/バラバラのフォーマットにならないようデータの正規化/必要なデータを選択、セグメント化/分析した結果を見て意思を決定する。その先に、デジタルデータの独立したフィードを集約or管理し、インターネットユーザーのより深い興味解析やOnline広告のターゲティングを強化し、mediaのセールス拡大のために行動するであろうインターネットユーザーのセグメントをリッチ化する。更にインターネットユーザーのデジタル体験を最適化するためにデジタルとデジタル以外のデータを集約してあらゆるチャネルをとおしてユーザーとコミュニケーションをする。</description>
    </item>
    
    <item>
      <title>機械学習の種類と特徴</title>
      <link>https://yutakikuchi.github.io/post/201602132157/</link>
      <pubDate>Sat, 13 Feb 2016 21:57:24 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201602132157/</guid>
      <description>[etc] : 機械学習の種類と特徴  人間ではなく機械が自動的に意思決定することのメリットとして、大量のデータをInputとした予測、推定、分類などの処理をAlgorithmの構築によって瞬時に行える事である。 1枚の画像だけを見て何が写っているかのような判断においては人間の脳が優れているものの、大量のデータInputを基にした組み合わせの選択や最適解に瞬時に辿り着くという目的においては機械に任せてしまったほうが効率的とも言える。昔から機械学習による予測、推定、分類などの処理は様々な手法として提案されており、どういった問題を機械に判断させるかという切り口で最適なものを人が選択する。下記表に機械学習の種類と特徴を纏めてみた。※ただし必ずしも6種類のいずれかに分類される訳ではない。例としてニューラルネットワークがあり教師あり学習であり深層学習にも位置する。
   機械学習の種類   特徴   代表的なAlgorithm   備考     教師あり学習   正解を持つデータを入力とし、特定の法則に従って予測データの出力を行うこと。
(例) 男女などの正解ラベルがあるデータを入力とし、ラベルが未知のデータに対して法則を適用し予測ラベル出力する。  SVM
NaiveBayes
回帰分析  教師あり学習 - Wikipedia    教師なし学習   正解ラベルなどが無く、データそのものを解析して特徴などを出す手法。
(例) データとして距離的に近いものを同一のクラスタとして定義する。   LDA
LSI
主成分分析
K-Means  教師なし学習 - Wikipedia    半教師あり学習   正解を持つデータの数は少ない場合、正解と未知のデータを合わせて学習することで効果を高める事ができる。
(例) 教師あり学習であるSVMを拡張して既知/未知データを合わせて学習する。   S3VMs(Semi-Supervised Support Vector Machines)  半教師あり学習 - 機械学習の「朱鷺の杜Wiki」    構造学習   個々のデータに対して推定を行わずに全体のデータ構造に最適な形として個々の推定を行う。</description>
    </item>
    
    <item>
      <title>Good Product Manager/Bad Product Manager</title>
      <link>https://yutakikuchi.github.io/post/201602071336/</link>
      <pubDate>Sun, 07 Feb 2016 13:36:17 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201602071336/</guid>
      <description> [Product Manager] : Good Product Manager/Bad Product Manager  Good Product Manager/Bad Product Manager 
↑は良いProductManagerとそうでないも内容のまとめ。タイトルについつい釣られて読んでしまったので日本語で起こしてみる。良い/悪いの判断なんて周りが決めることで、もし貴方がProductManagerであるならば例え途中で失敗したとしても自身のProductを成長させたいという信念を曲げずにTryしまくればいいと思いますね。
Good
市場,Product, Productの方向性と競合がどのようにうまくやっているかを知り,精通した基礎知識からの判断と信頼が必要。ProdctのCEOである。製品に対する全責任を取り,Productの成功によって自身を評価する。Productと時間と必要なすべてのものに対して責任を持つ。Contextの方向性も知っている,例えば会社,資金調達,競争等。言い訳をせずにこれらの責任を持ち勝ちプランを実行する。良いProductを良いタイミングで届けるために一緒に協力しなければならない様々な組織に対して時間の時間を全部取り上げるようなことはしない。またチームの時間をとるようなことや様々な機能のProject Manageもしない。自身はProduct TeamではなくTeamを管理する。Engineer Managerとのmarketingの窓口担当になる。ターゲットを定義して「何」(方法とは対照的な)を届けるかという管理を行う。Engineerの人たちと口頭と同じように書き込みでもコミュニケーションをする。非定型的な方向性を与えない。情報を非公式に収集する。FAQやプレゼン,説明資料などを作成する。Productの深刻な問題を発見し現実的なsolutionとして構築する。重要な問題から取り掛かる(差別化の銀の弾丸,頑丈な設計選択,厳しいPorductの決断,攻めや収益のための市場)。チームを収益と顧客に集中させる。多大な力と実行力で良いProductを定義する。インバウンド計画と到達する市場シェアと収益の目標の間に優れた価値を提供することを考える。問題の分解に優れている。プレスに扱われたいstoryを考える。プレスに対して質問をする。プレスとアナリストは賢いことを前提としている。自分の仕事と成功を定義する。訓練されているので毎週ステータスレポートを送信する。
Bad
言い訳を沢山持っている。資金調達が充分でない, Engineering Managerが優秀ではない, Microsoftは10倍のEngineering人材がいる, 自分はハードワークしている,方向性を得ていない。方法論について理解することが最善だと思う。Sales強化のために質問が殺到しそれに答えることに対して不満を言う。一日中問題の火消しになる。口頭で自身の意見を述べ、権限が無く何もできないことを嘆く。失敗を想像する。Microsoftがどのような多くの特徴を持っているかに集中させる。実現できないProductを定義したり必要なものを何でもEngineeringさせる。(それは最も困難な問題を解くこと) 価値の提供と競争力のある機能,価格,普遍性の間の差異について混乱する。全ての問題を一つに結びつけてしまう。多くの特徴をカバーと正確な技術をプレスしたいと考える。プレスのどんな質問にも答えてしまう。プレスとアナリストを区別していないく混ざっている。明白な説明を決してしない。常に誰かから何をすべきかを説明されたいと思う。ステータスレポートの送信を忘れてしまう。
 </description>
    </item>
    
    <item>
      <title>scalaのimmutable</title>
      <link>https://yutakikuchi.github.io/post/201601032003/</link>
      <pubDate>Sun, 03 Jan 2016 20:03:56 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201601032003/</guid>
      <description>[etc] : scalaのimmutable  immutable or mutable, val or var scalaでcollection(List,Seq,Set,Map,Tupple...)を扱う場合はimmutable(不変) or mutable(可変)を使うかで言語内部でのデータの持ち方が異る。また変数の宣言をval(再割当て禁止) or var(再割当て可能)を使うかで実行可否や挙動が変わる。よって2つの観点(immutable or mutable / val or var)の組み合わせで調査をする必要がある。関数型言語ではvalを利用する事が推奨され、scalaのcollectionはdefaultでimmutableが選択されている。よって自然な組み合わせはval × immutableとなり、変数を定義した後に変数に対しては再割当てが行えないので副作用無く安全な方法とされている。これにより通常valで宣言した変数に対して操作を加えた結果を格納する場合はまた別のvalで宣言する必要があるが、collectionにてmutableを用いてvalにて宣言した場合、模倣的な操作が可能なので挙動を追ってみる。
まずはcollection以外の場合、例えばStringなどはval or varのどちらで宣言しても扱うデータとしてはimmutableなので、元のデータに操作を加えた場合後は新しいデータよる再割当てを行おうとする。以下は再割当て挙動、同一性(ポインタが指し示すものが同じ)、等価性(値が同じ)、についての確認。
最初の例ではvalを使用し再割当てできないもの。immutableなのでデータ更新処理で再割当を行おうとしているが変数側のvalでエラーが表示される。
// valで文字列 scalaを代入。 scala val a = &#34;scala&#34; a: String = scala // valで別の変数にコピー。 scala val b = a b: String = scala // valなので再割当てができない。 scala a = a + &#34; java&#34; :8: error: reassignment to val a = a + &#34; java&#34;</description>
    </item>
    
    <item>
      <title>mysqlでgroup毎のTop-K行を取得する方法</title>
      <link>https://yutakikuchi.github.io/post/201512311829/</link>
      <pubDate>Thu, 31 Dec 2015 18:29:42 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201512311829/</guid>
      <description>[etc] : mysqlでgroup毎のTop-K行を取得する方法  How to select the first/least/max row per group in SQL · Baron Schwartz&#39;s Blog 
mysqlを用いた特定のgroupに所属する行を一定数ずつ(もしくは何かしらでsortされたTop-K行)取り出したいという問題がある。これしきの事、単純なgroup byを使って簡単に解きたい内容であるがちょっとしたテクニックを必要とする。調べたところ以下の解決方法がある。
 1. union allを使ってgroup毎に抽出した結果を結合。
 2. tableをgroupで自己結合し特定行数取得。
 3. session固有のユーザ定義変数を使って特定group内の行をcountしていく。
因みにposgre/sql serverはrow_number()というgroup毎に数を採番してくれる便利関数が存在してこれを利用するらしいが、mysqlにはまだ無い様子。
解決方法1の場合は各group毎に特定行抽出した結果をunionするのでgroupが増えるとQueryが冗長でダサい。2の場合はgroupのcross結合を行うのでgroupに紐づくデータが膨大だとつらい。よってここでは 3.session固有のユーザー定義変数を使って...について簡単に紹介する。
下はidの昇順にてTop-10を出している。最初の行でsession固有の変数を定義している。SQL中の@group = media_idがGroupの指定。group変数が未定義の場合は1を同一の場合にはnumをincrementしている。subquery内のrow_numberがincrement数なので最後のwhereにてrow_numberが10以下を指定するとTop-10を抽出できる。rand()にてgroup内からrandomで特定行数抽出したい場合はsubquery内のsubqueryで最初にrand()しておくと良い。
set @K := 10, @num := 0, @group := &#39;&#39;; select id, media_id from ( select id, media_id, @num := if(@group = media_id, @num + 1, 1) as row_number, @group := media_id as mid from contents -- randomに切り替えたい場合はfromとして下記を利用 -- from (select id, media_id from contents order by rand()) as c order by media_id ) as t where t.</description>
    </item>
    
    <item>
      <title>オンライン広告におけるアトリビューション分析の必要性</title>
      <link>https://yutakikuchi.github.io/post/201512240114/</link>
      <pubDate>Thu, 24 Dec 2015 01:14:18 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201512240114/</guid>
      <description> [etc] : オンライン広告におけるアトリビューション分析の必要性  
オンライン広告の評価 ダイレクトレスポンスといったオンライン広告配信を行い広告Clickからの次のActionを求めるような商材の場合、CPA(Cost Per Action/Acquisition)、ROAS(Return On Adevertising Spend)というCOST指標を用いて広告Vendorやキャンペーン評価を行うのが一般的である。この指標を出すためにConversion数(Action数/Acquistion数)を計測が必須で、InternetUserがClickした広告と紐づく直接Conversion、広告はClickせず表示のみを行ったアシスト数の間接Conversionに分けられ、現状ほとんどの広告主が前者の直接Conversionのみを見てCOST指標を評価している。直接Conversion数を出す際によく問題として発生するのが、広告主側でのConversion計測と各広告配信Vendorのレポート数に乖離が発生してしまう点である。乖離の発生原因は直接Conversion計測のやり方と定義の違いである。
まずはConvesion計測のやり方の違いについて言及する。広告主側では広告効果測定ツールというものを導入しており、広告のクリックURLやパラメータに配信Vendorを識別するものを含めInternetUserの広告Click後Conversion完了するまでその識別子を引き回して計測する。こうすることによってどの配信Vendor経由で直接Conversionに寄与したか判別可能となる。その一方で各広告配信Vendorが計測できることは自身が配信した広告のLast Clickと直接Conversionが発生した事実のみで、InternetUserにおける本当のLast Clickというものを各配信Vendorの計測はできていない状況にある。
次に直接Conversionの定義である。各配信Vendorは自身が配信した広告のLast Clickから規定日数以内に直接Conversionが発生したかを計測している。この規定日数が配信Vendorと広告主で異なり、更にはConversionとして認めるInternetUserの属性(新規Userのみ認め、リピーターは除外する)も影響する。この定義差を埋めるために配信VendorのAccount Executiveが配信レポートを提出する際に極力広告主側の定義に合わせているが、それでも上記の計測方法が異なるので完全な一致にはならない。
 ダイレクトレスポンスにおける広告配信Vendor間の競合 広告主は複数の広告配信Vendorを利用している事が多く、ダイレクトレスポンスによるリターゲティングを行っている場合は、1requestにおける配信獲得を目指してVendor間で最悪の場合同じ広告主のほぼ同じクリエイティブでImpressionの奪い合いが発生している。差別化のポイントは確実にConversionするInternetUserに対して無駄な予算を使うことなく適切なタイミングで適切な配信面に届けられているかという点になる。この背景に配信在庫に対してImpressionを幾らで入札するかというBidding Priceの精度が大きく関わっており、多くのVendorがMachineLearningを利用したBidding Priceの最適化を行っている。このVendor間の成績をCOST指標で評価してしまうと、Bidding力で勝るVendorに予算がアロケーションされて局所的な最適化が起こってしまう。局所的な最適化の一番の問題はCPAを合わせるために配信Vendor、運用者(代理店も含む)が動いてしまうので、最終的なConversion数を延ばすという点においてこれらの広告では貢献できなくなってしまう。更にこの状況が続くとオンライン広告市場全体としても喜ばしく無いはずである。どの配信Vendorも確実にConversionするInternetUserに対してはHigh Priceの最適化(局所的な高単価CPM)が行われているが、広告主として全体COSTを下げる必要があるので収益観点においては逆の立場である配信Vendor,Publisherは高単価が扱いづらいことになる。
 日本のRTB広告在庫の使われ方 日本のPublisherが純広告を抱え込み広告在庫をRTB Player等に積極的にならない原因としては日本のRTB在庫利用がダイレクトレスポンスに偏ってしまっている事が考えられる。上で説明したようにダイレクトレスポンスを在庫に入れると高収益性への期待が薄くなる。各RTB Playerも広告主による焼き畑作業が続くと苦しい一方なので、今後ブランド広告を収益の軸とすることも考えなければならない。RTBの仕組みとしてブランド広告を特定のPublisherに高単価で提供するPMP(Private Market Place)等の仕組みも登場してきているので今後の注目株である。PMPを普及させるためにもRTB Player ≠ リターゲティングのようなイメージをDemand側に啓蒙しなければならない。USの方ではブランド広告をRTB在庫として扱い、収益化に成功している様子。
 カスタマージャー二データを用いたアトリビューション分析 Conversionに至るまでの自然検索流入、純広告Impression、アフィリエイト広告Impression、広告Click等の複数チャネルに接触した履歴全てをまとめたカスタマージャーニーデータをアトリビューション分析では利用する。目的としては単純なCPA,ROASのラストクリックを獲得した広告Vendorの評価だけではなく、InternetUserの態度変容に対して各施策がどれだけ寄与したのかを中間指標としてスコアリングし、今後のマーケティング施策全体をどのように設計するかを考えることである。ただしアトリビューション分析がCPA,ROAS評価と比べると一気に難易度が高くなる。なぜならばオンライン施策がオフラインへ購買へどれほど起因したかのシチューエーション問題だけでなく、InternetUserを分析するのかPublisherを分析するのか、それらを対象とする分析のフレームワークやMdelingなど数理統計学の知識を必要とする。アクセス解析ツール側で分析Modeling手法を含んで可視化Solutionとして提供するものも存在するが、複雑すぎて運用者側が使いこなせていない印象がある。今後は広告Vendor側でも分析ツールやフレームワークを提供し、InternetUserの態度変容の可視化とマーケティングシナリオの自動化など運用者負荷を減らす施策が行われてくると予想される。
 リンク アトリビューション分析 - アナリティクス ヘルプ 
The Customer Journey to Online Purchase – Think with Google 
  </description>
    </item>
    
    <item>
      <title>AdExchangeとSupply Side Platform(SSP)との違いって何よ？</title>
      <link>https://yutakikuchi.github.io/post/201512230246/</link>
      <pubDate>Wed, 23 Dec 2015 02:46:29 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201512230246/</guid>
      <description> [etc] : AdExchangeとSupply Side Platform(SSP)との違いって何よ？    JP Chaosmap 2015-2016  from Hiroshi Kondo www.slideshare.net
What is the difference between ad exchanges and supply-side platforms? - Quora  説明に困るAdExchangeとSSPの違い。ポジションレイヤー的な話だとAdExchangeもSSPともにDSPやADNetworkを束ねてPublisher側に広告を流す仕組みであり、この点だけを述べてしまうとほとんど差異が無いように思われる。また現在に至っては使われ方もほとんど同じと言える。(細かい話だとAdExchangeはDSPを束ねてSSPと接続をする場合もあるがここはあまり意識されない。) Quoraを調べていたら2つの違いについて適切な解答があり、最もupvoteが付いているものを読んでみた。結論としては作られた目的が異なる。AdExchangeは薄利なレムナント在庫を自動管理するもの、SSPは単価の高い広告を適切な在庫に流してPublisherの収益最大化を行う。以下は大体の翻訳。
AdExchangeはStock Exchangeを例としたトランザクションを促進する自動化プラットフォームである。用途としては主にレムナント在庫に使われる。レムナント在庫は余剰在庫なので価値がとても低いから人的なリソースを使わずに売りたい。AdExchangeは薄利なので配信量を多くすること頼っている。SSPは供給サイドのサービスでTechnology Platform/Service/Sales(人)のコンポーネントの組み合わせである。彼らは複雑かつExchangeとは取引をしないような大規模なPublisher,DSP,広告主と関係を保っている。Publisherとしては自身のビジネスに専念できるし、扱いたい広告vendorの数を制限できる。(しかし注意しなければいけないこととしてはPublisherはスーパープレミアムな在庫はSSPに開放せず別途持っていてインハウスのSalesチームにBig Brandとして売らせている。) SSPは広告主の沢山の広告をRealTimeBiddingによって提供するようなInterfaceを持ち、Publisher側の個々のadspotの価値を戦略的に最大化させる。SSPはDMPなどのlayerと接続可能なのでAdExchangeより多くのGross Marginを得ている。(注意しなければいけないことはSSPはAdExchangeより様々な面での高いコストを支払っている。)

 Defining SSPs, Ad Exchanges And Rubicon Project | AdExchanger  上のリンクはAdExchangeとSSPとRubicon Projectの定義。以下は大体の翻訳
AdExchangeとSSPはかつては異なるものであったが段々と区別が困難になりつつあり、補完的な技術は統合されている。SSPが登場した頃、AdExchangeとSSPの違いはAdtech vendorの間ではとても重要な内容だった。デジタル市場の動きがとても速いという理由もあって特徴的な名前付けをする必要があった。SSPとAdExchangeの統合は新しいことではなく、GoogleはSSPのAdmeldとDoubleClick AdExchangeを統合している。歴史的な違いとしてはAdExchangeはトランザクションを有効にする一方で、SSPはPublisher側に価格調整機能などの多くのToolを提供してAdExchangeと同じことを行う。長い間様々なPlayerを見ていると、特定の市場空間にいる大体の会社がより優れた顧客サービスをできるようにツールの開発や機能提供を行う。Rubiconは収益最大化を目的としたSSPとしてスタートしたはずが、彼らが十分な在庫を持っていたならば、自然とAdExchangeの機能を統合している。この産業の誰もがmedia platformerであって、全てのPublisherが喜んでいない状況である。SSP,AdExchange,DSPやAdnetworkの統合は周囲(広告主やPublisher)を混乱させる。
  </description>
    </item>
    
    <item>
      <title>Recsys2015で発表されたCriteo社の最新レコメンド情報を読む</title>
      <link>https://yutakikuchi.github.io/post/201512200148/</link>
      <pubDate>Sun, 20 Dec 2015 01:48:45 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201512200148/</guid>
      <description>[etc] : Recsys2015で発表されたCriteo社の最新レコメンド情報を読む    RecSys 2015: Large-scale real-time product recommendation at Criteo  from Romain Lerallut www.slideshare.net
criteo社のRecommendation Logic Criteo社の広告枠への配信に対するCTR/CVR予測モデルの話以外にもRecommendLogicに関する発表がRecsys2015であったので見てみました。一般的なRecommed Systemでは良く見受けられる構成や手法なのでインパクトがある内容ではありませんが共有します。Keywords : Logistic Regression, Hadoop, Memcached, collaboration filterling, Recsys2015,
Criteo社のCTR/CVR予測モデルは下記論文などを参照 Simple and scalable response prediction for display advertising 
 Criteo社の説明とインフラ  2005年に設立、フランスに本社を置き世界に社員1700人(そのうちEnginnerが300人)、100億のUniqUserを持つ。 秒間80万のHTTPリクエスト、29000インプレッション、RTBの処理を10ms、Recommendの処理を100msでこなす。 3つの大陸にインハウスのデータセンターが7つ。サーバー15000台、ヨーロッパに巨大なHadoop Clusterがある。35PBのBig Dataがある。   Criteo社のデータソース  商品カタログデータが広告主ごとに100万ほど。その広告主を10000社持つ。 インターネットユーザーの行動履歴のイベント数が毎日2B。 広告表示やクリックなどのイベントデータが毎日20B。   Recommend Logic  やりたいこととしてはRecommendの関数にインターネットユーザーを入れた時に一番のオススメ商品が出るようにしたいが、実際のところ複数の商品が出るし、新鮮さも保たなければならない。 できることとしてはOfflineで事前に商品の推薦を選んでおき、OnlineでRecommendationを決定する。 Offlineで広告主からインターネットユーザーの行動ログを送ってもらい、商品ランキング/カテゴリ商品ランキングを定め、商品閲覧の共起から類似商品を導き出し、商品購買の共起から補足商品を推薦する。 インターネットユーザーが商品Xを閲覧した時の推薦する候補は以下のもの  閲覧した商品Xそのもの。(直接商品) 他のインターネットユーザーが商品Xと閲覧共起しているもの。(類似商品) 他のインターネットユーザーが商品Xを閲覧した上で購入したもの。(補足商品) 広告主のサイトで最も見られて/買われているもの。(売れ筋商品)    System Overview  4時間おきに広告主から商品カタログデータを貰い、Memcachedに入れる。 広告主サイト側でのインターネットユーザーのイベントログを送ってもらい、商品アイテム推薦を導き出すためにHadop MapReduceにかける。MapReduceされた推薦データは12時間おきにMemcachedに入れられる。 広告表示/クリック/コンバージョンデータを6時間おきに取得し、それを基に予測モデルをHadoop上で更新する。 Recommendのリクエストが来るたびに、インターネットユーザーのデータを用いて推薦アイテムを商品カタログから参照する。 機械学習の予測モデル(おそらくクリックやコンバージョン予測)はLogistic Regressionを利用している。なぜならばLRはスケールするし、速いし、たくさんの特徴を扱うことができる。(bitのhash magicを使うなどして) Memcachedに入れられた類似商品、最も見られている商品、最も買われている商品をスコアリングして高いものから推薦する。   Recommend Logicの評価  オンラインABテストを行う際はインターネットユーザーの50%ずつにそれぞれのモデルを適用するが、これらは以下の理由により煩わしい。  すぐに多くのお金を失う。 テストの期間が長くなってしまう。信頼がおける期間は2週間ぐらい必要。 ソースコードを本番同様にしなければならない。 インフラ的に高負荷になってしまう。 広告主のアカウントがそこまで十分な時間をくれない。  その代わりとしてOnlineのログを使ったテストフレームワークがある。 しかし自分たちで管理するデータしかもっていない。(外部データは非常に高価なため) それでも私達は完全な間違いをしないことの確信がある。 Onlineのパフォーマンスを上げるためにOfflineテストの最善策を見つけなければならない。以下は参考論文 Counterfactual Reasoning and Learning Systems    今後のUpdate  より長期的なUserのProfileデータを活用する。 より多く/より良い商品情報を利用する。(画像、文脈、NLP) 類似商品計算のUpdate。 商品スコアリングの結合。 私たちはClick予測のデータを公開している。  4GB display-click data : Kaggle challenge in 2014 http://bit.</description>
    </item>
    
    <item>
      <title>アジャイル開発の導入に向けた備忘録</title>
      <link>https://yutakikuchi.github.io/post/201508230213/</link>
      <pubDate>Sun, 23 Aug 2015 02:13:40 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201508230213/</guid>
      <description>[etc] : アジャイル開発の導入に向けた備忘録 SCRUM BOOT CAMP THE BOOK
作者: 西村直人,永瀬美穂,吉羽龍太郎出版社/メーカー: 翔泳社発売日: 2013/02/13メディア: 単行本（ソフトカバー）購入: 5人 クリック: 13回この商品を含むブログ (33件) を見るアジャイルサムライ――達人開発者への道
作者: Jonathan Rasmusson出版社/メーカー: オーム社発売日: 2014/06/25メディア: Kindle版この商品を含むブログ (4件) を見る

SCRUM BOOT CAMP と アジャイルサムライ  このブログも勉強したことの纏めサイト的なものになってしまってきているので、体験談や自分の考えも纏められるようにしていきたいですね、、、。今回はアジャイル開発手法の1つであるSCRUMについて計画,実施,振り返りを行う機会があったので、新しいインプットとしてSCRUM BOOT CAMPとアジャイルサムライを読んでみました。過去に独自色の強いアジャイルを試したことはありましたが、コミュニケーション時間の増加および課題の明確化ができるたものの、アジャイルの学習や進行に費やした時間に対するプロダクト成果まで結びつかず途中で挫折しました。その経験からチームビルドを半ば諦めて個人の力で打開する雰囲気作りをしていた節があったので、これを機にもう一度教科書を読んでから考え直してから実践したいと思います。掲載している内容は自分の言葉で置き換えておりますのでご注意ください。
SCRUM BOOT CAMP  事前に全てを予測するのは不可能。そこから計画することはできない。 という事が前提。 要求の分析や設計には時間をかけず、重要なものやリスクが高いものから先に作る。 要求の実現順、問題点を明確化、もっと良いやり方の適応などを意識する事。 開発チームの規模は3〜9人ぐらいがベスト。 スプリントは短くて1週間、長ければ4週間。週単位で区切る場合が多い。 スプリントにおいて開発する量は過去の実績値(velocity)を元に予測。 スプリント計画で合意した内容に全力を尽くすが、完了を約束するわけではない。 スプリント単位でリリース判断可能なプロダクトが求められる。完了の定義を事前に明確化し、途中での削除は避ける。 スプリント内では毎日の状況確認。 スプリント内で出来上がった動作するプロダクトを確認する。スプリントレビューはプロダクトオーナーに対してデモを披露する。 スプリントの最後に振り返り(スプリントレトロスペクティブ)を行い、もっと成果を出すためのプランを出す。 プロダクトオーナーとスクラムマスターの兼任は避けたほうが良い。プロダクトオーナーは製品を良くすることを考え、スクラムマスターは開発を円滑に進める。 プロダクトバックログのためにインセプションデッキを作成する。インセプションデッキのテンプレートは以下にある。  https://github.com/agile-samurai-ja/support/tree/master/blank-inception-deck  プロダクトバックログは全員が必ず理解する。 プロダクトバックログの工数や予算は実際に作業する担当者が見積もる。見積のズレは必ず発生するので、時間をかけずに行う。基準となる作業を見つけて相対的な見積をする。当てずっぽうの見積の中でベストを尽くす。 スプリント内で実現しようとするプロダクトバックログの量が多いと大変で良くない傾向なので、確実に終わらせる事ができる項目数を定義する。 デイリースクラムが単純な進捗報告会とならないように、問題を見つけるように工夫する。問題が出てきたらデイリースクラムの後に関係者で集まって対応を考える。 スプリントレビューではデモを目で見て、本当はどうすべきかを確認する。デモと完了の定義を比較して終わっているかも合わせて確認する。完了の定義はチーム内で合意をしていないといけない。 問題に対しては個人でなんとかしようとしがちなので、チーム全員で対応すべき。 タスクボードを使ってTodo,Doing,Doneを分かりやすくする。 バーンダウンチャートを使って縦軸に見積もり時間の合計、横軸に営業日を記録し、最終日が0になるような折れ線グラフを作る。これが理想線となり、実際のタスクの実行時間をプロットして理想線と比較する。 スプリントを旨く回すために自分たちでルールを決めても良い。 velocityに求められのは安定性。安定していないvelocityは予測に使えない。velocityを上げていくことだけを意識していくと悪影響がでる。 みんなをサポートしてプロジェクトの目的を達成していくのがスクラムマスターの仕事、サーヴァントリーダシップと呼ばれる。またスクラムマスターはあらゆる妨害を見つけて取り除く。 良いスクラムマスターだと妨害リストに50個以上項目を管理している。妨害リストはチームが見えるところに貼っておくと良い。 プロダクトバックログは日々更新されるべきで、いろいろな意見が集まるように誰でも更新できるようにしておくと良い。プロダクトバックログを観て削除できるものは優先度を下げたり諦めるなどをする。 開発メンバーのプロジェクトを進めるスキルがあるかどうかを判断するためにスキルマップを書いてみると良い。開発メンバーの性格や得意/不得意を知った上で各メンバーとこれまでどんなプロジェクトにいたか、仕事で重要視すること、期待されていることは何かを話し合っておくことも重要。 失敗を重ねて学んで行く。成長しないスクラムチームではプロジェクトがうまくいかない。失敗を繰り返さないように学んでいく。 通常のスプリントの後でリリースの前にリリーススプリントをやることも初期のスクラムチームでは採用される。リリースに必要なことを片付けるための期間。   アジャイルサムライ  チームの持てる力を最大限に出すことでプロジェクトの成功確率を向上させる。 本当に価値があるものに対して集中し、余分なものは捨てて身軽になる。 アジャイル開発は気弱な人には合わず、隠すことが無い、質の高い仕事へ情熱を持つこと、価値を生み出すことを期待するのであるならアジャイルを採用することによって得ることは多い。 プロジェクトの3つの真実。1.</description>
    </item>
    
    <item>
      <title>WORK RULES! はGoogleでの採用やマネジメントが分かる良本</title>
      <link>https://yutakikuchi.github.io/post/201508100206/</link>
      <pubDate>Mon, 10 Aug 2015 02:06:10 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201508100206/</guid>
      <description> [etc] : WORK RULES! はGoogleでの採用やマネジメントが分かる良本 ワーク・ルールズ！―君の生き方とリーダーシップを変える
作者: ラズロ・ボック,鬼澤　忍,矢羽野　薫出版社/メーカー: 東洋経済新報社発売日: 2015/07/31メディア: 単行本この商品を含むブログを見る
内容の覚書  Googleにいらっしゃる人事担当/上級副社長？のラズロボックさんが書いた本。最近の僕のtopicsはチームメンバーと自分が同じ方向性で共に成長できるかどうかということ、それに対する経営者やリーダーやマネージャーとしてのあり方というものであり、この本でもその点に注意して読んでみた。訳者の意訳力もあるのか、かなりスラスラ読める本なのでお薦め。忙しい人は冒頭と14章だけ読んでみると良いかもしれない。最高の人材(人財)を採用しチームとして機能させるための並々ならぬ努力が読み取れる。
 マネージャーはチームに奉仕し、現場の障害を取り除いてチームが躍動することに集中する。 最高の創業者は他の創業者が自分と並び立つ余地を生み出す。会社員でも自分を創業者とみなし行動する。 ミッションが個人の仕事に影響を与えるのは事業目的ではなく、道徳だからである。歴史上大きな力を振る舞った運動は道徳的な動機を持っていた。ミッションへの信頼が共有されているためほとんどのグーグラーは結束している。 社員の発言や意思決定を重要視する。(余談:中国での検索結果検閲の判断についても社員からの意見に耳を傾けた。www.google.cnはサービスを閉じたが、www.google.hkは継続している) 一般的な企業の場合は社員教育に資金や時間など力を注ぐが、Googleの場合は上位10%に入る求職者を採用し、その後の手間をかけずに済ませる。採用費用は平均的な企業の2倍ほど。 何らかの点で自分より優れた人だけを雇う。 アイビーリーグの平均/平均以上の卒業生より、州立学校をトップで卒業した聡明で勤勉な学生を採用したい。過去にやり遂げたことをに比べてどこの大学を出たかはさほど重要ではない。 単純に賢いからという理由で採用するのではなく、一緒に働く全ての人を成功に導いてくれる人を採用する。 本当に優秀な人は仕事を探していない、在籍している会社に満足しているし十分な報酬を得ている。そこでテクノロジーの力を利用して最高の人材を探し出すヘッドハンティングを行う。(gHire) 面接者は最初の5分間で評価を行い、それ以降の時間は最初の評価を確認するための時間として費やす。 人の職務能力を予測するための最善の指標は日々の業務内容と似た仕事(ワークサンプルテスト)を与えること。 自己複製採用マシーンを作成するための4つの事。1. 採用候補者の人材の質/基準を高く設定する。2. 社員が自分自身で採用候補者を見つける。3. 採用候補者を客観的に評価する。4. 採用候補者に入社すべき理由を伝える。 人材の質を確保するためにはあらゆる圧力には屈せず闘う。 「権力は腐敗する。絶対的な権力は絶対腐敗する。」権力を持つものは持たないものより道徳的規範を持たなければならない。 マネージャーの権限が少なければ少ないほどチームが取り入れるべき自由裁量の余地は増えることになる。 マイクロマネジメントはミスマネジメント。 常識はずれの野心的目標を設定して失敗したとしても、きっと何か素晴らしいことをやり遂げたことなる。 Googleでは各個人のOKRを確認することができる。 人間の本質を変えるためのマネジメントではなく、仕事を変えるためには何が必要かということ。 マネージャーの仕事は人々が働く意味を見つけることをサポートすること。 スター社員を分析して他の社員より成功している理由を見つける。 業績が低い人への思いやりも忘れてはいけない。採用のプロセスが適切ならその人への役割の与え方が間違っている。学習を支援して新しい役割を見つける。 人生で最低/最高の瞬間に出くわしたとしても自分より寛大な組織が後ろに居てくれると分かるだけで慰めになる。 評価決定前にマネージャー陣が集まって部下の最終評価を検討するキャリブレーションがあるので公平な評価が可能。 社員の評価においてボトムテールの人々の改善に投資をすることはチームの改善に繋がる。それでも業績を上げない場合は退職して別の会社で成功を収める。 パフォーマンスの高さは状況に依存するので、他の職場でうまくいっていても新しい環境でうまくいくとは限らない。 エンジニアは管理されることを嫌う。彼らはマネージャーの事が嫌いだし、マネージャーになりたくないと思っていことも確か。 優れたマネージャーの8つの属性。1. 良いコーチであること。2. マイクロマネジメントをしないこと。3. メンバーの成功や満足度に関心を示すこと。4. 生産性/成果思考であること。5. 話を聞いて、情報を共有すること。6. チームメンバーのキャリア開発を支援。7. 明確な構成と戦略を持つこと。8. チームに対して助言できる技術スキルを持っていること。この中でも技術的な専門知識は重要度が一番低い。 ある分野で専門的な知識を得るためには1万時間が必要。ただし1万時間の長さではなく、どう使うかが重要。少しでも確実にのこるものを学ばせるほうが賢い投資。 フィードバックのない反復はモチベーションが向上しないし進歩しない。組織やチームの学習効率をあげる方法の一つとして学習するスキルを細かい要素に分けて具体的なフィードバックに返すこと。更には学習がもたらした結果を測定する。 最善の学習方法は教えること。 外発的動機づけの報酬体系を洗練させ、4つの原則が生まれる。1, 報酬は不公平。2. 報酬ではなく成果を称える。3. 愛を伝え合う環境づくり。4. 思慮深い失敗に報いる。 最も優秀な人でも失敗するので、失敗にどう対応するかが問題になる。またリスクを賞賛しないと誰もリスクを取らなくなる。 社会的構造の隙間にいる人ほど素晴らしいアイディアを持っている可能性が高い。 意思決定のエネルギーはここぞという時に使うべき。自分をパターン化すること。些細な事に気をとられていたら仕事が終わらない。    </description>
    </item>
    
    <item>
      <title>RecommendSystem成果へのポインタ</title>
      <link>https://yutakikuchi.github.io/post/201504261051/</link>
      <pubDate>Sun, 26 Apr 2015 10:51:01 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201504261051/</guid>
      <description> [etc] : RecommendSystem成果へのポインタ </description>
    </item>
    
    <item>
      <title>FC2動画で非会員による1日の視聴制限を解除するBookmarklet</title>
      <link>https://yutakikuchi.github.io/post/201502091113/</link>
      <pubDate>Mon, 09 Feb 2015 11:13:24 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201502091113/</guid>
      <description>[javascript] : FC2動画で非会員による1日の視聴制限を解除するBookmarklet おしまい</description>
    </item>
    
    <item>
      <title>WebCrawler in Scala</title>
      <link>https://yutakikuchi.github.io/post/201412302237/</link>
      <pubDate>Tue, 30 Dec 2014 22:37:46 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201412302237/</guid>
      <description>[etc] : WebCrawler in Scala Crawler in Scala  検索Crawlerを作る - Web就活日記 
以前はnutchを使ったcrawlerを試してみましたが、今回はcrawler自体をscalaで書いているものをまとめようと思います。インターネットで紹介されているものの中には全然使えないものもあったりするので、選択には気をつけてください。個人的にはまとめた結果からJoup、HtmlUnitDriverが記述や設定が簡単で手軽に実行できるという点でお薦めしたいツールになっています。
nomad denigma/nomad 
JDK/JRE7、MongoDB、Debianを必要とします。これによって私はテストしませんでしたが。sourceの更新も2年前で止まってしまっていますね。。application.conf、filters.groovy、seeds.txtの3つのファイルを記述するだけで簡単に動かせて、結果をMongo上で確認できるようなのでDebianUserは試してみると良いかもしれません。
 simplewebcrawler Srirangan/simplewebcrawler 
ScalaとGridGainを必要とします。GridGainでjobを分割することで並列分散処理をやっているみたい。GridGainを使うというニッチ派は聞いた事が無いので、何かしら問題が発生した時に大変そうです。
 fureteur gip/fureteur 
おそらく紹介する中で一番作りこまれている印象を受けました。RabbitMQを利用した分散Crawlingを実現、akkaのactorでmoduleを実装、スケジューリングを管理してくれる、sbtで用意されているので簡単に利用できる等の特徴があります。本気でCrawlerを作りたい人向けですかね。
 crawler bplawler/crawler 
指定したURLから再帰的にリンクを見つけてCrawlしてくれます。ただしドキュメントがほとんど無く初心者には設定がよくわからない、READMEの記述が誤っている、複雑なTagのパースがうまくできない(？)、様々なサイトのリンクを取得しようとするとExceptionがたくさん出て処理が進まないなど使用するのは困難と言えます。評価出来る内容としてはsbtによるbuild管理と、少ないコード記述でcrawlerができてしまうこと。以下は簡単に導入の内容を記載しておきます。
$ git clone https://github.com/bplawler/crawler.git $ cd crawler $ cp build.sbt build.sbt.bak $ vi build.sbt $ diff -u build.sbt build.sbt.bak --- build.sbt 2014-12-30 09:17:54.895512815 +0900 +++ build.sbt.bak 2014-12-30 09:48:18.683566361 +0900 @@ -8,8 +8,6 @@ scalacOptions := Seq(&#34;-deprecation&#34;, &#34;</description>
    </item>
    
    <item>
      <title>データサイエンティスト養成読本</title>
      <link>https://yutakikuchi.github.io/post/201412291448/</link>
      <pubDate>Mon, 29 Dec 2014 14:48:14 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201412291448/</guid>
      <description>[etc] : データサイエンティスト養成読本 データサイエンティスト養成読本 R活用編 【ビジネスデータ分析の現場で役立つ知識が満載! 】 (Software Design plus)
作者: 酒巻隆治,里洋平,市川太祐,福島真太朗,安部晃生,和田計也,久本空海,西薗良太出版社/メーカー: 技術評論社発売日: 2014/12/12メディア: 大型本この商品を含むブログ (1件) を見る
データサイエンス  参加したイベントで頂いた「データサイエンティスト養成読本 R活用編」の冒頭を中心に読んでみました。前に出ていた同等の養成本よりデータサイエンティスト養成の問題と本書の位置づけがはっきりしていて読みやすかったと思います。21世紀で最もセクシーな職業という噂のデータサイエンティストについて、本の中で重要だと思った内容の意訳をメモしておきます。(あまりデータサイエンティストの話はまとめてなく、結局は統計の話ばかりメモしてしまっています。すみません。)
 アカデミック領域と異なるビジネス領域でのノウハウの少なさ。 データの前処理のような泥臭い話は書籍やWebには出てこない。 使用するツールに拘りを持たず、効率を重視して使い分けが必要。 ビジネスへの意思決定にも携わるため、新卒からいきなり経験させてもらえない。 3つのキャリアタイプがあり、組織の上では各キャリアタイプの人材の協力が必要。  ビジネス寄りのタイプ(ユーザーの気持ち理解) 統計理論に強いタイプ(統計解析) エンジニアリングに強いタイプ(データ処理のコードを書く)  R言語はメモリに載り切らないデータ量、リアルタイム処理には不向き。 統計学の全体像  記述統計学  真ん中 = 平均、中央値。 構成 = 比率。 ばらつき = 分散、標準偏差。  推測統計学  全体の真ん中が同じ確率 = t検定。 全体の構成が同じ確率 = χ二乗検定。 全体のばらつきが同じ確率 = F検定。 関係から全体分け = コレスポンデンス分析、主成分分析、因子分析、クラスタ分析。 関係から全体を当てる = 重回帰分析、判別分析。   標準偏差は分散の平方根。 検定と一部のデータを利用してもデータ感に差があると言えるかを確認する手法。差がない確率が一定未満の場合は差があると判定。 クラスタリングには階層的、非階層的がある。非階層的な例としてk-meansがある。 アソシエーション分析を行って、有益なルールを探す。 決定木はツリー構造で可視化できるのでわかりやすい、CARTが有名。CARTはジニ係数でデータを分割。ジニ係数は0〜1の値、0だと平等、1だと不平等。 機械学習は目的変数が離散型か連続値かによってクラス分類、数値予測に分けられる。 クラス分類ではデータ数が少ない時に精度が最も高いとされるSVMが有名。 ランダムフォレストはアンサンブル学習(集団学習)により分類/回帰を行う。アンサンブル学習は弱学習機をたくさん作って結果の多数決を取る。ランダムフォレストは数値予測にも利用可能。 線形回帰は目的変数を直線的な関係で予測。評価指標にはRMSE(平均2乗誤差)、決定係数などを利用する。 以上が第一、二章の要約でした。個人的にもデータサイエンティストは何でも出来る人として見られる事が多く、データの分析からビジネスの未来を考えられる人と思われる事が多いように感じていますが、データサイエンティスト側としてはそれは要求レベルが高過ぎることで、決して一人ではできるような内容ではないと言えます。エンジニアにもネットワークエンジニア、Webアプリケーションエンジニア(サーバーサイドエンジニア、フロントエンジニア)、スマートフォンアプリエンジニアなど細かく定義できるようにデータサイエンティストにも当然得意不得意な領域があって、誰しもがコンサルタントやマーケッターなどの領域をカバーできないという世間一般の認識を少し変えていきたいですね。</description>
    </item>
    
    <item>
      <title>What is spray?</title>
      <link>https://yutakikuchi.github.io/post/201412240032/</link>
      <pubDate>Wed, 24 Dec 2014 00:32:05 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201412240032/</guid>
      <description>[etc] : What is spray? What is spray  spray | Introduction » What is spray? 
adtech × scala勉強会でsprayの事例が幾つかあったので軽く触ってみることにしました。上のWhat-is-sprayで紹介されている内容をここでも簡単に記述しておくと以下のような感じです。2014.12.23日現在、最新のVersionは1.3.2のようです。尚、本記事の実験環境はCentOS Linux release 7.0.1406 (Core)で動かしています。
 Akka上で動く、HTTP/RESTをサポートしたクライント/サーバーサイドのライブラリ。 JVMのlayerを触りたい場合もレガシーなJavaの実装が必要なくscalaのAPIの記述で事が足りる。 sprayはフレームワークというよりはライブラリ志向で作られている(哲学的に)。 完全非同期、Non-Blocking。 ActorとFutureベース。 ハイパフォーマンス、軽量、テスト可能。 twirlというtemplate engineもある。 中心となるspray-io、インメモリのキャッシュであるspray-caching、spray-io上でClient/Serverとして動くspray-can、Servlet上で動かすためのspray-servletなどのモジュールが用意されている。    Getting Start  Hello world Getting Started with sbt — Installing sbt on Linux 
まずはscalaのbuild toolをinstall、次にsprayを動かしてみます。プロジェクト名はhello-worldとします。以下の実行によりHTMLを吐き出すところまでは出来ました。spray-templateはspray-can上でspray-routingを実行するためのtemplateなようです。
$ wget https://dl.bintray.com/sbt/rpm/sbt-0.13.7.rpm $ sudo yum localinstall sbt-0.13.7.rpm -y $ git clone git://github.com/spray/spray-template.git hello-world $ cd hello-world $ sbt  test  re-start $ curl &#34;</description>
    </item>
    
    <item>
      <title>DMP vs DSP : CookieとDataのSync</title>
      <link>https://yutakikuchi.github.io/post/201411190121/</link>
      <pubDate>Wed, 19 Nov 2014 01:21:21 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201411190121/</guid>
      <description> [etc] : DMP vs DSP : CookieとDataのSync Syncの流れ
  How does cookie sync work between DMP and DSP? - Quora
上のQuoraにデータ分析した結果を売ってお金にしたいDMPと分析された結果を広告配信のターゲティング精度に還元してお金にしたいDSPとの間でCookieの同期とデータ分析結果の受け渡しについて良いまとめがあったので要約したいと思います。上の内容に書いてない事で僕が知識として持っていることも加えておきます。
Cookie Sync  特定のWebサイトはサイト分析やより精度の高い広告配信のためにDMPのJavaScriptタグを設置する。 設置されたDMPのJavaScriptタグはDMP側での分析サーバに送られるリクエスト以外にDSPドメインのpixelタグが含まれている。 DMP/DSPの両方にRequest処理が走り、そのResponseを受け取るのでCookieがDMP/DSPそれぞれで発行される。通常のCookieの場合はブラウザを識別できる一意のIDが付与されている。(※以下の説明ではCookie = ブラウザ識別のための一意のIDと置き換えて考えて良い。) DMPおよびDSPでCookieがそれぞれ異なるが同一のブラウザだと認識する必要があるので、Cookie対応表を作成し少なくともDMP/DSPのどちらか一方で保持する必要がある。 CookieはRFCの規格により特定ドメイン内でのみ有効なので、HttpHeaderのリクエストでCookieを別ドメインに直接渡すことはできない。 DSPのpixelが呼ばれるときにDMPのCookieをRequestパラメータとして渡せば、DSPはDSPのCookieとパラメータからDMPのCookieの両方が取得できるので、DSP側ではそれの対応表が作成することができる。もし対応表をDMP側で対応表を持つ場合はDSPのCookieをパラメータとしてDMP側のサーバにRedirectする。 もし対応表を作成するタイミングでDSPのCookieがまだ無ければ新規発行し、HttpResponseとして返す。 処理の流れのイメージは上図参照。   Data Sync  CookieのSyncはOnlineで行うのが一般的。DMPの分析データはOnlineのリアルタイムで行うと処理コストが大きいので、Offlineのバッチ処理で行う。 DMPはDMPのCookieに紐づく各種Webサイトから収集した行動履歴によりAudienceの分析を行い、結果をDSP側にバッチ処理で転送する。 DSP側は対応表からDSPのCookieに紐づくデータに変換してAudienceの分析データを利用する。    </description>
    </item>
    
    <item>
      <title>機械学習のOverfitting対策</title>
      <link>https://yutakikuchi.github.io/post/201411130140/</link>
      <pubDate>Thu, 13 Nov 2014 01:40:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201411130140/</guid>
      <description> [機械学習] : 機械学習のOverfitting対策 Overfitting対策  How can I avoid overfitting? - Quora 
機械学習で偏った学習データに適合したモデルを評価データに対して利用した場合、精度が悪い結果が得られることがあります。単純にモデルにInputする訓練データが少なかったり、局所領域に存在するデータ扱っていたり、モデルの自由度が高く複雑である事など幾つか原因が考えられ、上のQuoraで解決策について意見が書かれています。ここでは結論として書かれた内容について簡単に紹介します。
K-Fold Cross Validation 単純な解決方法としては学習時に偏ったデータに適合しすぎないように学習データをK個のまとまりに分割して、K-1個のデータを用いて学習、残りの1個を用いて評価する作業を組みわせパターン全てで行うというK-Fold Cross Validationという手法が用いられます。こうすることによってデータの偏りは防ぐことができますし、モデルの汎化性能(評価データへの適用能力)を正しく評価することも可能であり、複数のモデルからより秀でたモデルを選択する手段としても有効かと思います。
 Regularization Regularizationは過学習を抑止するためのペナルティ項で、モデルのパラメータがより複雑になればなるほど値を大きくする仕組みです。 代表的な例としてL1,L2,L1L2正則化といったものがあり、それぞれ精度やメモリの消費、スパース具合が異なります。よく目にする報告としてはL1がL2よりも計算量が少なく済む(精度が低く、スパースになりがちであるため)があります。Support Vector Machineの一つのツールであるliblinearを例にとると、学習コマンドであるtrainで学習データを指定すると同時に正則化パラメータを設定することが可能です。
Support Vector Machineは特徴ベクトルの「マージン最大化」により分類を行う手法ですが、綺麗にデータを分類できない場合にマージンの具合を決めるための定数Cというものを指定したりします。Cを大きくすると誤りを許さないハードマージン、Cを小さくするとソフトマージンで多少の誤差は許容とするという働きになります。正確にはRegularizationとは異なりますが、役割は似ているところがあります。
   </description>
    </item>
    
    <item>
      <title>Recsys2014の発表から現在のRecommend Systemの問題点を読み取る </title>
      <link>https://yutakikuchi.github.io/post/201411082018/</link>
      <pubDate>Sat, 08 Nov 2014 20:18:50 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201411082018/</guid>
      <description>[etc] : Recsys2014の発表から現在のRecommend Systemの問題点を読み取る 集合知プログラミング
作者: Toby Segaran,當山仁健,鴨澤眞夫出版社/メーカー: オライリージャパン発売日: 2008/07/25メディア: 大型本購入: 91人 クリック: 2,220回この商品を含むブログ (277件) を見る

Recsys 2014 Tutorial - The Recommender Problem Revisited  Recsys 2014 Tutorial - The Recommender Problem Revisited 
仕事でRecommenderに関わっているのでRecsys2014の最初の発表を読んで現在の問題点を再確認したいという気持ちで、内容を起こしてみます。途中に出てくる数式の理解および書き写しが大変なので、概要だけ書きます。また意味を理解するためには「機械学習の手法」と「Recommend」に対する知識をそれなりに必要とされます。
  The Recommender Problem   Recommendは古くから&#34;過去の行動履歴&#34;、&#34;他のUserとの関係性&#34;、&#34;アイテムの類似度&#34;、&#34;Context&#34;に基づいてどのようにUserがアイテムを好むかを自動的に予測する便利な関数を作る事である。 RecommendationはDataを基に前処理、モデルの学習、テストおよびValidation評価をする一般的なデータマイニング問題としてみなすことが可能。 機械学習以外の側面として、UserInterface、Systemの要件定義、セレンディピティ、多様性、気づき、説明などの要素がある。 セレンディピティは直接求められていないものを探す。Userが既に知っているアイテムを紹介してはいけず、Userを興味に近しい領域に拡張させる 様々なカテゴリジャンルのアイテムを表示する事が多様性と気づきである(意訳)  Collaborative Filterling (CF) Traditional Methods  協調フィルタリングには似ているUserベースでの予測をするものと(Personalized)、全てのUserの平均でのやり方がある(Not Personalized)。 手頃な手法でRecommendを作ることは簡単だが精度を改善することは難しい。手頃な手法と精度の間にはビジネス価値に大きな差がある。 User-Baseの協調フィルタリングはターゲットしたいアクティブUserを見つけて、近似関数によりUserを識別し、似ているUserが好むアイテムを見つけて、予測をして、TopNのアイテムをRecommendする。 Item-Baseの協調フィルタリングはターゲットしたいUserが持っているアイテムを探し、他のUserが過去に持っているアイテムのみを使ってどれほど似ているかを計算し、最も似ているk個を選択しRecommendする。 ほとんどの場合良い結果が期待される。次のことにできれば挑戦したい。user/itemの相互作用は1%以下なのでDataがsparseになる。NNはUserとアイテムの両面での数を多く必要とする。   Model-Based Collaborative Filterling  Memory-Basedの手法は予測のために全てのUser-Itemののデータベースを利用する。またNNのような統計学技術を必要とする。 Model-Basedの手法は最初にUserのModelを作成する。例えばBayesian Network、Clustering、Rule-base approace、Classification、Regression、LDA等。 Netfilix PrizeでのModel-Basedから学んだことはとても高い精度であり、改善の精度が10%あったこと。 2007年のPrizeではTop2のアルゴリズムがSVD(RMSE:0.</description>
    </item>
    
    <item>
      <title>Criteoが発表したCross Device Advertisingのreportを読む</title>
      <link>https://yutakikuchi.github.io/post/201410182232/</link>
      <pubDate>Sat, 18 Oct 2014 22:32:12 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201410182232/</guid>
      <description>[etc] : Criteoが発表したCross Device Advertisingのreportを読む アドテクノロジー プロフェッショナル養成読本 ~デジタルマーケティング時代の広告効果を最適化! (Software Design plus)
作者: 簗島亮次,佐藤裕介,松田佑樹,時吉啓司,石黒武士,小川卓出版社/メーカー: 技術評論社発売日: 2014/04/16メディア: 大型本この商品を含むブログ (4件) を見る
Cross Device Advertising  Cross-device advertising: How to navigate mobile marketing&#39;s next big opportunity | Criteo 
リターゲティング広告の分野で最強と言われているCriteoが2014年の9月に書いているCross Device Advertisingのreportについてまとめを書いておきます。そもそもCross Deviceって何よっ思った方の為にも簡単に説明を書いておきますが、個人が複数のデバイスを利用してインターネットをしているとタッチポイントが分散してしまうので、個人の興味を解析してAdを配信したいサービスにとっては断片的な行動ログをそれぞれの端末から得るより、複数のデバイスを利用している人を1個人として特定できたらいいよねって話です。現状の識別情報については1ブラウザの特定はCookie、1端末の特定はIDFA、1ユーザーの特定はUserIDで行われているのが一般的で、これらは纏めた管理ではなく基本的に独立して扱っています。UserIDを使うと特定サービス内において複数デバイスを紐付ける事も可能なんですが、UserID自体が各メディアサービス(FacebookIDやTwitterID等)の識別子であるため、メディ側での利用に限定される事とAd配信サービス側は直接知る事ができないので扱いづらいとされています。
複数デバイスの解決をテクノロジーを使って推し進める上での気をつけなければいけない点は&#34;インターネットユーザーの不快&#34;と&#34;個人情報保護&#34;とされています。個人情報についてはどこまでなら許容可能という線引きを国が握ってしまっているのでそこは法律に従うしか無い状況です。数年前から個人情報を一切利用せずにCookieの代替IDを生成する41st Parameter社のADTruthやtactadsというSolutionにも注目が集まって来ていますが、今のところ精度はまだまだという感じのようです。
AdTruthとは? | AdTruth 
Cross-device advertising solutions 
  Criteo Report / Technology and Tracking : Volume vs. Accuracy  結局最後まで読んでみて目新しい話はあまりありませんでしたが、User識別の精度が低いと広告費用や機会損失をしているというメッセージが良く伝わってきました。 簡単にまとめるとCross Deviceによる広告配信はまだまだこれからということだと思います。 一部ですが重要だと思った事を意訳メモします。
 スマートフォン、タブレット、ラップトップのようなモバイルデイバスを40億人が利用する時代である。 Cross Deviceの広告配信による収益は素晴らしい物になるはずだが、市場関係者はCross Device識別は技術的に難しいものになると考えている。なぜならばプラットフォームをまたいで分析する事が現状はできないから。 60%の人が複数のプラットフォームを利用している。それはdesktopを使っているアカウントに比べ凄く多くなっている。またTVを見ながらタブレットを使うようなケースも珍しい事ではない。 インターネットユーザーは流動的にデバイスを使い分けるのでますます難しい問題になるが、ちゃんと識別して広告を配信しないと無駄な費用を使ってしまう。間違って識別してしまうと複数人のように扱ってしまう。 インターネットユーザーは昼にPCで探して、更にスマートフォンでも探して、最終的には翌日にお家のPCで買うようなケースがあるから、市場関係者はこういった問題を解決するために複数のデバイス分析をちゃんと行い、いつどこでどれだけ商品が購入されるかを予測しないといけない。 モバイル広告をうまく展開できない3つの障壁として、ユーザー属性が取得できない、デバイスを跨ぐ消費者をうまく識別できない、サービスの最適化やパフォーマンスが低いのでコンバージョンが低いことが挙げられる。 2016年には年には消費者がモバイルを使う率が44.</description>
    </item>
    
    <item>
      <title>検索Crawlerを作る</title>
      <link>https://yutakikuchi.github.io/post/201409231746/</link>
      <pubDate>Tue, 23 Sep 2014 17:46:03 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201409231746/</guid>
      <description>[etc] : 検索Crawlerを作る Solr in Action
作者: Trey Grainger,Timothy Potter出版社/メーカー: Manning Pubns Co発売日: 2014/04/05メディア: ペーパーバックこの商品を含むブログを見る
Nutch + Solr + Hbase + Zookeeper  Nutchで特定のWebPageをCrawlingしてSolrのIndexを作ろうとした時にかなり嵌ってしまったので作業のメモを記録しておきます。(※タイトルに語弊があるようですが、検索Crawler自体を作るという話ではありません。)特にNuth/Hbase間のVersion依存があるので、installしてみたけど動かなかったという人の参考になればと思います。Webを色々と探してみるとNutch2.2.1とHbase0.90.Xを組み合わせると良いようです。僕が試してみた環境は以下のものです。因にZookeeperは個別にinstallする必要は無いようです、Hbaseを起動するとZookeeperも実行されます。
 OS : CentOS 6.4 Java : 1.7.0_51 Nutch : 2.2.1 Solr : 4.7.0 Hbase : 0.90.6 Zookeeper : 3.4.6   またNutchのTutorialに関しても注意が必要です。Webで紹介されているものは1.X系のものが多く、Crawlingのコマンドも掲載されているものをそのまま実行するとdeprecatedされていて動かなかったりします。NutchのVersionを確認して必要なTutorialを参照するようにしましょう。 特に一番したのリンクのCommandLineOptionsに記載されているVersion毎の×印を参考にすると良いでしょう。
 NutchTutorial - Nutch Wiki  Nutch2Tutorial - Nutch Wiki  CommandLineOptions - Nutch Wiki     Setup  基本的にはどれもWebから圧縮ファイルをdownloadして解凍し、設定ファイルの変更と起動コマンドの実行により動きます。ここではJavaは既に設定されているものとして話を進めます。</description>
    </item>
    
    <item>
      <title>Android Studioを入れてFacebookSDKのLogin機能を使うまでの作業記録</title>
      <link>https://yutakikuchi.github.io/post/201408130321/</link>
      <pubDate>Wed, 13 Aug 2014 03:21:55 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201408130321/</guid>
      <description>[Android] : Android Studioを入れてFacebookSDKのLogin機能を使うまでの作業記録 Android StudioではじめるAndroidプログラミング入門
作者: 掌田津耶乃出版社/メーカー: 秀和システム発売日: 2014/04メディア: 単行本この商品を含むブログ (2件) を見る
Android Studioの導入以降  柄にも無くAndroid Appliの開発に手を染め始めた@yutakikucです。
Android Appliの開発をする為にはEclipseかAndroid Studioを導入すると良いようです。ぐぐってみると断然Eclipseのドキュメントが多いようですが、EclipseはGradleというAndroid Appliをビルドするツールが導入しづらいとの事で、僕はAndroid Studioを選びました。Android Studioの導入はdotinstallに詳しく載っているので、僕と同じ初学者の方は一度参考にする事をお勧めします。Androidアプリ開発入門 (全12回) - プログラミングならドットインストール またschooでも同じような講義が公開されていますがGradleに対する説明がある冒頭だけ見れば良いと思います。その他はdotinstallでOK。Android StudioではじめるAndroidアプリケーション実践入門 - 無料動画学習｜schoo（スクー） 
dotinstallで無料会員で視聴できるのはMyActivity.Java、activity_my.xmlを編集してボタンを押下した時に画面に表示する文言を変更するという所までです。アプリの細かい開発の動画もあるようなんですが有料会員でないと利用できません... Oh...。Javaにビジネスロジック書いて取得したデータをViewに反映したいという欲求を満たす為に今日は自分で試してみた事を記録しておきます。
尚、僕が試している環境は以下の通り。
 PC : OSX 10.9.4 Android Studio : Beta 0.8 FacebookSDK : 3.17.1    FacebookSDKの設定  Facebook SDKのImport Android用Facebook SDKスタートガイド 
Getting Started with the Facebook SDK for Android (Android Studio)</description>
    </item>
    
    <item>
      <title>類似度計算と転置Indexとb-Bit Minwise Hashing</title>
      <link>https://yutakikuchi.github.io/post/201408120030/</link>
      <pubDate>Tue, 12 Aug 2014 00:30:48 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201408120030/</guid>
      <description> [調査] : 類似度計算と転置Indexとb-Bit Minwise Hashing Recommend Engineでの類似度計算  RecommendEngineを作る時の話。アイテム間の相関を計算する為にユーザーの購買データからJaccard係数やCos類似度を求める手法が一般的です(アイテム×ユーザーTableと、アイテム×アイテム相関Tableが必要)。しかしアイテムの個数(N)×ユーザー数(M)の行列を作り、Nの中から2つのアイテムを取り出してそれぞれの係数や類似度を求め、それを個数分繰り返していたら行列が大きくなる程計算が大変になります。特にアイテムの購買という行為がほとんど発生しないので、購買のベクトルがほとんど0となる疎ベクトルが作られて効率が悪く感じられます。一時期はこれを回避する為にベクトル数を減らす(購買データが多いユーザーに超超限定する)事で回避していたんですが、ユーザーが偏るしデータも少なくなってしまう事を問題として認識していました。そこでデータ数を減らすよりもっと色んな方法あるっしょって事で調べてみました。
レコメンドにおける類似度計算その傾向と対策 #DSIRNLP 第4回 2013.9.1 // Speaker Deck 
転置Indexを使う手法。特定のアイテムAを買ったUser一覧をIndexから引き、User一覧が買った商品一覧を引いて来てアイテムA以外の共起回数を計算する。この方法では共起回数の計算はそこまで大変ではなく、アイテム数とユーザー数の両方が増えても処理時間への影響が小さい(らしい)です。
b-Bit Minwise Hashing 
b-bit miniwise Hashingという手法。ハッシュ関数(MurmurHash3等)を使って2つのアイテムの全ベクトル要素に対して適用し、それぞれの最小の値が一致する確率はJaccard係数と等しいという理論から導きだされます。ハッシュ関数だけ共有すれば分散処理も行ける優れもの。b-bitというのは保存するbit数の事でMurmurHash3の下位1bitで良いようです。ただしハッシュ値の衝突が生じるので衝突確率を補正した値をJaccard係数とするようです。
自分が詳しく把握していなかったのは上の2つなんですが、他に調べていて手法が見つかったらここに纏めて行こうと思います。
  </description>
    </item>
    
    <item>
      <title>ブラウザ識別用Cookieを生成する「mod_oreore(仮)」を作ったった</title>
      <link>https://yutakikuchi.github.io/post/201408071705/</link>
      <pubDate>Thu, 07 Aug 2014 17:05:32 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201408071705/</guid>
      <description>[Apache] : ブラウザ識別用Cookieを生成する「mod_oreore(仮)」を作ったった Apacheクックブック 第2版 ―Webサーバ管理者のためのレシピ集
作者: Ken Coar,Rich Bowen,笹井崇司出版社/メーカー: オライリージャパン発売日: 2008/09/26メディア: 大型本購入: 6人 クリック: 144回この商品を含むブログ (32件) を見る
mod_oreore(仮)  ネーミングセンスが糞すぎる@yutakikucです。
アクセス履歴をLogに落として行動履歴を追いたい時はCookieに識別子を設定するのが一般的かと思います。一般的にあるCookie識別子の設定のタイミングはFWやアプリケーションのでやるというように様々パターンを見かけますが、今回はApacheのレイヤーで自動的に付与してくれるModuleを作ってみました。因に同じようなApacheModuleは幾つか存在しますが、完全なる一意性が保証されていないことやApacheのVersionで使えなかったり等、ちょっとイケテナイ感じがしたので自作してみました。※mod_oreoreとはユーザー視点で「俺だよ！俺！」っというLogに自ら足跡を残す意味で、決してオレオレ詐欺とは関係ありません。
mod_usertrack ※ 一意性に問題あり
mod_session_cookie ※ apache2.3以降で利用可能
github : mod_oreore(仮)
識別子の値にはRequestを受け付けたサーバーのIPアドレス、リクエスト時刻(タイムスタンプ:マイクロ秒)、ApacheのプロセスID、コネクションIDを重ね合わせ、最終的な出力はbase64のURLSafeな形でencodeしています。base64する前に生成した識別子を暗号化しようと思ったのですが、処理が冗長的な気がして辞めました(ソースには暗号化をそのまま残しております)。またDOS攻撃を防ぐ処理は入れていません。
  設定と確認  $ sudo yum install httpd httpd-devel openssl openssl-devel $ git clone https://github.com/yutakikuchi/apache_module.git $ cd apache_module/mod_oreore $ sudo apxs -i -a -c -I/usr/include/openssl -L/usr/lib64 -lcrypto mod_oreore.c $ sudo cp conf/oreore.conf /etc/httpd/conf.d/ $ sudo vim /etc/httpd/conf.</description>
    </item>
    
    <item>
      <title>常駐型受託開発の経験から</title>
      <link>https://yutakikuchi.github.io/post/201408011325/</link>
      <pubDate>Fri, 01 Aug 2014 13:25:16 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201408011325/</guid>
      <description> [起業] : 常駐型受託開発の経験から 受託開発の極意―変化はあなたから始まる。現場から学ぶ実践手法 (WEB+DB PRESS plusシリーズ)
作者: 岡島幸男,四六出版社/メーカー: 技術評論社発売日: 2008/04/08メディア: 単行本（ソフトカバー）購入: 25人 クリック: 1,381回この商品を含むブログ (91件) を見る
常駐型受託開発  久しぶりにブログを更新します。「常駐型受託開発」という言葉が正確かどうかも分からないけど、とりあえず取引先のシステムを作ってました@yutakikucです。とある会社様(クライント)の新規事業立ち上げが目的で、そのクライアント様のOfficeにお邪魔しながら約1年程携わらせて頂きました。結論から言うとこの経験が凄く身になってとても良かったと感じました。(※優秀なスタッフさんが沢山いらっしゃる環境で、とても親切にして頂いた事は厚くお礼を申し上げます。)
受託やらないほうが良いぜ！論を当然否定することはしませんし、逆に強く薦めもしませんが、新規事業立ち上げに必要な事を凄く近いところで経験できたのは今後自分が事業を立ち上げる事にもプラスの材料になったと思います。今日はこの開発現場で感じた受託開発とマネジメントについて記録するだけなので、内容的には糞つまらないことかもしれませんが、どなたかのお役に立てればと思います。因に僕の会社には社員がいないので、一人で出向いていました。
  常駐型受託で良かった点、残念だった点  良かった点  自分と同じような受託社長さん達と知り合う事が出来た(ここ凄く大事)。また受託開発現場というのを肌で感じれた。 自分の技術力の立ち位置を把握できた。また技術力よりも人間力/信頼力が仕事を得る材料である事も理解できた。 常駐する事で社員さんとのコミュニケーションが密にできたし、認識の大きな間違いも少なかった。 事業立ち上げフェーズから入れてもらえた事で社員さんと同等のStartUPの進め方のノウハウを身につける事ができた。 0からのスタートなので、幅広く必要なシステムの根幹を作り込んだ。またその仕組みをいち早く作れるように考える頭も身につけた。 注文請書、請求書等のやり取り作業を全部自分で経験できた。契約面で質問したい内容があれば直ぐに現場でも解消する事が出来た。   残念だった点  常駐型+時間契約だったので労働時間の対価を得るような内容になってしまった事。またそれによって時間的な拘束が多く発生してしまった事。これにより他社からの開発を受けられなくなってしまった。 受託に重きを置きすぎて自社開発に手がつけられなくなってしまった事。(僕はここの比率を途中で変更してしまったのだが、最初に決めた信念を貫く必要があると感じた。) 決められた方針に反論しづらい、また自分が思った改善策や良いアイディアを率先して現場に浸透させづらい。どうしても所詮受託の意見という考えが浮かんでしまう。 イベントや勉強会で直接的な成果を発表しづらい。発表する場合はコアな話は出来なく、概要レベルもしくは一般論に置き換えて話す必要がある。 頑張った事に対する評価についてあれこれ考えてしまう。     受託エンジニアの種別  受託エンジニアと言っても様々な背景を持った人と知り合う事ができたのでそれもプラスの材料になりました。今回の現場では受託エンジニアご本人が別の会社に所属しているか否(独立している)か、採用に紹介会社がいるのか否かという2×2の4パターンありました。凄く失礼な話かもしれませんが、この区分けによりなんとなーくのエンジニア特性が見れて取れたように思います。ここでも書けるような内容としては個人を見た場合、会社所属の人たちは要件の抽出とそれを具体化する力、独立している人達は何かやったるぜー！という野心と個性が強く見れた気がします。マネジメントをする人はご自身で特性を見つけ出して知っておくと良いかもしれません。ここはもっと詳しく書きたいんですがこの辺まで(笑)
  受託の人数  現場で社員を雇えない状況で今直ぐ対応する人が欲しい場合は受託の人数を増やすのが手っ取り早いですが、その人数が増えすぎるとマネジメントが確実に行き届かなくなります。これはどんなに優秀な社員マネージャーがいたとしても受託に対する指示を一人ずつに細かく伝えるのは不可能であり、結局のところ一番コミュニケーションが発生するのは社員マネージャと受託では無く、機能担当の社員エンジニアと受託の直接になります。この社員さんのコミュニケーションコストが半端無い。仕様の伝達、ソースコードレビュー、改修依頼等、ただでさえ忙しい社員エンジニアさんもその辺のサポートに相当な工数が掛かってしまいます。今回の経験での一つの指標として、どんなに苦しい状況でも社員エンジニアの数より受託の数を増やすことはNGであり、更に言うと理想的には受託人数が社員数の半分より小さくなるように採用すべきかと思いました。もしそれでも人数対工数の折り合いが付かないのであれば、常駐型だけではなくマネジメントを含めて外部委託会社に依頼する、そもそものリリーススケジュールを見直すか機能の削ぎ落としや簡略化を図るのが得策と思います。大事なので2度言いますが、単純に常駐型受託の人数を増やすのはマネジメントのリスクが大きくなります。あと、受託に重要な機能を任せると後々いなくなってしまった時に大変な事になるので、機能の重要度で委託するかどうかを考える事が必要だと思います。
  紹介業者の戦略  僕も最初は受託を1人採用するのは紹介会社1社を経由するケースしか無いのかと思ったのですが、実際には間に紹介会社を2,3社挟むケースもあってその分受託サラリーのマージンが引かれているようです。このケースで美味しい思いをしているのは間にいる紹介業者だけであり、発注元は金額が高くなるし現場で頑張っているエンジニアの労は報われません。人材だけでなく仲介業全般での当然の話なんでしょうけど、このシステムってなんとかならないんでしょうか。受託の士気を上げる為にも常識的なマージンであって欲しいと強く希望します。
紹介会社は既に送り出している受託の人から都度現場状況をヒアリングして、更に人数を増やせるか、そしてどれぐらいのレベルのエンジニアなら長期採用されるのかというのを把握しています。紹介会社としても当然長く現場にいて欲しいという希望があるので、今求められているスキルと新たに送り出せる人のマッチング具合が気になるのだと思います。ある程度の採用単価が保証される事も当然前提の一つですが、ちゃんとした技術基盤があり高い目標を常に持ち続けられるいい環境という情報がうまく紹介会社側に伝われば優秀なエンジニアが来てくれる可能性は高まります。逆に炎上状態やスキル面での低レベル感が伝わると当然紹介会社もハイエンドな人材を送り出したいとは思わなくなってしまう。後者に嵌ってしまうと現場としては完全なる悪循環ですよね。
  </description>
    </item>
    
    <item>
      <title>男なら潔くC言語書けよと言われた話。〜mod_db,mod_dbdの実装〜</title>
      <link>https://yutakikuchi.github.io/post/201404040828/</link>
      <pubDate>Fri, 04 Apr 2014 08:28:32 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201404040828/</guid>
      <description>[C] : 男なら潔くC言語書けよと言われた話。〜mod_db,mod_dbdの実装〜 C実践プログラミング 第3版
作者: Steve Oualline,望月康司(監訳),谷口功出版社/メーカー: オライリー・ジャパン発売日: 1998/06/15メディア: 大型本購入: 7人 クリック: 158回この商品を含むブログ (45件) を見る
恩師に言われた言葉  Geek女優の池澤あやかさんに会いたいと思っている@yutakikucです。
池澤さんはRubyが出来てSFCで女優さんなんて羨ましいですね〜。僕なんてRubyは得意じゃないし東京とは言えないような都心から離れた場所の地味な国立大だし、何よりお金も無いパンピーだしね〜。
僕の学生時代にもRubyはあったんですけどRailsはまだ出始めでそんなに流行っている雰囲気は無かったし、Webを書くには面倒くさいJSP/ServletかPerlかって感じでした。ApacheのModuleでWebを書ける事も学生ながら知っていたんですが、ポインタ、メモリの動的確保/解放の間違いが頻発して開発効率が落ちるから極力Javaで、どうしてもCを書かなければ行けない時はC++で逃げてました。
でも学生時代に恩師に言われたんですよね、JavaやPerlを書く奴はチャラいやつと。男なら潔くC言語書けよと。 (因に恩師はJavaとJavascriptの違いも良く理解していなかったと思いますけどww) おそらく恩師が言いたいのは学生の時から高級言語に頼る事無く、まずはプログラミングの仕組みを理解するためにC言語で苦労してエキスパートを目指せっていう意図だったと解釈しています。
C言語をやると他の言語と違って問題の発生率が高い。 要は言語側であまり頑張ってくれないから自分で工夫する必要があります。特に他のコンポーネントに接続する時には接続数の制御/ポートの使い切り、ConnectionPoolの実装、使用メモリのオーバー、良く分からないSegmentation Fault等必ず経験します。こういった問題解決の経験値をつける事でエンジニアの実力が養われるのだと思います。
恩師の言葉を信じてCの勉強を重ねた結果、会社の開発チームに配属された時は凄く重宝されましたし、極力PHP以外を書く案件を回して貰えました。だからCを勉強して来て良かったと思ってます。しかし今後C言語はどうなるんでしょうか... 身の回りではゲーム系、広告配信エンジニア以外は触って無さそうです。その他の言語が強力で簡単WebアプリぐらいだとCで頑張って書くとか全く無いんですよね。学生の勉強の仕組み理解と違って会社は効率化や作業時間短縮を掲げてくるのでC推しも難しいですね。
前置きが長くなりましたが、それでも僕はCを触る事が多いのでこのエントリーではC言語からのDB接続について書きたいと思います。
  DB接続  mysqlclientを使った自作ApacheModule mod_db.c mysqlclient,prepared stmtを使ったApache Moduleのmod_dbは以下になります。完全なる自作なのであまり自信が無いですが、並列テストをしてもSegmentation Faultは発生しませんでした。下のソースではbindしたいparameterとresultをmemsetで設定するところが分かりづらいのと、Pointerの設定を間違えてmemory errorを起こし易いので注意が必要です。このプログラムの駄目な点はmysql_real_connectで都度Connectionしているので、接続のOverHeadが大きいところです。実際にabでbenchmarkを取ってみるとCommand Connectが多く発生していますし、netstatでも大量のTIME_WAITが出てます。都度接続の方式で頑張る場合は、TIME_WAITが残り続けると実行Port不足に陥って処理が進まなくなる可能性があるので、/etc/sysctlのnet.ipv4.tcp_tw_recycle=1に設定してリサイクルを速く回す設定を入れるか、net.ipv4.ip_local_port_range = 1024 65535のように使用可能なPortを増やすかのどちらかが有効になりそうです。ただしsysctlの設定をチューニングする場合は十分にテストすると良いと思います。
$ sudo yum install mysql-devel -y $ sudo vim /etc/httpd/conf/httpd.conf IfModule mod_db.c DBHost localhost DBPort 3306 DBUser root DBPass root DBName helloworld DBTableName hello /IfModule $ sudo apxs -i -a -c -I /usr/include/mysql -L /usr/lib64/mysql -lmysqlclient mod_db.</description>
    </item>
    
    <item>
      <title>速いよ Java Play Framework</title>
      <link>https://yutakikuchi.github.io/post/201404010836/</link>
      <pubDate>Tue, 01 Apr 2014 08:36:33 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201404010836/</guid>
      <description>[Java] : 速いよ Java Play Framework Play Framework 2徹底入門 JavaではじめるアジャイルWeb開発
作者: 掌田津耶乃,原一浩出版社/メーカー: 翔泳社発売日: 2013/12/17メディア: 大型本この商品を含むブログ (7件) を見る
言語とFrameworkの選定  phpにはあまり魅力を感じていない@yutakikucです。本題とは関係ありませんが4.25(金)@ヒカリエのイベントに登壇します。ネタは同窓会GrowthHackとログ集計/解析の2本立てです。興味のある方はどうぞ。【ヒカ☆ラボ】同窓会GrowthHack！×データログ集計、解析！をテーマに事例をまじえお話します！ 16年ぶりの再会でも参加率6割の同窓会を開くには？Yahoo出身のエンジニアが語る、アクセスログ可視化、 ユーザ属性解析を行うためのシステム設計のコツとは？ 
Round 8 results - TechEmpower Framework Benchmarks 
さて、本題に入ります。僕がphpを書き始めたのも前職のmain言語として指定されていたことがあり、あまり書いていて楽しく無いとは思っていながらも泣く泣く仕事としてやってい感じです。過去に何回かphpのエントリー書いてますけどそれも仕事で利用する為です。前職は本当にphperの集まりでWebだけじゃなくてバッチ処理やスクリプト処理も全てphpで書こうとする姿勢を初めて見たときは驚愕しました。まぁ速くコードを書くならそれでもいいんでしょうけど。
言語とFrameworkの選定にはドキュメント量、言語のCommitterや精通者がいる、必要なライブラリや機能がある、書き易い、チームの多数決等が基準となり決定されるケースが多いと感じます。僕なら「処理速度が速い」を正義とし、それに掛け合わせる形で「書き易さ」で選ぶと思います。時間コストを考える場合、もし新しい言語とFrameworkの導入で2人月掛かったとしても(2人月掛かることが許される場合)、Daily100万PVのサイトで0.2secリクエストが速くなったとしたら1日の処理削減コストは100万*0.2/(3600*24)の2.3人日。ということは一か月で導入コストの2人月は巻き返せる事になります。更にはユーザーがアプリを使った時の満足感も上がるはずですし。
僕は今Frameworkを使わずにCを書いていてphpの3倍以上の速さが出ているので満足しているんですが、書き易さの点からは本当に最悪な状態。メモリの動的確保/解放や配列処理を本当に間違える...まぁそんな事で処理が速いとされ、C言語より書き易いJava,ScalaやGoのFrameworkを少しずつ勉強かつ紹介していけたらなと思い、今日はPlay Frameworkについて書きます。Play Frameworkはソース更新後の最初のアクセスで自動的にJavaをrebuildしてくれるようなのでコンパイルの手間が省けてとても便利です。
  Play Framework  環境の確認と設定 CentOSは6.4、Javaはjava-1.7.0-openjdk、playは2.2.2を使っています。
$ cat /etc/system-release CentOS release 6.4 (Final) $ yum list installed | grep java java-1.5.0-gcj.x86_64 1.5.0.0-29.1.el6 java-1.7.0-openjdk.x86_64 java-1.7.0-openjdk-devel.x86_64 java_cup.x86_64 1:0.10k-5.el6 @base tzdata-java.noarch 2014a-1.el6 @updates $ java -version java version &#34;</description>
    </item>
    
    <item>
      <title>SolrのSpatial Searchを試してみた</title>
      <link>https://yutakikuchi.github.io/post/201403170820/</link>
      <pubDate>Mon, 17 Mar 2014 08:20:36 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201403170820/</guid>
      <description>[検索] : SolrのSpatial Searchを試してみた 
前書き  10代の頃は(ゴースト)ライターという職業に憧れていた時期もありました@yutakikucです。
Geospatial Indexes and Queries ― MongoDB Manual 2.4.9 
MySQL :: MySQL 4.1 リファレンスマニュアル :: 10.6.1 空間インデックスの作成 
位置情報IndexをMongoDBで管理する手法については前に調査済みで、mysqlにもSpatialindexはあまり普及していない印象、ということで...今日は検索SolrのSpatial Searchについて調べてみます。最終的にはFessやNutchでWebPageをCrawlingして得た住所データをGeocodingでLat/Lngデータに変換して自前のServerにIndexingしていく事を考えており、その前段階の作業です。Solrを選ぶ理由ですがSpatial Search以外にもTermVectorでの類似度を算出してくれるMoreLikeThisという機能があり、Lat/Lngデータの掛け合わせでコンテンツを面白くSuggestすることを考えています。MoreLikeThisについても調査したら書きますね。
  Solr設定  java, tomcat6, Solr javaとtomcat、Solr本体が必要なので以下の手順でInstallです。Solrは2014.3.15現在で最新のV4.7.0を取ってきます。僕が3年程前にSolrを使っていた時はV1.*とかだったので、もう過去の記憶や記録は役立たなさそうですね...
$ sudo yum install java-1.7.0-openjdk tomcat6 --enablerepo=remi $ wget &#34;ftp://ftp.riken.jp/net/apache/lucene/solr/4.7.0/solr-4.7.0.tgz&#34; $ tar xf solr-4.7.0.tgz   Portfowarding Solrのadminツールに接続する為の設定です。僕の場合はMacでVirtualBoxを立ち上げ、HostOSからGuestOS(CentOS)に接続してSolrを使っているのでVirtualBox内のPortfowardingとGuestOS側のFireWallの設定をします。VirtualBoxでは設定=ネットワーク=ポートフォワーディングで以下の画面に辿れます。Solrのdefaultportである8983を指定しておきます。※GuestOS側のIPアドレスをifconfigで調べて設定してください。

 Firewall 下はHostOS側のFirewall設定です。
$ sudo vi /etc/sysconfig/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT -A INPUT -m state --state NEW -m tcp -p tcp --dport 80 -j ACCEPT #追加 -A INPUT -m state --state NEW -m tcp -p tcp --dport 8983 -j ACCEPT $ sudo service iptables restart   Solr-exampleの起動 Solrのexampleにある管理画面を表示してみます。tarで展開したディレクトリ以下にstart.</description>
    </item>
    
    <item>
      <title>OpenSSLの暗号処理が爆速な件</title>
      <link>https://yutakikuchi.github.io/post/201402280825/</link>
      <pubDate>Fri, 28 Feb 2014 08:25:43 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201402280825/</guid>
      <description>[C] : OpenSSLの暗号処理が爆速な件 OpenSSL―暗号・PKI・SSL/TLSライブラリの詳細―
作者: John Viega,Matt Messier,Pravir Chandra,齋藤孝道出版社/メーカー: オーム社発売日: 2004/08メディア: 単行本購入: 4人 クリック: 89回この商品を含むブログ (25件) を見る
目次   OpenSSLによる暗号 実行環境 OpenSSLによる暗号化速度 ECBとCBCの違い PHP  OpenSSLとMcrypt関数のalgorithms比較 OpenSSLとmcrypt関数のDES,AESの速度比較 Mcryptのゼロpaddingの癖  C  DES暗号 AES暗号 OpenSSLとMcryptのDES,AESの速度比較     OpenSSLによる暗号  OpenSSL日本語サイト: The Open Source toolkit for SSL/TLS 
あどてくやっている@yutakikucです。
今日はOpenSSLの共通鍵暗号について調査した内容を纏めます。OpenSSLについて特にC言語での日本語ドキュメントが少なく、あったとしても内容が古くてあまり参考にならなかったりするので色々とサンプルを上げて行きます。PHPについて記載します。
結論を先に書いておくとC,PHPともにMcryptでは無くOpenSSLを使って暗号化した方が処理効率がはるかに向上します。以下がECBモードで100万回の暗号化/復号化処理時間(sec)の表になります。速度以外の点としてはMcryptはPadding(後述)の仕様が厄介です。みなさん、OpenSSLを使いましょう！※下で挙げているソースコードにはほとんどError処理が書いていないので、コピペする人はその辺に気をつけてくださいね。コードはgithubにも上げたのでご自由にどうぞ。
Crypto/openssl at master · yutakikuchi/Crypto 
   暗号ライブラリ   暗号メソッド   言語   実行回数   暗号化処理時間   復号化処理時間   備考     OpenSSL   DES-ECB   PHP   1000000  1.</description>
    </item>
    
    <item>
      <title>【男の浪漫】身長、体重およびスリーサイズを素性とした「アレ」のサイズを推定する実験</title>
      <link>https://yutakikuchi.github.io/post/201402170828/</link>
      <pubDate>Mon, 17 Feb 2014 08:28:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201402170828/</guid>
      <description>[機械学習] : 【男の浪漫】身長、体重およびスリーサイズを素性とした「アレ」のサイズを推定する実験 目次   実験の要点 データセットの作成 実験  5-Fold Cross Validationによる精度評価 予測  結論 Github    実験の要点  アダルトフィルタ実装に向けたA○女優リストの自動抽出 + α - Yuta.Kikuchiの日記 
Multi-Class Classifier of Bra Size used as the feature value with vital statics - Yuta.Kikuchiの日記 
※本エントリーは2013年7月22日に公開したMulti-Class Classifier of Bra Size used as the feature value with vital staticsの続編になります。
好きな女性芸能人のスリーサイズは公開されているが、「アレのサイズ」(ブラサイズ)は非公開ということがよくあります。直接的なアレのサイズは公表しない風習が漂っていますよね。そんなもどかしい状態の中、世の中の男性は脳内で必ずと言って良い程アレのサイズを想像(補完)していると思います。アレのサイズを推定する先行研究？として身長を基にした「ゴールデンカノン」による理想サイズ算出という方法があるようです。あくまで理想サイズなんですが、男性陣が知りたいのは現実サイズだと思います。美少女、バストカップ数測定スクリプト 070731 
Vital Statics - Wikipedia 
一般的な話ですがスリーサイズからアレのサイズを導きだすのは難しいとされています。( BraSize = TopBust - UnderBust。UnderBust ≠ Waist ) 個人のスリーサイズからの算出は難しいのかもしれませんが、集団データから傾向の推定はできるのでは無いかというのが今回の実験のテーマです。現実のサイズを知るために身長、体重、スリーサイズを素性(特徴)とした推定実験を行います。データに利用したのはWikipediaに記載されているA○女優の方々のデータです。※当然アレのサイズのエリート属性の方々ですので、今回の実験で作成するModelが一般ぴーぽーに当てはめる事はできないと思います。アダルトフィルタ実装に向けたA○女優リストの自動抽出 + α - Yuta.</description>
    </item>
    
    <item>
      <title>ログ集計システムを自前で作る</title>
      <link>https://yutakikuchi.github.io/post/201402120828/</link>
      <pubDate>Wed, 12 Feb 2014 08:28:27 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201402120828/</guid>
      <description>[Log] : ログ集計システムを自前で作る 
Index   ログ集計システムの要件 DB設計  データ保存方針 table設計 サーバ構成  Fluentd  fluentd,fluent-plugin-mysql-bulk install td-agent.conf mysqlにデータが格納される事を確認する  集計用のバッチ その他  Table肥大化防止 可視化     ログ集計システムの要件  爆弾ログ処理班の@yutakikucです。
ログ集計システムというものを作る時に皆さんはどのように対応していますか？ 以下の候補から要件のレベルで使い分けをしている人が多いと予想しています。ざっくりの評価ですが、導入難易度、正確性、可視化、リアルタイム、長期集計、スケール、運用費用という点で評価を書いています。
   ツール   導入難易度   正確性   可視化   リアルタイム   長期集計   スケール   運用費用   リンク     GA(スタンダード)   ○   ×   ○   ○   ○   ○   ○  Google アナリティクス公式サイト - ウェブ解析とレポート機能 – Google アナリティクス      自前Hadoop   ×   ○   ×  ×   ○   △  ○  NTTデータのHadoop報告書がすごかった - 科学と非科学の迷宮      Kibana   ○   ○   ○  ○   ×   ×  ○   Kibana入門 // Speaker Deck      TresureData   ○   ○   ○   △   ○   ○  ×   数百億件のデータを30秒で解析――クラウド型DWH「Treasure Data」に新サービス - ITmedia エンタープライズ      Redshift  ○   ○   ○   △   ○   ○   ×   Amazon Redshiftではじめるビッグデータ処理入門：連載｜gihyo.</description>
    </item>
    
    <item>
      <title>7万5千円で会社を作ったった【後編】</title>
      <link>https://yutakikuchi.github.io/post/201402100142/</link>
      <pubDate>Mon, 10 Feb 2014 01:42:22 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201402100142/</guid>
      <description>[起業] : 7万5千円で会社を作ったった【後編】 LLC(合同会社)の設立・運営ができる本
作者: 五十嵐博一出版社/メーカー: 日本実業出版社発売日: 2013/02/21メディア: 単行本（ソフトカバー） クリック: 2回この商品を含むブログ (2件) を見る
7万5千円で会社を作ったった【前編】のまとめ  7万5千円で会社を作ったった【前編】 - Yuta.Kikuchiの日記 
@yutakikucです。7万5千円で合同会社を作った話の続きをします。後編では設立後の手続きについて触れます。まずは前回のまとめから。
 印鑑の扱いは注意が必要。個人の印鑑は市役所系で、法人の印鑑の扱いは法務局で登録申請や証明書の発行を行う。 印鑑は一般的に4種類必要。1.代表者個人としての印鑑/印鑑証明書、2.会社代表としての印鑑、3.会社銀行印、4.会社認め印。 資本金は一時的な資金。代表の口座に振込をして通帳を証明としてコピーする。登記時に法務局にコピーを提出し、登記後はお金は自由にしてもOK。 合同会社設立は勉強時間〜登記までを含めて1か月程見積もっておくと良い。@yutakikucは7万5千円で設立した。 設立までにやった事は次の内容。     やる事   必須   手続き場所   必要経費       会社概要の検討/決定   ○   無し   無し     会社実印の作成   ○  はんこ屋さん   3本セットで8000円     個人印鑑登録   ○  市区役所   300円だったような.</description>
    </item>
    
    <item>
      <title>7万5千円で会社を作ったった【前編】</title>
      <link>https://yutakikuchi.github.io/post/201402030751/</link>
      <pubDate>Mon, 03 Feb 2014 07:51:35 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201402030751/</guid>
      <description>[起業] : 7万5千円で会社を作ったった【前編】 LLC(合同会社)の設立・運営ができる本
作者: 五十嵐博一出版社/メーカー: 日本実業出版社発売日: 2013/02/21メディア: 単行本（ソフトカバー） クリック: 2回この商品を含むブログ (2件) を見る
退職エントリー後  Yahoo!を退職します。 - Yuta.Kikuchiの日記 
同窓会連絡のノウハウをブログに書いたら2chでdisられた@yutakikucです。
月日が経つのは早いもので「退職エントリー」たるものを昨年に書いてから7か月も経過してしまいました。Yahoo辞めてから何をやっていたのかというと、起業準備、事業プラン作成、システム開発のお手伝いやコンサルティング...等々の仕事でたくさんの人と出会い、今迄経験の無い内容もたくさん対応させていただきました。
まだココでも報告していなかったのですが2013年11月11日に起業しました。法人の種別は合同会社です。今のところ社員は僕だけです。当然起業に関する知識は0から始め、書類作成以外は全て自分で対応したのでそこそこ時間が掛かってしまいました。手続きの失敗等もいくつかあったので今日は手順や掛かった費用について経験談を書きたいと思います。
  合同会社設立の理由  会社設立の理由は人それぞれだと思いますが、僕の場合はスマフォ新規ビジネスを一緒に造って行く仲間を集めたかったのと取引の場で法人化する必要があったというのが大きな要因です。以下個人事業主と会社を区別して話しますが、ロイヤリティを持って仕事に臨むには個人事業主を束ねた集団では難しく、会社という組織の中で共感できるビジョンを造って行く必要があると思います。外部と契約を交わす時も個人事業主という立場だと倒産した時等は無限責任となっているのでリスクが大きく契約にたどり着けないという話を良く聞きます。会社の場合は有限責任なのである程度のリスクを回避する事ができます。その他出資や融資は会社の方が受け易い、ある程度の年商ラインからは会社の方が節税で有利などの話もあるので色々と調べてみると良いと思います。
合同会社 - Wikipedia 
なぜ合同会社なのかというと株式会社設立よりも安くできる、書類の手続きが少し楽、決算公告が不要という対株式会社のメリットを活かしたいと考えたためです。僕みたいな不安定な人はとにかくスモールスタートでいい。費用と手続きコストを抑えられるのはスタートアップ向きですよね。社員にロイヤリティを植え付けたいなら株式会社だろっていう意見も聞こえてきそうですが(確かに株式会社と比べると合同会社は認知度が低い)、シスコシステムズやAppleJapanも合同会社という体制でやっているんですよね〜。
  設立には1か月と7万5千円を見積もっておくと良い  一日も早く起業したい人が「やっておくこと、知っておくべきこと」読了 - Yuta.Kikuchiの日記 
手順については上のエントリーの「会社設立のためのスケジュール」という項目通りになりますが、箇条書きだと「どの役所」で「細かい何をすれば良い」のかが分からないので再度纏め直します。
特にややこしかったのが印鑑の扱いです。印鑑は4種類必要と考えておくと良いと思います。1.代表者個人としての印鑑/印鑑証明書、2.会社代表としての印鑑、3.会社銀行印、4.会社認め印。このうち登記時に必要なのは個人としての印鑑証明書と会社代表としての印鑑の2点です。個人の印鑑登録および印鑑証明書の発行は市役所や一部の出張所でできます。会社の代表者印は法務局に登記する際にあれば大丈夫です。 当然と言えば当然の話なのですが、この印鑑の違いをしっかり理解しておいた方がいいです。僕は手続きの際に最初は自宅近くの出張所に行って個人の印鑑登録を受け付けて貰らえず(※出張所も証明書を発行するだけのところがあります)、案内に従って印鑑登録ができる出張所に行き案内のアルバイト？の人に「法人関係の印鑑の扱いは全て法務局になる」という指示を受けて法務局に行き、法務局では個人の印鑑登録は全て市役所へという案内を受けて... たらい回しにされた感じの役所巡りで半日以上を潰してしまいました。2回目の出張所での僕の説明と案内の人の受け答えが食い違っている事に気づけば良かったんですが、そこまで知識が無かったので残念な結果になってしまいました。
注意したい点としては定款を紙でなく電子でやると4万円安くなります。僕は電子定款作成を行政書士の方にお願いをしてやっていただきました。法務局に登記する書類フォーマットは以下のURLにありますが、ありすぎて訳が分からないので書類作成の代行サービスを利用しました。法務省：商業・法人登記申請 
その他として資本金は自分で決めた額を自分の個人口座に振込をして通帳の表紙と1枚目と入金があるページの3枚のコピーを取れば大丈夫です。順番としては定款書類を作成した後に振込をします。コピーは法務局に登記する際に必要になります。あまり知られていない事で資本金は一時的な入金証明であり、登記が完了したら自由に出し入れしても良いようです。どうでも良い事ですが会社の設立日は登記申請した日になります。書類がミスしてやり直しになっても最初の申請した日が設立日になります。(僕は電子定款ファイルが壊れていて後日やり直しになりました。)大安や1の付く日を狙って申請する人が多いと法務局の担当の人から聞きました。印鑑は良い物だと高価なのでこだわりがある人は色々と調べてみると良いです。僕は3本で8000円のものにしました。
   やる事   必須   手続き場所   必要経費       会社概要の検討/決定   ○   無し   無し     会社実印の作成   ○  はんこ屋さん   3本セットで8000円     個人印鑑登録   ○  市区役所   300円だったような.</description>
    </item>
    
    <item>
      <title>FacebookとLINEで呼びかけ、16年ぶりの再会をした同窓会の結果報告</title>
      <link>https://yutakikuchi.github.io/post/201401060822/</link>
      <pubDate>Mon, 06 Jan 2014 08:22:09 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201401060822/</guid>
      <description> [Life] : FacebookとLINEで呼びかけ、16年ぶりの再会をした同窓会の結果報告 
16年ぶりの再会  LevelInfinity.Labという会社の代表をやっています@yutakikucです。2014年1月3日に新潟市のANAクラウンプラザホテルという素晴らしい会場にて新潟市立小新中学校同窓会を実施しました。今回は幹事代表として全員への連絡や会の企画等全てにおいて責任を持たせて頂きました。1,2次会ともに2時間半ずつの時間でしたが、正直あっという間と思える程充実した時間であり、参加者からは賞賛の嵐でした。素晴らしい時間と場所を提供していただいた会場並びにスタッフの方々には感謝の気持ちで一杯です。
最初の方針として「必ず全員に案内連絡が行き届くように」というものを掲げていたので、準備期間の9か月間、連絡に関してはとことん対応しました。結果として連絡が行き届かなかった人は2名だけでした。参加結果ですが、生徒/学年主任/担任を合せて172名中96名が参加、率は6割近くになりました。近隣の中学校の実績が2割ぐらいで居酒屋開催という話を聞いていたので、かなり高い参加率だったことが分かります。ここまでの実績を残すのには長い道のりと辛い事ばかりでした。「幹事代行サービス」というものが巷に存在する意味も良く分かりましたが、終わってしまえばやって良かったと思います。今日のエントリーでは幹事として工夫した点やその効果について書きたいと思います。
  同窓会をやる事になった経緯と東京からの旗上げ  そもそも16年ぶりに会う同窓会を実施する事になった経緯ですが、僕のところに開催して欲しい依頼が過去10年間で昨年が一番殺到したからです。昨年は30歳を迎えた歳であり、僕自身も昨年会社を辞めて起業しましたし、生徒それぞれが人生について何かしら思うタイミングがちょうど合ったのかもしれないです。
新潟の同窓会なのになんで東京在住の人が代表をやるのか？という疑問を持った方もいると思いますが、僕の中学校の問題は「適切なリーダーが新潟に存在していなかった」という一言になります。同窓生みんなが「誰かがリーダーをしてくれれば協力はする」という姿勢であったので、16年間一度も開催されずにここまで来たのだと思っています。地元のみんなは会の準備等細かい作業は率先してできたり得意ですが、全体の方針を打ち出したり意思決定をするのに時間がかかるというリーダー特性が少し弱かったようにも思います。地元の人が自ら事を起こせるようにと半年以上意識改革を促してきたので少しは浸透し始めていると思いますし、会中の僕の挨拶で次回代表は新潟在住の人を強引に指名して来たので、今回の成功事例に従って今後は地元から盛り上げて行ってくれると思います。
  葉書案内を全員に出すのは辞めた方がいいと思う理由  僕の中学校では卒業アルバムに先生/生徒の住所一覧が載っていました。「そこに案内葉書を送ればいいじゃん？」と思う方もいると思うのですが、この方法だとおそらく参加率が3割ぐらいになってしまうと思います。葉書だけの送付だと参加者はいったいどんな感じの会になるかも不安ですし、そもそもアルバムの住所に住んでいないので連絡が届かない、返信締め切りを忘れてしまう、葉書を無くすケースが多いとネットで調べました。一般的な事例だと葉書への返信は大体5割、その中の6割ぐらいが参加としても全体の参加率は3割程度になってしまいます。
個人的には葉書案内を出す人数を最小限にし、Facebook/LINE/メールで呼びかける事に注力した方が良いと思います。 日々同窓会の更新情報をアップデート出来ますし、参加者への伝達も速く、また他の人のTLやLINE上でのコミュニケーションも活性化し、皆が最近どんな感じなのか雰囲気を知る事ができます。メールもメーリングリストを使う等連絡の手間を極力少なくするための有効な手段だと思います。
ただし、Facebook/LINEを連絡の軸とする場合は「みんなの理解」が必要です。IT系の人にとっては使っていない人なんていないぐらいの常用ツールですが、他の業界の人にとってはビックリするぐらい意味不明なツールらしいです(笑)　実際に連絡の軸とする方針を打ち出した時は一部の人からの反発が酷かったです。「誰もFacebookやLINEは使わないし、なんで同窓会専用会員サイトをお前が造らないのか？」とまで言われました。会員サイトを造る為に100万ぐらいみんなから貰えたなら実行したかもしれませんが(笑)、今あるツールを旨く使いこなして手軽に連絡ができるようにしました。実際に理解を求めるためにひと月ぐらいメリットを最大限に伝えるためのメールを打ちまくりました。その頑張りもあってFacebookとLINEのグループに総計100名集まりました。 ネットワークは強力で人から人を呼ぶと2か月あれば100名集まります。
  コミュニケーションリテラシー  人をFacebook、LINEのグループに集める事は成功したのですがここで更なる問題が起ります。特にLINEで表出した問題としてはグループでの発言ルールが曖昧だと色んなコミュニケーションを取ろうとする人が出てきます。みんなとLINE上でおしゃべりしたい人は積極的に時間やタイミング関係無く発言し雑談が湧く、そうすると同窓会事務連絡のどれが大切なのかを把握できなくなってしまう。ノートとTLを旨く使いこなして大切な情報はノートへ書いたとしても、そもそもLINEの着信音が五月蝿くてイライラする等色んな問題が起きました。
最終的には目的毎にグループを用意して、連絡事項を伝えるグループ、雑談をしたいグループ等、交通整備をしました。またLINEの着信をグループでOFFにするようルールも徹底化したことで落ち着きました。更には雑談グループの長を決め、盛り上げるネタを毎日呟いてもらうなどのコンテンツ強化も行いました。この運用を7か月以上も続けたことが効いて、Facebook/LINEに集まって貰った人は8割が参加となりました。下らない話に思えるかもしれませんがここは凄く重要で、小さい問題を常に潰していく地道な努力が必要です。
参加希望の受付もFacebook/LINE/メールで行いました。Facebookはイベント機能があるのでボタン一つで意志がわかります。LINEでは参加する/参加しないをノートのコメントに記載してもらうようにしました。
  幹事のモチベーションコントロール  僕以外の幹事のモチベーションをコントロールする事は正直うまくできませんでした。幹事をやったからと言って何かを得する訳ではなく、寧ろ個別にメールや電話で連絡した返事で厳しい意見を貰う事もあったようで実らない努力をしていると考えた人もいたと思います。参加費を低く抑えないと参加率も低くなるので、幹事への報償も無しで対応しました。最終的には同窓会への成功が最大の喜びに成るとは幹事全員が感じていたものの、一般参加者との温度差もあったりしてやる気を継続できませんでした。
ただ代表は何があっても成功に向けて突き進むぐらいの強い意志がある人で無いと駄目だと思います。最終的には全てを背負わないといけないので、途中で投げ出すような人であれば会自体を中止にするか直ぐに交代した方が良さそうです。
また幹事の打ち合わせも対面でやる必要もありました。幹事同士の意識合わせもLINEでやっていましたが、ちゃんと顔を合せて話し合わないとお互いの考えが伝わらない場面も多くありました。
  実家訪問  連絡が全く取れなかった方への対応は実家訪問も合せて行いました。そこまでするか？という意見もあったと思いますが誠意を見せたかっただけです。僕も東京から新潟に帰省して自宅を訪問しましたが結果として誰一人本人に会う事は出来ず、親御様に事情を全部説明して東京に戻ってきました。実家訪問も最終的には成果として実りませんでしたが、個人的にはやり切った満足感を得る事はできました。
  参加費受付用口座作成  当日の受付でのお金のやり取り時間を減らしたり、払った/払わなかったの意識違いを生まないようにするために参加者には事前にこちらで作成した銀行口座に振り込んでもらいました。これはかなり有効でした。多くの人が振込締め切り期限のギリギリにならないと対応してくれなかったという自体は発生しましたが、振込をしなかった人はいませんでした。振込手数料が200円ぐらい掛かってしまいますが、有効な手段であったことは間違いないです。当日100名のお金を1次会、2次会ともに預かるのは難しいと思います。口座を作るのが面倒で無ければお勧めします。
  会当日  分刻みで時間を管理するぐらいの綿密な進行表を作りましたが、100名近い参加者を誘導したり説明したり等うまく行かなかったことがほとんどです。ちょっとした練習しかしないで本番に臨むのであれば正確な進行表は不要で、むしろ参加者に自由にやってもらった方が良いと思いました。2時間半の会場確保では短すぎるというのが正直なところで、本当に一瞬です。費用や会場都合もあると思いますが3時間以上は確保したいですね。
話に夢中になりすぎて料理に全く手がついていませんでした。僕たちは人数-15人分ぐらいでホテル側に依頼し、会中も料理を積極的に食べるよう促しましたが、それでも大量に余ってしまいました。あまりにも料理人数を削りすぎると逆にホテル側から断られるケースもありそうなので塩梅が難しいところだと思います。
  まとめ   同窓会の案内をFacebook/LINEを使って積極的に行うのは得策だと思います。 Facebook/LINEの導入時での説明がうまく対応し、コミュニケーションのルールを決めておけば自動的に盛り上がれる環境を作り出す事ができると思います。 Facebook/LINEのグループに参加してくれた人の8割が同窓会にも参加してくれました。 お金の管理を口座振込にすると当日の集金が不要となり、振込履歴が通帳に残るので安心。 料理人数はできるかぎり抑えたいですが、会場の都合もあるので良く相談した方がいいです。    </description>
    </item>
    
    <item>
      <title>「DSP/RTBオーディエンスターゲティング入門」読了</title>
      <link>https://yutakikuchi.github.io/post/201312110756/</link>
      <pubDate>Wed, 11 Dec 2013 07:56:17 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201312110756/</guid>
      <description>[AD] : 「DSP/RTBオーディエンスターゲティング入門」読了 DSP/RTBオーディエンスターゲティング入門 ビッグデータ時代に実現する「枠」から「人」への広告革命 (Next Publishing)
作者: 横山隆治,菅原健一,楳田良輝出版社/メーカー: インプレスR&amp;D発売日: 2012/05/25メディア: オンデマンド (ペーパーバック)購入: 4人 クリック: 58回この商品を含むブログ (10件) を見る
DSP/RTBオーディエンスターゲティング入門読了  あどてくやってます@yutakikucです。
今日は帰省中の新幹線で読んだ「DSP/RTBオーディエンスターゲティング入門」についてのまとめを書きたいと思います。すごく基礎的な事しか書いてなかったり同じ説明が何度も繰り返されたりしていますが、あどてくやあどまーけてぃんぐに関わっている人は読んでおいて損はないかと思いました。一言で本の内容をまとめると「DSPを導入するとオーディエンスデータが見えるから施策が打ちやすいよ！欲求施策にはリタゲが効果的だよ！」って感じかなと。個人的にはもう少しDSPの予測技術について内容を書いて欲しかったなぁというところもあったりしました。下記で記載する内容はかなり噛み砕いているのと、最後の章を記載していません。また初版が2012年5月という情報なのでそこには注意してください。
  まとめ  Chapter 1-1 進化した広告配信  広告は「枠」から「人」へパラダイムシフトしている。 初期のネット広告は期間保障で特定に1社の画像をベタ張りしていた。サイトのPV=掲載料という仕組み。 その次に登場したのが各種メディアが作ったアドサーバー。Webページからアドサーバーの広告を読み込むタグを埋め込む。広告を全てアドサーバーで管理。 複数の掲載面と掲載場所に広告を配信するアドネットワークの登場。広告営業、配信管理、レポート業務を全て代行してくれるのでメディアはサイト作りに専念。 第三者配信サーバーは一つの広告主キャンペーンが複数のサイトにまたがる場合でも一箇所で管理できる。 第三者配信の利点は広告主側のサイトへの流入管理をしやすい、ポストインプレッションを捕らえやすい、SEMと統合的に流入管理がしやすい。 第三者配信はクリエイティブの評価もしやすい。その結果最適化もしやすい。   Chapter 1-2 DSP/RTBの基本的な仕組み  DSP(Demand Side Platform)とは広告主などの広告を張りたい側のPF。SSP(Suply Side Platform)は媒体社が使うPF。 DSPの入札とSSPの応札を1Imp毎に瞬時に行うのがRTB(Real Time Bidding)。 ユーザーが訪れた媒体はまずSSPにRequestする。SSPはDSPにユーザーID、IPアドレス、ブラウザ、OS、掲載先ドメイン、カテゴリ、広告枠ID、広告サイズ、許可広告主、その他業種などの情報をRequestしている。 DSPはBid Requestに対して条件に一致する広告を探し、SSPに対して金額を含めたBid Responseを返す。  1Impについてユーザーが媒体にアクセスした瞬間にBid Requestによって買う/売るをRealTimeでやり取りしている。 SSPは複数のDSPに対してBid Requestを行うので、DSP側としては他のDSPに条件で勝つ必要がある。 こういった仕組みがでてきた背景にはユーザーの行動履歴を分析できるようになったこと、膨大な処理を一瞬で行うコンピューティングの向上がある。   Chaper 1-3 広告の価格はどのように決まるか  従来の枠に対する広告販売と管理はコストがかかる。 DSP/RTBは売り手/買い手ともに都合が良いエコシステム。買いたい側は買う側の理屈に合わせて、売りたい側も広く受注を受けて最適なものを選択できるから。 DSP/RTBが生まれた背景にはリーマンショックで失業した金融工学のエンジニアが広告業界に転職したこともある。   Chapter 1-4 トレーディングデスクの業務 ※たいした事書いてないので飛ばし。</description>
    </item>
    
    <item>
      <title>Redisにマルチプロセスで接続する時に気をつけたい事</title>
      <link>https://yutakikuchi.github.io/post/201312060824/</link>
      <pubDate>Fri, 06 Dec 2013 08:24:36 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201312060824/</guid>
      <description>[Redis] : Redisにマルチプロセスで接続する時に気をつけたい事 Redis in Action
作者: Josiah L. Carlson,Salvatore Sanfilippo出版社/メーカー: Manning Pubns Co発売日: 2013/06/25メディア: ペーパーバック クリック: 1回この商品を含むブログを見る
Redis  広告配信やっています@yutakikucです。
Redisの内部処理が1スレッドで受けているようなので、マルチプロセスからRedisに書き込み処理を大量に流した時にどうなるのかを検証してみました。言語はCを、Libraryはhiredisを使います。redis/hiredis 
hiredisを使って単一プロセスで実行した場合と、Apache Moduleにhiredisを組み込んでマルチプロセスの実行状態で検証します。検証機はCentOS6.4です。
  hiredis  Redisのinstall、version確認、起動 RedisのVersionは2.4.10です。
$ sudo yum install redis -y $ redis-server -v Redis server version 2.4.10 (00000000:0) $ sudo chkconfig --add redis $ sudo /etc/init.d/redis start   hiredisのinstallとテストプログラム hiredisのinstall後に単純にHashをincrbyするプログラムを実行してみます。まずはプロセス内で単純に指定回数incrbyを繰り返します。今回は指定回数を1万としたので、かなり高速に書き込みが完了している事が分かると思います。実行後にはredis-cliで書き込みが出来ているかを確認します。
$ sudo yum install hiredis hiredis-devel -y $ gcc -I/usr/include/hiredis -L/usr/lib64 -lhiredis redis-test.</description>
    </item>
    
    <item>
      <title>Apache ModuleでRequest ParameterをParseしてDBからデータを取得する</title>
      <link>https://yutakikuchi.github.io/post/201311220824/</link>
      <pubDate>Fri, 22 Nov 2013 08:24:40 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201311220824/</guid>
      <description>[C++] : Apache ModuleでRequest ParameterをParseしてDBからデータを取得する Apacheクックブック 第2版 ―Webサーバ管理者のためのレシピ集
作者: Ken Coar,Rich Bowen,笹井崇司出版社/メーカー: オライリージャパン発売日: 2008/09/26メディア: 大型本購入: 6人 クリック: 144回この商品を含むブログ (32件) を見る
Request Parameter取得とDB接続  母校の同窓会幹事代表を務めています@yutakikucです。
最近C++のエントリーを書く事が多いですが、今日もApache Moduleについて書きます。
Apache ModuleでRequest Parameterを取得する際はApacheのVersionに気をつけましょう。Versionによって使える関数が異なるようです。基本的には上位互換が保たれているようですが、最新Versionのドキュメントを参照している時に、実際には古いVersionを使ってしまっていると実装時に長い時間嵌る可能性があります。また単にRequest Paramterを取得しただけでは面白くないので、Parameterに従ってDB上のデータを参照する事を行いたいと思います。Apache ModuleやCのPreparedStatementに関する日本語ドキュメントは少ないので少しでも開発者の方々へ貢献できるように今後も頑張ります。
  GitHub  GitHub Path このエントリーで使用するソースコードGitHubに置きました。
CPlus/apache_module/ps/mod_db.c at master · yutakikuchi/CPlus 
 Compile &amp; Install 以下のコマンドでcompile &amp; installしてくれます。installが完了したらApacheを再起動します。
$ sudo yum install httpd-devel mysql-devel -y $ git clone git@github.com:yutakikuchi/CPlus.git $ cd CPlus/apache_module/ps/ $ sudo apxs -i -a -c -I /usr/include/mysql -L /usr/lib64/mysql -lmysqlclient mod_db.</description>
    </item>
    
    <item>
      <title>crypto&#43;&#43;でのお手軽暗号</title>
      <link>https://yutakikuchi.github.io/post/201311150820/</link>
      <pubDate>Fri, 15 Nov 2013 08:20:22 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201311150820/</guid>
      <description>[C++] : crypto++でのお手軽暗号 C++プログラミング入門
作者: グレゴリーサティア,ダウグブラウン,Gregory Satir,Doug Brown,望月康司,谷口功出版社/メーカー: オライリー・ジャパン発売日: 2001/11メディア: 単行本購入: 9人 クリック: 147回この商品を含むブログ (29件) を見る
お手軽暗号  @yutakikucです。
今日はcppで暗号/復号するためのエントリーを書きます。PHPやPythonにはcrypt標準モジュールがありドキュメントも充実しているので簡単に暗号化できますが、cppではそれらがあまり整備されていない事もあってプログラム書く時にちょいと苦労します。尚、ここでお手軽暗号と言っているのは暗号化のブロック方式をECBで対応するためです。CBC方式でも良いのですがIVの管理も面倒だし、そもそもそこまで暗号化強度に拘らないケースを想定しています。cppでの暗号化を行う為のLibraryとしてcrypto++という様々な暗号化方式をサポートする物を利用します。
Crypto++ Library 5.6.2 - a Free C++ Class Library of Cryptographic Schemes 
  crypto++  install 以下のOneLineです。
$ sudo yum install cryptopp cryptopp-devel -y   DES × ECB TripleDES - Crypto++ Wiki 
以下のコードでDES × ECBの暗号化/復号化ができます。crypto++のwikiにTripleDES × CBCの内容が載っていたのでそれを真似て作ってみました。暗号化の対象テキストが「魔法少女まどか☆マギカ」、暗号化keyに「Soul Gem」を指定します。ECBブロック暗号なのでIVを必要としません。処理としては単純にマルチバイト文字列を暗号化 = バイナリを16進数変換 = 16進数をバイナリに変換 = マルチバイト文字列を復元です。下に実行結果も載せておきます。
#include  #include &#34;</description>
    </item>
    
    <item>
      <title>スケジュールの仮登録を神速で管理できる「Cu-hacker」がセクシー過ぎる件</title>
      <link>https://yutakikuchi.github.io/post/201311050826/</link>
      <pubDate>Tue, 05 Nov 2013 08:26:41 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201311050826/</guid>
      <description> [StartUP] : スケジュールの仮登録を神速で管理できる「Cu-hacker」がセクシー過ぎる件 Cu-hackerとは  
あどさーばーを開発中の@yutakikucです。
今日はビジネスマンの面倒くさいスケジュール管理を強力にサポートしてくれる「Cu-hacker」の仮登録機能を紹介したいと思います。前もってスケジュール候補をたくさん抑えたい人には超お勧め機能なので、是非使ってみると良いと思います。以下のCu-hacker操作のannotationにはSkitch | Evernote を利用しています。
  Cu-hackerの仮登録管理がセクシー！  
スケジュール管理にGoogle Calendarを使っている人も多いのでは無いでしょうか。今のGoogle Calendarには残念ながら仮予定を一括で管理する機能がありません。1つの予定候補を1つずつ管理するといった手間がかかってしまいます...
この予定候補のおまとめ管理といったセクシーな機能をCu-hackerは実現してくれています。下がCu-hackerの操作画面です。 
以下は使い方の簡単な説明です。
1.まずは仮予定として登録したい日時のカレンダーをマウスでドラッグ選択します。
2.左のクリップボードに仮予定一覧がまとめられるので、「仮登録」ボタンを押します。予定名を入力して、仮登録完了！
3.予定が確定した段階で、本登録します。仮登録で抑えた時間の枠に対して「予定として確定」ボタンを押します。これで本登録完了！
4.本登録された予定以外は自動的に消してくれます！この機能が仮予定を沢山登録したいビジネスマンに対する優しい気持ちの表れだと思います。セクシー過ぎて鼻血がでますね！(笑)
  他人とのスケジュール調整がセクシー！  Cu-hackerにはまだまだセクシーな機能が備わっています。他人とのスケジュール調整をしたい時に、まずは自分の空いている日時を伝えて、相手にそこから選んでもらいたいケースがあると思います。これもCu-hackerなら出来ますYO！
まずは上で登録した仮登録URLを調整して欲しい相手にメール等で渡します。

次はURLを受け取った相手側の予定調整画面です。URLを送った人の紹介が出ますので、「登録を始める」ボタンを押します。そうすると送った人の仮予定一覧を見る事ができるので、URLを受け取った人はOKな日を選択し「登録へ進む」ボタンを押します。URLを受け取った相手側はCu-hackerに未ログインでも使えてしまうところがセクシー過ぎますね！

相手には自分の名前の入力と登録をしてもらいます。下の画面で名前を入力し、「登録を確認する」ボタンを押します。内容を確認し「登録する」ボタンを押すと、URLを送って来た人のCu-hackerスケジュールに登録されます。また親切なCu-hackerはThank youページで相手に調整が終わった事を知らせるメール文言を表示してくれるので、それをコピーしてURLを送ってくれた人に返信します。


URLを送った本人のCu-hackerスケジュールに確定として予定が登録されている事が分かります。これで他人との予定調整は神速で完了です。さすがのCu-hackerさん、セクシー過ぎてエロいですね！僕ももはや色んな人と予定が調整したいです！
  Cu-hackerを紹介しているサイト   Googleカレンダーで華麗にアポ取り！打合せや飲み会の日程調整・仮押さえを効率化するCu-hacker（クウハッカー）が快適！！ | jMatsuzaki  スケジュール調整を10倍早くする「Cu-Hacker (クウハッカー)」ベータ版登場 【増田 @maskin】 | TechWave  「Cu-Hacker」の野望　スケジュール調整を10倍早くする→気づかいエンジン、そしてその先へ 【増田 @maskin】 | TechWave  マウス操作だけでスケジュール調整ができるWebサービス「Cu-hacker」 - noriaki blog はてな出張所  わかってる人が作ったと感じさせるCu-hackerの設計のすばらしさと実用性 | akazah blog  Googleカレンダー予定管理のCu-hacker、スマートフォンに対応 | ICT Headline directed by Ｐ検     </description>
    </item>
    
    <item>
      <title>C&#43;&#43;でApache Moduleを書きたい人へのTutorial</title>
      <link>https://yutakikuchi.github.io/post/201311010801/</link>
      <pubDate>Fri, 01 Nov 2013 08:01:29 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201311010801/</guid>
      <description>[C++] : C++でApache Moduleを書きたい人へのTutorial C++ Apache Module Tutorial  あどさーばー作っています@yutakikucです。
広告配信等の処理高速化の実現手段としてCを使ってApache/NginxのModuleレイヤーで処理を書く事があります。Apache/NginxのModuleはCを基本としているんですが、char*の処理は面倒でstringにしたい、連想配列でデータを管理し易くしたい、その他C++にしか無いライブラリを使いたいといったC++への欲求が出てきてしまいます。ぐぐってもApache ModuleをC++で実現している人って結構少なく、おそらく一般的にはApache Moduleにそんな複雑な処理を書く要件や期待なんて無いんだろうなと想像はしていますが、このエントリーではC++で書いてみます(笑)。
Apache API C++ Cookbook 
Apache 2.x Modules In C++ (Part 1) - CodeProject 
1つ目のサイトの「A Basic C++ Apache Module Example」にExampleが載っているんですが内容が素晴らしく古くて使い物にならなかったり、2つ目のサイトはMakefileの説明がよく分からないんで、僕がTutorial作ってみます。
  C++ Source &amp; Makefile  Tutorial File CPlus/apache_module/tutorial at master · yutakikuchi/CPlus 
下で使うTutorialの内容を置いておきました。
 Package Install 今回のC++ ApacheModule作成はCentOS6.4 x86_64でやっています。
先に開発に必要なパッケージをInstallしておきましょう。
$ sudo yum install httpd httpd-devel make gcc gcc-c++ -y $ tree .</description>
    </item>
    
    <item>
      <title>日本全国避難所データと現在地周辺の避難所地図を公開しました</title>
      <link>https://yutakikuchi.github.io/post/201310150809/</link>
      <pubDate>Tue, 15 Oct 2013 08:09:39 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201310150809/</guid>
      <description>[OpenData] : 日本全国避難所データと現在地周辺の避難所地図を公開しました 避難所  防災情報 全国避難所ガイド | ホーム 
避難所マップ - Yahoo!天気・災害 
@yutakikucです。オープンデータへの貢献という大義名分っぽい事を掲げ、避難所データを構造化テキストで作成し、更にはポイントを地図上へのマッピングします。避難所というと1st MediaのアプリやYahoo!避難所マップに情報が掲載されていますが、それ以外の避難所野良Web等は避難所データが整理されていません。Yahoo!避難所マップも都道府県毎に公開／非公開があるようでこちらもデータとしては不十分です。今回僕がデータ作成と簡単な地図マッピングを行うので、これを参考に良いアプリケーションが出てくる事に期待しています。
  作成した避難所データと地図  作成した避難所データはgithub、それをマッピングした地図をGAEで公開します。地図はGeoLocationAPIを利用して現在地付近の避難所のみを取得します。※利用は全て自己責任でお願いいたします。
github Data/shelter/pref at master · yutakikuchi/Data 
 公開地図 避難所マップ 

   避難所データソース  都道府県避難施設一覧 - 内閣官房 国民保護ポータルサイト 
1st Mediaの避難所データ 
内閣官房 国民保護ポータルサイトのデータを利用します。注意として「平成22年4月1日現在のデータを参考として掲載しております。」とあるようにデータが最新の物では無いです。上のサイトに掲載されているpdfからtextを抽出し、構造化テキストに変換します。代替のデータソースとしては1st MediaのWebPageに掲載されているデータを引っ張ってくる方法もいいかと思いますが、取り扱いには注意してください。
  避難所構造化テキスト作成  pdftotextを利用した避難所YAMLファイルの生成 まずはpdfからtextに変換する為のツールであるpdftotextをCentOS6.4へinstallします。install後に国民保護ポータルサイトからpdfファイルをdownloadして、pdftotextコマンドを実行してみます。pdftotextにより標準出力されたデータを見てみるとちゃんと変換されていることが分かりますが、pdf中の表に入力された改行がそのまま改行として出力されています。改行が1つであれルール化できそうですが、2つ以上ある箇所は改行がメイン行の前後に出力されるので、そのようなデータを拾う事はここでは諦めました。拾いたい場合は人の目で判断する必要があると思います。
※pythonにはpdfをtext変換するpyPdf ライブラリもあるので、そちらを使っても良いと思います。
$ sudo yum install poppler-utils -y $ wget http://www.kokuminhogo.go.jp/pdf/hinan_hokkaido.pdf $ pdftotext -enc UTF-8 -layout -nopgbrk hinan_hokkaido.</description>
    </item>
    
    <item>
      <title>町田ヒルズ族になってしまいました(｀・ω・´)ｷﾘｯ</title>
      <link>https://yutakikuchi.github.io/post/201309250814/</link>
      <pubDate>Wed, 25 Sep 2013 08:14:29 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201309250814/</guid>
      <description> [Life] : 町田ヒルズ族になってしまいました(｀・ω・´)ｷﾘｯ  僕が22,3歳の頃はちょうどLivedoorでホリエモンが大活躍をしていた時で、「俺も30歳になる時は六本木ヒルズに住んでやる！ホリエモンの部屋の隣に住んでやる！」って夢を大きく語っていた、そんな大志を抱いていた時期もありました。あれから7年近く経って30歳になった僕の現実の住まいは、「六本木」では無く「町田」です。いや、むしろ「町田」と呼ぶ事も正しく無い程の町田市の外れで、駅からも遠く、そしてかなり急な坂の上になります。本当に登る気も失せるような急で長く立派な坂です。僕が住んでいる家は坂の頂上にあるので、勝手ながら自宅周辺を「町田ヒルズ」と呼んでいます。このブログを書いている時間も車が通る音は一切聞こえない、コオロギだけが鳴いている静かな環境です。
先週末に一人暮らしをしていた横浜市から引っ越してきました。引っ越しの理由は父/母/姉の家族との時間を大切にする為で、4人がのんびりと過ごす生活がスタートしました。今僕の隣の部屋にいるのは自分をドラえもんと呼んでいる父(ドラえもんと誕生日が一緒らしい)です。ホリエモンはどこに...。数ヶ月前に書いた退職エントリーにはWebの未来について色々とカッコいい事だけを書いていましたが、それに加えて優先度を上げて家族を守る長男としても頑張って行く事になりました。暫くは家族と真剣に向き合って行こうと思います。かなり都心には出にくい環境になってしまいましたが、町田の急な坂の上から新しいWebサービスを考えることと技術的な情報の配信は可能な限り継続します。ただこのブログは適当な僕のメモ書きも多いので、ブックマークが沢山付けられてしまうと心苦しい時もあります。それでも本当に良い事を僕が書いたと皆さんが思われた時は支援をお願いします。今後も何卒宜しくです。
静かで何も無い環境の方がプログラミング作業自体は捗るかもしれません。とりえあず町田市民ということで今後はFC町田ゼルビアを応援します！あ、僕は元気でやってますよ。
 </description>
    </item>
    
    <item>
      <title>ギーク野郎のTerminal生活</title>
      <link>https://yutakikuchi.github.io/post/201309240809/</link>
      <pubDate>Tue, 24 Sep 2013 08:09:19 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201309240809/</guid>
      <description>[Linux] : ギーク野郎のTerminal生活 ギーク野郎  ギーク野郎とは開発用Terminalを常に立ち上げてプログラミング言語をいじっている人の事をここでは意味します。ギーク野郎はモニターを複数台所有し、それぞれがTerminal用、ネットサーフィン用、ニコニコ動画専用というような使い分けをしています。しかしそれぞれのモニターへの視線とマウス移動はフラストレーションを溜める1要因になります。それを回避するために本当のギーク野郎は1台モニタのTerminalだけで作業します。すみません、左の発言は適当です笑。この記事ではTerminalだけで作業を完結したい人を対象とし、GUIを使わずにCUIだけでの作業環境構築を目指し、それに役立ちそうなアイテムについて紹介します。
  tmux  tmux 
プロセス管理の初歩テクニック - Yuta.Kikuchiの日記 
tmuxを使ってWorking Spaceを効率的に使う事を強く薦めます。個人的にはtmuxが好きですが、screenでも良いと思います。tmuxはwindowを複数立てたりpaneと呼ばれる単位の子windowへの分割もできます。分割した領域間の移動は設定したキーバインドで可能です。tmuxは一つのプロセス内で作業領域を管理するので、アタッチ/デタッチという機能を使うと作業の中断/再開が簡単にできます。プロセスを消さない限りターミナルを落としても問題ありません。作業領域に関してはターミナルに備わっているタブ機能の方が便利と思われる方も多いと思いますが、個人的にはターミナルのタブ機能は使わずに1つのwindowをtmuxで細かく分割して作業内容を全て1画面で見れるような使い方の方がしっくり来ます。また複数のWorking Spaceの移動をtmuxでやるとキーバインドが統一されて、使用しているTerminalに依存しません。以下はtmuxの設定ファイルの.tmux.confとtmuxの使用画面例になります。
$ sudo yum install tmux -y $ vi ~/.tmux.conf #新しいウィンドウのベース番号 set-option -gbase-index 1 #全てのベルを無視 set-option -gbell-action none #各セッションで保持するバッファ数 set-option -gbuffer-limit 20 #256色端末を使用 set-option -gdefault-terminal &#34;xterm&#34; set-option -gdefault-command /usr/local/bin/zsh set-option -gdefault-shell /usr/local/bin/zsh #ウィンドウ履歴で保持される最大行数 set-option -ghistory-limit 5000 #Escapeキー入力時の待ち時間(0.5秒)をキャンセル set-option -sescape-time 0 #ウィンドウを実行コマンド名で自動リネーム set-window-option -gautomatic-rename on #スクロールモード、コピーモード、選択モードで vi のキーバインドを使う set-window-option -gmode-keys vi #ウィンドウで動作があるとステータスラインでハイライト set-window-option -gmonitor-activity on #UTF-8 シーケンスが現れることに備える set-window-option -gutf8 on #set-option -g mouse-resize-pane on #set-option -g mouse-select-pane on # prefix + r で設定ファイルを再読み込み set-option -gprefix C-g unbind r bind r source-file ~/.</description>
    </item>
    
    <item>
      <title>defineを辞めてhidefを使う</title>
      <link>https://yutakikuchi.github.io/post/201309170853/</link>
      <pubDate>Tue, 17 Sep 2013 08:53:38 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201309170853/</guid>
      <description>[PHP] : defineを辞めてhidefを使う 追記  PHPでdefineのかわりにhidefをつかう必要はない - id:k-z-h 
トラックバックに対して反応を書いた事が無いんですが、ちゃんとした内容が掲載されていたのでこちらでも追記しておきます。エントリーアップ時にdefineを辞めてhidefに切り替えることを強く推薦したつもりは全くありませんでした。確かに僕が書いた「まとめ」の項目だけを見るとそう捉えれる事もできるのでまとめの項目を「hidefの導入を検討しても良いと思います」にしました。kazuhaさんが仰られているY!社の現状予想と僕の読解力の無さの話は置いておいて(笑)、その他defineの改善効果とPECLの話はご指摘通りかなと思いました。カンファレンス当日の発表の一部を深堀したつもりだったんですが、問題の本質に誤解を与えてしまう内容を書いた事は反省します。
その他の方からdefineとhidef以外でオブジェクト定数(const)でもいいじゃんという話もコメントに頂いてまして、パフォーマンスの検証とかしてないですけどその方法もありかなと思いました。
  PHPカンファレンス2013に参加してきました  PHPカンファレンス2013 
2013/9/14(土)に開かれたPHPカンファレンス2013に参加してきました。主催、運営、スピーカーを担当された方々、大変お疲れ様でした。全体的には大半の人が知っている基礎的な内容が多かったと思います。スピーカーの方々も本当はもっとコアな話がしたいんだろうなぁとか、でも難しい話をし始めるとみんな分からなくなるんだろうなぁと思いながら聞いていました。個人的にセキュリティ面の知識が不足しているという事もあって、以下の2タイトルがとても勉強になりました。
 XML と PHP のイケナイ関係 (セキュリティ的な意味で) -Introduction of XXE attack and XML...  安全なPHPアプリケーションの作り方2013  前職の先輩方、大学の後輩も積極的にスピーカーを担当していて凄いなぁと感心していました。PHPを2年以上書いていない僕もどこかで間違いの無い知識を発表してみようかなぁと思ったり。(笑)今日は前職の先輩がPHPのhidefについてさらりと触れられていた内容について僕の方でも追加で紹介したいと思います。
  defineでは無くhidefを使う  PECL :: Package :: hidef 
カンファレンスの説明でdefineはスクリプト実行毎に呼ばれるので、起動時一度定数読み込みするためにPHPExtensionを使うのが良いって説明もありました。ただしExtensionは開発コストが大きいのでPECLのhidefを使う話をします。
PHPとCentOSのversion 今回僕が実行した環境です。PHPのversionは5.4.19、CentOSは6.4になります。
$ php -v PHP 5.4.19 (cli) (built: Aug 22201308:03:53) Copyright (c) 1997-2013 The PHP Group Zend Engine v2.4.0, Copyright (c) 1998-2013 Zend Technologies $ less /etc/redhat-release CentOS release 6.</description>
    </item>
    
    <item>
      <title>Object Oriented JavaScriptの入門</title>
      <link>https://yutakikuchi.github.io/post/201308270841/</link>
      <pubDate>Tue, 27 Aug 2013 08:41:19 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201308270841/</guid>
      <description> [javascript] : Object Oriented JavaScriptの入門 謝罪  Object Oriented JavaScriptの入門 - Yuta.Kikuchiの日記 
2013/08/27に公開したObject Oriented JavaScriptの記事ですが、prototype周りの説明がいい加減だったため内容を一旦削除することにしました。多くの方にBookmarkして頂いたお陰でホットエントリーにも掲載されたのですが、正直それに見合う内容ではありませんでした。
あやふやな知識を書いてしまったのは僕の勉強不足が原因です。ネットで収集した情報だけで自分の知識を固めて行くのは危険だという事を身にしみて感じました。同時に今回とても悔しく情けない思いもしたので、必ずや正しいJavaScriptのオブジェクト指向知識を身につけて再投稿することをここに宣言します。 &#34;I shall return！&#34;
(function() { console.log( &#34;I shall return！&#34; ); }()); ＞ 後で読むタグをつけた方、もう記事はありません(笑)。修正版の再投稿を期待していてください。
＞ ブックマークをしていただいた方、申し訳ありませんがブックマークの削除をお願いします。
＞ id:yuisekiさん はい、仰せの通りCoffeeScriptで書くというのも一つの手だと感じています。
＞ id:anemoさん もしまた変な知識を書いていたら今度は容赦なく突っ込みをお願いします。
  </description>
    </item>
    
    <item>
      <title>プロセス管理の初歩テクニック</title>
      <link>https://yutakikuchi.github.io/post/201308190844/</link>
      <pubDate>Mon, 19 Aug 2013 08:44:39 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201308190844/</guid>
      <description>[Linux] : プロセス管理の初歩テクニック Index   一言 Terminalログアウト後もプロセスを残す プロセスの2重起動防止 簡単に不要なプロセスをkillする tmuxの薦め    一言  はてなって不思議なコミュニティーで具体的な実装や調査分析よりも便利なコマンドとかを紹介するネタが何故か重宝されるようなので、良く使うプロセス管理系コマンドの初歩テクニックを書いてみます。( コマンドの学習をより必要とする人が多く一般的な話だからでしょうか？ )
  Terminalログアウト後もプロセスを残す  単一の処理で長時間かかってしまうようなプログラムを実行後にログアウトするとプロセスが消えて悲惨な目に遭います。tmuxやscreenを使って再起動可能な設定をしている人は特に気にする必要はありませんが、これらを使っていない場合はnohupでログアウト後もプロセスを残しましょう。
$ vi nohuptest.sh #!/bin/sh sleep 1000 $ chmod +x ./nohuptest.sh $ nohup ./nohuptest.sh &amp; [1] 13564 #nohup: ignoring input and appending output to `nohup.out&#39; $ exit # 再度ログイン $ ps auxww | grep nohuptest yuta 13564 0.0 0.1 106088 1172 ? SN 03:56 0:00 /bin/sh .</description>
    </item>
    
    <item>
      <title>MongoDBのCapped CollectionとTailable Cursorを使ったRealTimeAccess集計</title>
      <link>https://yutakikuchi.github.io/post/201308130844/</link>
      <pubDate>Tue, 13 Aug 2013 08:44:30 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201308130844/</guid>
      <description>[MongoDB] : MongoDBのCapped CollectionとTailable Cursorを使ったRealTimeAccess集計 
Index   RealTimeAccess集計 Capped Collection Tailable Cursor まとめ    RealTimeAccess集計  RealTimeAccess集計をするためにMongoDBの利用を考えます。サーバーの構成は上図のようなイメージで各種ApplicationServerからFluentdでLogAggregatorにRealTimeでLogデータを転送し、LogAggregator MasterがMongoDBにFluentdで書き込んで行きます。ここで言うRealTimeAccess集計の機能要件を整理すると以下のようになります。
 Access発生後、1分以内で集計結果をWebツール上で確認したい。集計区間も1分単位など。 複数条件が指定可能で、柔軟なCross集計がしたい。 RealTimeAccess集計のSystem負荷を出来る限り抑えたい。 Access発生後の直ぐに確認ということでFluentdが必要、柔軟なCross集計のために1行のAccessLogをそのままMongoDBに格納という方法を採用します。以下ではこれを実現するためのMongoDBの機能であるCapped CollectionとTailable Cursorについて説明します。
  Capped Collection  MongoDBにはCapped Collectionというデータ数とサイズの上限を指定できるCollectionがあります。cappedコレクション - Docs-Japanese - 10gen Wiki 
Capped Collectionの良いところは指定した上限サイズに達した場合は古いデータを自動的に削除してくれるところです。よってRealTimeAccess集計のような短時間のみデータが必要なケースに向いています。更に自動削除によりエンジニア泣かせとされる深夜帯の定期削除バッチたるものが不要になります。また通常のCollectionより書き込みが速くなります。速度のパフォーマンス比較は以下の通りです。50万件のInsertでCapped Collectionの方が20%ほど処理時間が短くなります。
$ mongo MongoDB shell version: 2.2.3 connecting to: test  use capped_test;  db.createCollection(&#34;cappedcoll&#34;, {capped:true, size:100000, max:10000}); { &#34;ok&#34; : 1 }  db.createCollection(&#34;normalcoll&#34;); { &#34;</description>
    </item>
    
    <item>
      <title>Google先生の検索結果リンクが予想以上に作り込まれていた件</title>
      <link>https://yutakikuchi.github.io/post/201308090846/</link>
      <pubDate>Fri, 09 Aug 2013 08:46:44 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201308090846/</guid>
      <description>[調査] : Google先生の検索結果リンクが予想以上に作り込まれていた件 Index   検索結果のリンクは単なるRedirectorでは無かった  検索結果のhttps化 httpsからhttpページへの遷移ではブラウザはRefererを送らない Google先生はRerererを送る仕組みを実装してくれた Refererが送信される処理の流れを追う httpsからhttpsページへの遷移はどうなるか Google Analyticsで検索Queryが「not provided」となる本当の理由  まとめ    検索結果のリンクは単なるRedirectorでは無かった  知らなかったのが僕だけだったら凄い恥ずかしい内容なんですが、今までGoogle先生の検索結果として表示されるリンクのURLはGoogle内部でClick集計するためのRedirector機能だと思っていました。カウントアップの集計を記録したら本来のURLに遷移させるような。当然そのClick数を集計する機能も持ち合わせているんでしょうが、もう少しユーザーにも優しくClickした遷移先のサービスの事も考えられた親切な仕組みになっていたのでここにメモを残しておきたいと思います。
検索結果のhttps化 2011年頃からGoogle先生は検索結果ページをhttps化させていますね。最初はGoogleのログインユーザーが対象だったと思うんですが、最近は全ユーザーを対象とするようになって来ています。これは検索機能にしては結構チャレンジングな事だと思います。httpと比較すると認証局との通信等を含めて処理が重たくなるし、単純に証明書のお金も掛かるし、セキュリティレベルの高い情報入力ページ以外での導入は避けられがちです。(※最近はhttpsの処理高速化としてSPDYとか技術開発も進んでいるみたいですね。SPDY - Wikipedia ) セキュリティ面を気にするユーザー視点からすると通信の暗号化という点は安心できる事だと分かりますが、大半の人は処理がさくさく動いてくれれば特にというレベルでしょうか。
 httpsからhttpページへの遷移ではブラウザはRefererを送らない HTTP/1.1: Security Considerations 
 Clients SHOULD NOT include a Referer header field in a (non-secure) HTTP request if the referring page was transferred with a secure protocol.
 RFCの記述にもあるようにClientはhttpsからhttpページへの遷移の場合はRefererを送らないようにと記述されています。一応各種ブラウザはこのルールを守っていてhttpsからhttpへの遷移ではRefererを送りません。Refererは遷移元を特定するための超重要なHttpHeaderです。このブラウザが「Refererを送らない」という仕様がWebサービスを公開している人にとってはとても痛い事で、Googleからの遷移かどうかが分からなくなってしまいます。
 Google先生はRerererを送る仕組みを実装してくれた ブラウザが「Refererを送らない」問題を偉大なGoogle先生は解消してくれています。httpsの検索結果からhttpのページへRefererを送信しています。処理手順は以下のようになります。
 検索結果のリンクはhttp://www.google.co.jp/urlのようにhttpのスキーマで定義する。 http://www.google.co.jp/urlは200OKでhtmlを返す。(302 Foundで遷移先URLを返さない) ブラウザは200OKのhtmlを取得する。取得したhtml内部のjavascriptおよびMETA http-equiv=&#34;</description>
    </item>
    
    <item>
      <title>データ集計コマンドを極めてシステム処理と業務速度を爆速化するお話</title>
      <link>https://yutakikuchi.github.io/post/201308010843/</link>
      <pubDate>Thu, 01 Aug 2013 08:43:58 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201308010843/</guid>
      <description>[Linux] : データ集計コマンドを極めてシステム処理と業務速度を爆速化するお話 Index   データ集計コマンド 爆速で検索したいぜ！  lookを使う LC_ALL=Cを設定する  データのランダムサンプリングがしたいぜ！  sedを使う awkを使う sortの--random-sortを使う Script言語を使う shufを使う ランダムサンプリング速度比較  合計と平均値を集計したいぜ！  列データ取得 重複行のカウント 合計値出力 平均値出力  複数ファイルのデータ結合がしたいぜ！  共通項目での結合 同じ行数での結合  まとめ    データ集計コマンド  joinコマンドが便利過ぎて生きるのが辛い - Yuta.Kikuchiの日記 
lookコマンドによる二分探索が速すぎて見えない - Yuta.Kikuchiの日記 
今日はデータ集計を行う上で絶対に覚えておいた方が良いコマンドと知識を紹介したいと思います。これを身につければシステム処理と業務効率化に大きく繋がると思います。この記事で紹介するコマンドはlook、sort、cut、uniq、shuf、awk、sed、join、pasteで、設定知識としてLC_ALL=Cについても軽く触れたいと思います。その他perl、pythonの一行野郎についても少し書きます。下で紹介する例はとにかく一行野郎に拘って書いていますので、見づらい事は予めご了承ください。
  爆速で検索したいぜ！  lookを使う 検索するコマンドで誰もがgrepを使っていると思いますがドキュメント全てを検索対象としてしまうので処理速度が遅くなります。grep以外に検索するための良いコマンドとしてlookというものがあります。lookは予め検索対象のデータをsortしておく必要がありますが、2分探索が可能なので繰り返しシステムが大量データから検索する場合に有効だと思います。以下1000万行のデータに対してgrepとlookを使った時の検索時間の比較になります。使用するデータのFormatは英数字の32Byte文字列になります。
#!/usr/bin/env perl use warnings; use strict; use String::Random; open(FH, &#34;data.txt&#34;); for(my $i=0; $i10000000; $i++ ) { my $rand_str = String::Random-new-randregex(&#39;[A-Za-z0-9]{32}&#39;); print FH $rand_str .</description>
    </item>
    
    <item>
      <title>Multi-Class Classifier of Bra Size used as the feature value with vital statics</title>
      <link>https://yutakikuchi.github.io/post/201307220846/</link>
      <pubDate>Mon, 22 Jul 2013 08:46:59 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201307220846/</guid>
      <description>[機械学習] : Multi-Class Classifier of Bra Size used as the feature value with vital statics Multi-Class Classifier of Bra Size  アダルトフィルタ実装に向けたA○女優リストの自動抽出 + α - Yuta.Kikuchiの日記 
前回のA○女優リストの自動抽出の流れから今日は実験を行います。本来の目的だったアダルトフィルタ作成から話がどんどんズレて行きます。今日のお題はVital Staticsを特徴量としてBra SizeのMulti-Class問題を解きます。タイトルを英語にしたのはこの下らない実験をさも真面目な研究としてやったかのようにカモフラージュするためです。初めに断っておきますが今回の実験を振り返った結果、反省はしている、だが後悔もしている状態です。
Vital Statics - Wikipedia 
一般的な話ですがVital StaticsからBra Sizeを導きだすのは難しいとされています。( BraSize = TopBust - UnderBust。UnderBust ≠ Waist ) 確かに個人のVital Staticsからは算出は難しいのかもしれませんが、集団データから傾向の推定はできるのでは無いかというのが今回の実験のテーマです。下のようにBraの項目が欠損している方もいるのでそこをMachine Learningの力で補完して行きます。
Name:瑠川リナ Bust:81 Waist:57 Hip:83 Bra:D65 Name:☆LUNA☆ Bust:83 Waist:57 Hip:82 Bra:D-65 Name:RUMIKA Bust:82 Waist:60 Hip:85 Bra:C Name:麗花 Bust:88 Waist:58 Hip:87 Bra:E Name:Reo.</description>
    </item>
    
    <item>
      <title>アダルトフィルタ実装に向けたA○女優リストの自動抽出 &#43; α</title>
      <link>https://yutakikuchi.github.io/post/201307190846/</link>
      <pubDate>Fri, 19 Jul 2013 08:46:28 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201307190846/</guid>
      <description>[programming] : アダルトフィルタ実装に向けたA○女優リストの自動抽出 + α アダルトフィルタ実装に向けて  エロデータサイエンティストの@yutakikucです。
今日はSystemで使うアダルトフィルタの辞書データ作成を目的としていた事が、予想外な方向に突き進んでしまった事をお話します。
アダルトフィルタの良くある活用例としてはユーザーが投稿する内容に卑猥単語が含まれている場合は登録を弾くことです。アダルトフィルタの辞書データを作成/運用することは実は結構大変だったりします。なぜならば人目で1語ずつ卑猥か否かを判断したり、自分で判断できない際どい単語はGoogle先生に聞きながらだったり..当然新語が増えればアダルトフィルタ辞書の運用コストも膨れるからです。
そこで作成/運用のコストを大幅に下げるために辞書の更新を自動化したいと考えます。例えば新しいA○女優がデビューした場合、名前をアダルトフィルタの辞書に自動登録するようなものです。今日はそれをPythonで実装し、最終的にはLTSV形式のデータで出力します。
Labeled Tab-separated Values (LTSV) 
  サンプルコード  A○女優一覧 - Wikipedia 
データソースはWikipediaに乗っている日本のA○女優一覧になります。上のURL以下をスクレイピングする事で名前のリストを抽出します。今回のアダルトフィルタに必要となるデータ項目は名前ですが、折角なのでVital Statistics(Three Size)、Bra Sizeもついでのついでのついでに取得します。保存するLTSVのフォーマットは以下のように定義します。
Name:  Bust:  Waist:  Hip:  Bra:
以下がスクレイピングを行うPythonコードです。更新データが直ぐに欲しい場合はCron等で定期的に実行してください。※時間が無くて10分ぐらいで書いたコードなので汚いです。
#!/usr/bin/env python # -*- coding: utf-8 -*- # アダルトフィルタのデータ抽出 import sys,re,urllib,urllib2 f = open( &#39;av.txt&#39;, &#39;w&#39; ) opener = urllib2.build_opener() ua = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/534.51.22 (KHTML, like Gecko) Version/5.</description>
    </item>
    
    <item>
      <title>【進撃の巨大データ】RealTimeLog集計を目的としたRedisの活用</title>
      <link>https://yutakikuchi.github.io/post/201307170850/</link>
      <pubDate>Wed, 17 Jul 2013 08:50:23 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201307170850/</guid>
      <description>[Redis] : 【進撃の巨大データ】RealTimeLog集計を目的としたRedisの活用 
Log集計の設計を再考  【進撃の巨大データ】Log集計用DBとシステム構成の美しい設計を考える - Yuta.Kikuchiの日記 
人生を前向きに楽しむことを心に誓った@yutakikucです。最近はこのブログで【進撃の巨大データ】というタイトルで何回かBigDataに関する記事を書いています。前回はLog集計用DBとシステム構成の美しい設計を考えるという題でInnoDB、InfiniDBを使ったLog集計のmerit/demerit、SystemPerformanceについて記述しました。それから時間をおいて再考し、InnoDBを使う場合のメリット/デメリットと注意事項が不足している事に気づいたのでここで追記します。更に集計の緊急度に合わせて使用するDBを変えます。リアルタイムではRedis、定期処理ではMysqlを使って集計することを試してみたいと思います。
Log集計方法のmerit/demerit 4のRedisを使うは今回新しく追加した項目です。
   No   Method   merit   demerit     1   InnoDBを使う。格納時点でデータを集計。
1行中の集計用カラムを都度Update。   特定のデータ集計を1行で管理可能。
行数が膨らまない。
集計SQLのPerformanceを出す事ができる。   書き込み口が複数あると行ロックが心配なので、
サーバ構成を工夫しなければ行けない。
格納時にデータを集約してしまうので、
シンプルで特定の集計しかできない。
【追記】データの書き込みや集計に失敗するとリカバリが大変。     2   InnoDBを使う。格納時点でデータを1行ずつ書き込む。
集計時にSQLで行数をSUMる。  行ロックの心配が無くなる。
【追記】処理ミスの影響範囲が行レベルに収まる。   行数が膨大になる。
集計用SQLも重くなるし、GROUP BYにも限界がある。
行数が膨らまないように
定期的に古いデータは削除するなどの処理が必要。     3   InfiniDBを使う。格納時点でデータを1行ずつ書き込む。</description>
    </item>
    
    <item>
      <title>【進撃の巨大データ】Log集計用DBとシステム構成の美しい設計を考える</title>
      <link>https://yutakikuchi.github.io/post/201307090849/</link>
      <pubDate>Tue, 09 Jul 2013 08:49:07 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201307090849/</guid>
      <description>[Mysql] : 【進撃の巨大データ】Log集計用DBとシステム構成の美しい設計を考える 
Log集計用DB設計  考える問題 Document無しのAgile開発をガチで推奨したい@yutakikucです。【進撃の巨大データ】の第2回目として巨大アクセスLog集計用DBの設計について勉強した内容についてメモしたいと思います。DB周りはそこまで詳しく無いので詳しい皆様からの突っ込み大歓迎でございます。また図々しいですが知恵をください(笑)。
今日の主目的は下の2要件を叶えるためのDB設計を考える事です。特に問題になるのがRealTimeの話でTableにLogDataを書き込む処理と集計のSQLをどのように組み立てるか、それ以外にもSystemPerformanceとArchitectureにも関わってきます。
 リアルタイムで大量データを集計したい 定期処理で大量データを集計したい   使うもの  Fluentd : Fluentd: Open Source Log Management  Mysql Innodb : MySQL :: MySQL 5.1 リファレンスマニュアル :: 13.5.3 InnoDB 設定  Mysql Infinidb InfiniDB – the high performance, column oriented analytic database  Fluentdを使ってNginxLogをMysqlにリアルタイムで格納する - Yuta.Kikuchiの日記 
FluentdはRealTimeでLogをLog収集用のサーバに転送とDBへの書き込み、Mysql InnodbはLogデータを格納するDBです。以前これらを用いてNginxのLogをリアルタイムで格納することを試したのでよければ上のリンクを参照してください。なぜMysqlか？という質問が出そうですが、安定と実績を買います。Mongodbのようなスキーマレスの方が後から柔軟に集計が可能という話もでてきそうですが、必要なデータ項目だけを抽出してディスク容量を抑えることと集計の高速化を目指します。そんなことしないで最初からTresureData使えよって話もありそうですが、自前で作りたいというプライドのためです(笑) Mysqlの種類も行指向のInnoDBと集計処理に向いているとされる列指向のinfinidbの両方を使ってみます。
 リアルタイム大量データの格納をどうするか？ リアルタイム大量データ格納の方法を決める上で重要ポイントとなのがMysqlのrowをどのように使うかです。取り得る方法とそれぞれのメリット/デメリットを考えて下にまとめました。もしMongodbのようなKVSを使ったLog集計でもKeyとValueをどのように使うかという同じ問題になると思います。
   No   Method   merit   demerit     1   InnoDBを使う。格納時点でデータを集計。</description>
    </item>
    
    <item>
      <title>【進撃の巨大データ】自作ApacheModuleとRedisでWebBrowserを一つ残らずUnique管理する</title>
      <link>https://yutakikuchi.github.io/post/201307030856/</link>
      <pubDate>Wed, 03 Jul 2013 08:56:03 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201307030856/</guid>
      <description>[Apache] : 【進撃の巨大データ】自作ApacheModuleとRedisでWebBrowserを一つ残らずUnique管理する 
BrowserID管理の必要性  BehaviorTargeting調査レポート - Yuta.Kikuchiの日記 
進撃の巨人とADTechnologyの面白さを最近の楽しみとしている@yutakikucです。BigDataという言葉が大変流行っていますが、巨大な力を持つ大量のユーザーアクセスとそれから生まれるログ、その処理と分析に追われるエンジニア/データサイエンティストはまさに進撃の巨人と人間の闘いのようです（笑）この記事のタイトルは進撃の巨人でエレンが言った「巨人を一匹残らず駆逐してやる」を文字っています。今日はそんな巨大データを扱うADTechnology分野のUserTrackingに欠かせないBrowser識別子とUnique管理について触れたいと思います。ADTechの面白さを少し話しておくと検索やKVS等の最新技術だけでなく機械学習や統計のアカデミック領域の知識も必要で、本当に毎日が勉強の連続です。目的はユーザーに最適な広告を表示してCTRを稼ぐ事でゲームをやってる時のワクワク、宝の山を探すロマン?!みたいなものを感じたりするんですよね。そんな最適な広告表示のためにCookieの中にBrowserIDという識別子を設定して、AccessLogからBrowserIDを取得してRedisに行動履歴を書き込むための技術を紹介します。実行環境はCentOSの6.3です。
  Apacheの標準Moduleを使ってBrowserIDを生成  mod_usertrack mod_usertrack.c 
ApacheModuleを自作する前に標準Moduleのmod_usertrackを使ってみます。mod_usertrackはCookieを指定した名前と有効期限で設定することができます。利用するためにはhttpd.confでLoadModule usertrack_module modules/mod_usertrack.soを有効にし、CookieTracking on CookieExpires &#34;1 years&#34; CookieName BrowserIDの3行を追記します。これで有効期間が1年間のBrowserIDを自動で発行します。追記が終わったらapacheのrestartを実行します。それでは設定したhostにアクセスしてみます。今回はlocalhostになります。curl --dump-headerでアクセスしたファイルのCookieを見てみるとBrowserID=::1.1372482316740936; という名前のCookieで識別子が振られている事がわかります。1.1372482745843590という値ですが、アクセス元IPとUnixTime情報の組み合わせのようです。しかしmod_usertrackのBrowserID生成には以下の問題があります。IPAddressとUnixTimeではパラメータのバリエーションが少ないので、以下の二つの条件が重なると問題が発生します。この問題を解決するために次章では自作のApache ModuleでBrowserIDを管理します。
 アクセス元が携帯キャリアGWの場合同一になるケースが存在する。 UnixTimeに同一時間にRequestが来てしまうケースがある。 $ sudo vim /etc/httpd/conf/httpd.conf LoadModule usertrack_module modules/mod_usertrack.so CookieTracking on CookieExpires &#34;1 years&#34; CookieName BrowserID $ sudo /usr/sbin/httpd -k restart $ curl --dump-header - &#34;http://localhost/cookie_test&#34; HTTP/1.1 200 OK Date: Sat, 29 Jun 2013 05:12:25 GMT Server: Apache/2.2.15 (CentOS) Set-Cookie: BrowserID=::1.</description>
    </item>
    
    <item>
      <title>誰もが一度は陥る日付処理。各種プログラミング言語におけるDateTime型/TimeStamp型の変換方法のまとめ</title>
      <link>https://yutakikuchi.github.io/post/201306170835/</link>
      <pubDate>Mon, 17 Jun 2013 08:35:13 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201306170835/</guid>
      <description>[programming] : 誰もが一度は陥る日付処理。各種プログラミング言語におけるDateTime型/TimeStamp型の変換方法のまとめ 日付型の変換処理  Date/Timestamp変換のまとめ - Yuta.Kikuchiの日記 
10言語のプログラミング言語に対してそこそこの知識を保有している@yutakikucです。いろんなプログラミング言語を使用していると文法を覚えるのは大変ですよね。PHP書いている途中からJavaScriptの文法を誤って書き始めたり... それぞれの言語の文法の違いを事細かく覚える事は無理に近いです。今日はそんな各種言語仕様の記述で難解なDateTime/TimeStampについて紹介したいと思います。扱うのはWeb系のメジャープログラミング言語のSQL/C++/Java/JavaScript/Perl/PHP/Python/Rubyになります。Mysqlからデータを取り出した後や、WebAPIから取得したデータを表示用の日付フォーマットに変換する事があると思うのでそこで利用できる知識になるかと思います。そもそもDateTime型/TimeStamp型って何だよっていう人もいるかと思うので簡単にまとめると以下のようになります。※ここでのTimeStampはUnixTimeを表現しています。
   型   意味   例     DateTime   YYYY-MM-DD HH:mm:SSで表現する時刻Format   2013-05-31 20:33:20     TimeStamp   1970/1/1からの秒数で上限は2037年まで   1370000000        変換重要処理早見表     言語   DateTime取得   TimeStamp取得   DateTime変換 = TimeStamp   TimeStamp = DateTime変換    Mysql   NOW()   UNIX_TIMESTAMP()   UNIX_TIMESTAMP()   FROM_UNIXTIME()       C++   time_t型, struct tm構造体, strftime(datetime, N, &#34;</description>
    </item>
    
    <item>
      <title>MongoDBのAggregation Framework/MapReduceを使ってより賢く集計を行うためのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201306100845/</link>
      <pubDate>Mon, 10 Jun 2013 08:45:45 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201306100845/</guid>
      <description>[MongoDB] : MongoDBのAggregation Framework/MapReduceを使ってより賢く集計を行うためのまとめ 
Mogodb集計  MongoDBの集計機能が便利過ぎて泣けてくるお話し - Yuta.Kikuchiの日記 
1月程前にMongoDBを使った集計機能の紹介をさせていただいた@yutakikucです。内容は全く大した事無かったのですが、タイトルで誘導を引っ張って200近いbookmarkを集める事ができました笑。みなさんの参考にしていただけたこと、大変嬉しく思います。今日はMongoDBの集計をもう一歩踏み込んだ内容を紹介して行きたいと思います。題材としてはAggregation FrameworkとMapReduceについてです。因に今回試してみたMongoDB-Versionは2.2.3です。Versionによって挙動が変わると思うので注意してください。
$ mongo --version MongoDB shell version: 2.2.3   Aggregation Framework  Aggregation ― MongoDB Manual 2.4.4 
SQL to Aggregation Framework Mapping Chart ― MongoDB Manual 2.4.4 
Aggregation FrameworkはRDBMSのSQLに備わっている便利機能をMongoDBに付与したFrameworkという説明が分かりやすいと思います。MongodbのVerson2.2以降に備わった機能です。以下はMongodbのAggregation OperatorとRDBMSのSQLの対応表になります。一つずつのOperatorはFilterに近いイメージで、Chainをしてパイプライン処理とすることができます。最終的にはChainした結果を取得します。
   MongoDB Aggregation Operator   SQL Term,Function,Concepts     $match   WHERE     $group   GROUP BY     $project   SELECT     $sort   ORDER BY     $limit   LIMIT     $sum   SUM(),COUNT()    $avg   AVERAGE()   CentOSでNginxのログをFluentdを使ってMongodbにリアルタイムで格納する - Yuta.</description>
    </item>
    
    <item>
      <title>一日も早く起業したい人が「やっておくこと、知っておくべきこと」読了</title>
      <link>https://yutakikuchi.github.io/post/201306060837/</link>
      <pubDate>Thu, 06 Jun 2013 08:37:57 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201306060837/</guid>
      <description>[起業] : 一日も早く起業したい人が「やっておくこと、知っておくべきこと」読了 起業前に  こんにちは、広告配信の最適化で常に頭が一杯な@yutakikucです。平日の深夜に読み進めた本は起業前の初歩的な知識がまとまっているものでした。具体的な起業手順が分かりやすく書いてあるので、実際の作業にとても参考になると思います。[必要なもの]の項目にあるような普通に考えると当たり前の内容も多いのでその辺りは読み飛ばし、お金/起業手順を中心に読み進めると良いと思います。重要な部分と感じた一部をフレーズベースで紹介させていただきます。少しでもここを見てくれた方の参考になればと思います。他にも起業に関する内容を書いていて、以下のものです。
スタートアップを目指す人は必読！起業成功マニュアルの前半を読んでまとめを書きました - Yuta.Kikuchiの日記 
スタートアップを目指す人は必読！起業成功マニュアルの後半を読んでまとめを書きました - Yuta.Kikuchiの日記 
無駄無駄無駄無駄無駄ァ！ - The Lean StartUpから学ぶ無駄の無い起業プロセス - - Yuta.Kikuchiの日記 
  心構え  起業の現実  起業できる人が100人いたとすると  1年目で25%が廃業 2年目で残りの25%が廃業 3年目で残りの25%が廃業  起業しても3年後に生き残っているのは4割。最初の3年間が重要 金銭、仕事の責任、健康、看板/信用など会社員とは異なる 責任を負う代わりに全て自由にできる   起業する分野の選択  人生で実現したいこと 強みを活かすことが起業/独立への成功の近道 社会が求めていることを理解する  マーケットイン：市場のニーズを捉え、それに合った商品やサービスを提供すること プロダクトアウト：売り手が売りたい商品やサービスを提供すること  マーケットインで進められるようにしたい   客層/商品/エリアを絞る  ビジネスのブレない軸を作る  商品/サービス、エリア、客層、デザイン、価格など  大企業と勝負するのではなく、ニッチトップを狙う   ターゲット層からのヒアリング  聞く人をちゃんと選ぶことと、無料相談などを通して専門家の意見を聞く ニーズや悩み、改良、価格などを聞いてみる   競合他社との比較  自分の強み弱みを分析する   許可が必要かどうか  事業を始めるためには法的な許可が必要なケースがあるので、そこを確認する  許認可、資格の取得など    事業計画書を作る  必ず事業計画書を作成する。何回も検討しながら精査していく。 売れる理由と客観的な裏づけ     組織形態を決める  個人事業主/株式会社    項目   株式会社   個人     登記/定款   必要   必要     設立費用   かかる   かからない     信用   高い  低い     融資   受けやすい   受けにくい     債務責任   有限   無限     事業内容   定款に定める範囲内   自由     記帳   複式帳簿   単式帳簿も可(青色申告者は複式帳簿)       税務などの運営   複雑   簡単     健康保険/厚生年金  加入できる(半分は会社の損金で)   国民健康保険/国民年金    課税   法人税   所得税       個人事業開業までのスケジュール  開業日、屋号、事業所、事業概要、記帳 税務署への届出  個人事業の開業届出書 : 開業日から1ヶ月以内 青色申告承認申請書 : 開業日から2ヶ月以内 青色事業専従者給与の届出：専従者がいることになった時点から2ヶ月以内 給与支払い事務所などの開設届 : 開業日から1ヶ月以内 源泉所得税の納期特例の承認に関する申請書  手続きの流れ  銀行口座作成 許認可の手続き 年金事務所 日本政策金融公庫や自治体への創業融資の申し込み 労働基準監督署への手続き ハローワークへの手続き    会社設立のためのスケジュール  設立日、商号(会社名)、本店所在地、事業目的、資本総額、営業年度、取締役 会社設立までの流れ  会社概要の検討/決定 会社実印の作成 個人印鑑証明書の取得 定款作成 設立総会の開催 定款認証 出資金の払込 登記書類の作成/押印 登記書類の提出 履歴事項全部証明、印鑑カード、会社の印鑑証明書の取得  会社設立後の手続き  税務署への届出 都道府県税事務所への届出 市町村への届出 会社銀行口座の作成 許認可の手続き 年金事務所 日本政策金融公庫や自治体への創業融資の申し込み 労働基準監督署への手続き ハローワークへの手続き    立地が業績を左右  自宅 賃貸オフィス レンタルオフィス(専用個室) 自治体インキュベーションオフィス バーチャルオフィス レンタルオフィス 間借り 賃料の検討  1人3坪、売上の3日分が目安      起業資金  融資による資金調達  融資の申し込み先を選択 日本政策金融公庫  http://www.</description>
    </item>
    
    <item>
      <title>FluentdとMysqlを利用した簡単なRecommendEngineの開発</title>
      <link>https://yutakikuchi.github.io/post/201305310719/</link>
      <pubDate>Fri, 31 May 2013 07:19:33 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201305310719/</guid>
      <description>[Log] : FluentdとMysqlを利用した簡単なRecommendEngineの開発 WEB+DB PRESS Vol.72
作者: 近藤宇智朗,生井智司,Dr.Kein,tokuhirom,森田創,中島聡,堤智代,A-Listers,はまちや2,竹原,川添貴生,久保達彦,道井俊介,飯田祐基,中村知成,規世やよい,後藤秀宣,天野祐介,奥野幹也,WEB+DB PRESS編集部出版社/メーカー: 技術評論社発売日: 2012/12/22メディア: 大型本購入: 11人 クリック: 94回この商品を含むブログ (10件) を見る
RecommendEngineを作りたい  Fluentd Casual Talks LT #fluentd #fluentdcasual 
Fluentdを使ってNginxLogをMysqlにリアルタイムで格納する - Yuta.Kikuchiの日記 
興味連動型広告におけるマッチングの微妙な知識だけを活かして最近は仕事をしている@yutakikucです。このエントリーではSimpleなRecommendEngineの仕組みを考えたいと思います。Userの行動履歴から類似ItemのSuggestを行うために相関データを導き出し、関連性上位のItemを抽出します。今回Logの蓄積を行うのにFluentd、AccessDataや類似度Dataを管理するためにMysql、COS類似度を計算するバッチ処理をJavaで記述します。Fluentdを使ってLogをMysqlに格納するための仕組み設定は上のエントリーを確認していただければと思います。
  AccessItem相関のCOS類似度  コサイン類似度 
AccessItemの相関を求めるためにCOS類似度を利用します。COS類似度は広く利用されており今回のItem相関だけでなく、複数の単語類似度などにも利用されています。COS類似度はベクトルと内積から算出され、計算は以下の通りです。
、 、、  AccessItem格納用DB構築  Tableの用途 Userの行動履歴から類似Itemを提案することを目的としています。それを実現するためにDBのTableは2個作成します。一つは選択中のItemに対してのAccess履歴を蓄積するTableで、もう一つはItem間のCOS類似度を格納するTableです。Tableにデータを書き込むタイミングはAccess履歴はNginxLogに書き込まれるタイミングでリアルタイムでitem_accessテーブルに格納し、COS類似度計算用のrecommend_itemTableにはJavaのバッチ処理で2つのitem間の類似度を計算したタイミングで格納します。
 item_access Table    item_id   user_id   access_count   created_time   modified_time     int(11)   int(11)   int(11)   datetime   datatime   CREATE DATABASE IF NOT EXISTS `recommend`; use recommend; CREATE TABLE IF NOT EXISTS `item_access` ( `item_id` int(11) unsigned NOT NULL, `user_id` int(11) unsigned NOT NULL, `access_count` int(11) unsigned NOT NULL, `created_time` datetime NOT NULL, `modified_time` datetime NOT NULL, PRIMARY KEY (`item_id`,`user_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8;   recommend_item Table    item_id   user_id   access_count   created_time   modified_time     int(11)   int(11)   int(11)   datatime   datatime   CREATE DATABASE IF NOT EXISTS `recommend`; use recommend; CREATE TABLE IF NOT EXISTS `recommend_item` ( `main_item_id` int(11) unsigned NOT NULL, `recommend_item_id` int(11) unsigned NOT NULL, `cos_score` int(11) unsigned NOT NULL, `created_time` datetime NOT NULL, `modified_time` datetime NOT NULL, PRIMARY KEY (`main_item_id`, `recommend_item_id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8;     AccessItemログのリアルタイム格納  error_log 今回WebServerにはNginxを利用します。NginxでのItemAccessLogを以下のPathに記録してリアルタイムでMysqlにデータを格納することを行います。Logの流れとしてはPHP→NginxLog→Fluentd→Mysqlのようになります。NginxのPHPApplicationではItemAccessが発生した場合は以下のようなログ出力Scriptで/var/log/nginx/item/access.</description>
    </item>
    
    <item>
      <title>Fluentdを使ってNginxLogをMysqlにリアルタイムで格納する</title>
      <link>https://yutakikuchi.github.io/post/201305220839/</link>
      <pubDate>Wed, 22 May 2013 08:39:22 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201305220839/</guid>
      <description>[Fluentd] : Fluentdを使ってNginxLogをMysqlにリアルタイムで格納する エキスパートのためのMySQL[運用+管理]トラブルシューティングガイド
作者: 奥野幹也出版社/メーカー: 技術評論社発売日: 2010/06/12メディア: 大型本購入: 16人 クリック: 204回この商品を含むブログ (31件) を見る
重要なデータはMysqlへ  CentOSでNginxのログをFluentdを使ってMongodbにリアルタイムで格納する - Yuta.Kikuchiの日記 
MongoDBの集計機能が便利過ぎて泣けてくるお話し - Yuta.Kikuchiの日記 
最近は大学生に良く間違われる現在30歳の@yutakikucです。今日はNginxLogの必要な項目をMysqlにリアルタイムで格納することを試してみます。重要なデータはどこに格納するの..？ Mysqlでしょ！という人向けに書いてみます。過去にMongodbに格納して集計するエントリーを書いたのでそちらも参照してください。次回はFluentd-WebHDFSを使ってリアルタイムにHDFSに格納することに挑戦してみようと思います。
  Fluent-Plugin-Mysql  plugin Fluentd plugins 
tagomoris/fluent-plugin-mysql · GitHub 
fluentdの公開されているpluginを利用すると実現したいことが簡単に出来るかもしれません。Mysqlへのリアルタイム格納にはtagomorisさんが開発したfluent-plugin-mysqlを利用します。
 fluentd install まずはfluentdの本体をinstallします。fluentdのpackageを管理しているtresuredataをrepos.dに登録してyumにてinstallします。
$ sudo vim /etc/yum.repos.d/td.repo [treasuredata] name=TreasureData baseurl=http://packages.treasure-data.com/redhat/$basearch gpgcheck=0 $ sudo yum install td-agent -y Installed: td-agent.x86_64 0:1.1.11-0 Dependency Installed: compat-libtermcap.x86_64 0:2.0.8-49.el6 compat-readline5.x86_64 0:5.2-17.1.el6 openssl098e.x86_64 0:0.9.8e-17.el6.centos.2 td-libyaml.x86_64 0:0.</description>
    </item>
    
    <item>
      <title>10分でFuelPHPの基礎を理解する</title>
      <link>https://yutakikuchi.github.io/post/201305200832/</link>
      <pubDate>Mon, 20 May 2013 08:32:29 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201305200832/</guid>
      <description>[PHP] : 10分でFuelPHPの基礎を理解する FuelPHP入門
作者: 早川聖司出版社/メーカー: ソーテック社発売日: 2012/06/02メディア: 単行本購入: 10人 クリック: 192回この商品を含むブログ (9件) を見る
様々な技術を求められる開発現場  じゃあ、いつRails始めるの？... 今でしょ！ - Yuta.Kikuchiの日記 
様々な開発現場でそこに必要な技術を求められていて四苦八苦中の@yutakikucです。ついこの前Railsのエントリーを書いたばかりなのに今度はFuelPHPについて調べなければいけなかったり...僕の体はいつ休まるんだろうか。FuelPHPもRailsと似ているところが多いのでRailsを学習した記憶が新しいうちにFuelPHPについて学んだことをどんどん書いていこうと思います。Railsに比べると少し設定が面倒なんで、ハマった方の少しでも参考になればと思います。
  Index   FuelPHP Setting  Nginx WebServer oil Install / oil create Error - date_default_timezone_get() DB設定 Scaffoldで雛形を作る Routingの設定 Crypt Key Error FuelPHPに接続  FuelPHP  Coreのライブラリのautoload Controller Model View Security/htmlentities     FuelPHP Setting  Nginx WebServer CentOSでNginxのログをFluentdを使ってMongodbにリアルタイムで格納する - Yuta.Kikuchiの日記 
FuelPHPを動かす環境をNginxを利用します。CentOSでのNginxの設定は上のエントリーを確認しながらやってみてください。ここではProcessが立ち上がっていることの確認とphp-fpmのCGI設定について記述します。プロセスの確認はpsコマンドで、php-fpmはWebでPHPを利用するためのCGIパッケージでyumにて最初にinstallしておくと良いでしょう。</description>
    </item>
    
    <item>
      <title>じゃあ、いつRails始めるの？... 今でしょ！</title>
      <link>https://yutakikuchi.github.io/post/201305070304/</link>
      <pubDate>Tue, 07 May 2013 03:04:00 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201305070304/</guid>
      <description>[Ruby] : じゃあ、いつRails始めるの？&amp;hellip; 今でしょ！ 実践 Rails ―強力なWebアプリケーションをすばやく構築するテクニック
作者: Brad Ediger,株式会社クイープ出版社/メーカー: オライリージャパン発売日: 2008/10/27メディア: 大型本購入: 7人 クリック: 90回この商品を含むブログ (43件) を見る
Index   はじめに RailsのInstall Railsの基礎 Rubyの基礎文法    はじめに  Ruby on Rails入門 (全46回) - プログラミングならドットインストール 
PythonistaからRubyistへの鞍替えを試みている@yutakikucです。DotInstall等を通してRuby on Railsの基礎を学び中なので学習した内容をまとめていきます。Yahoo!勤務時代はWebFWを自作していた経験もありFWについてはある程度知識を持っている僕から見てもRailsはとても便利ものだということが直ぐに分かりますが、色々と自動的にやり過ぎてくれて理解が難しくもあったりします。このエントリー内容は超初歩的なものになるので、これから勉強したいという人向けに記述します。この記事の基礎となっているのはdotainstallです。dotainstallのlessonは全部で46回ありますが、#4,#5,#09〜#036を見ると良いと思います。
  RailsのInstall  環境 RailsをInstallする環境はCentOS6.3です。
$ cat /etc/redhat-release CentOS release 6.3 (Final)  RailsInstallに必要なPackageInstall RailsのInstallに必要なPackageを先にInstallしておきます。以下のPackageが無いとRailsのInstall時にErrorが大量に出力される可能性があります。
$ sudo yum install gcc zlib zlib-devel yaml openssl openssl-devel curl curl-devel sqlite sqlite-devel readline readline-devel mysql mysql-devel -y  NodeのInstall 後々に必要になるServerSideのJavascriptの実行環境としてNodeをInstallしておきます。yumからinstall出来なかったのでSourceを直接DownloadしてCompileします。※注意:rails newでプロジェクトを作成した後にGemfileを修正してtherubyracerをinstallすればNodeは必要無くなります。</description>
    </item>
    
    <item>
      <title>MongoDBの集計機能が便利過ぎて泣けてくるお話し</title>
      <link>https://yutakikuchi.github.io/post/201304261239/</link>
      <pubDate>Fri, 26 Apr 2013 12:39:08 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201304261239/</guid>
      <description>[MongoDB] : MongoDBの集計機能が便利過ぎて泣けてくるお話し MongoDBイン・アクション
作者: Kyle Banker,Sky株式会社玉川竜司出版社/メーカー: オライリージャパン発売日: 2012/12/14メディア: 大型本購入: 5人 クリック: 55回この商品を含むブログ (4件) を見る
MongoDB集計機能  CentOSでNginxのログをFluentdを使ってMongodbにリアルタイムで格納する - Yuta.Kikuchiの日記 
時給3000円のCEOと揶揄されている@yutakikucです。今日は簡単にMongodbのログ集計機能を紹介します。機能が豊富過ぎて泣けてくるんで、ログ解析する人は是非使ってみて下さい。FluentdでMongodbにNginxのLogを流し込む設定は上のエントリーを参照して下さい。次回はAggregationFramework/MapReduce周りについて触れたいと思います。
  泣ける話 : 集計Query  インデックス - Docs-Japanese - 10gen Confluence 
クエリー - Docs-Japanese - 10gen Confluence 
MongodbはKVSでありながら、RDBの機能も持っているというところが便利で泣けます。具体例としてはIndex機能と、集計Queryをサポートしているところでしょうか。ここでは集計Queryについて説明します。集計QueryはRDBのlimit、count、group、sortなどが使えることや、比較演算子による条件指定もできます。また集計のデータ検索に正規表現が使えるところも魅力的です。それらを紹介していきます。
条件指定はfind()、行数取得はcount() RDBで言うSELECTとしてMongdbではfindを使います。MongdbのQueryでの条件指定はJSON形式だと思って下さい。x = 10 AND B = 20をJSONの条件とすると{ x : 10, y : 20 } こんな感じです。またfindでは正規表現の指定ができるので、特定のRequestURIを含むログだけを出力したいときなどに便利です。例えば/api/userというURIへのaccessは{ path : /\/api\/user/ }のように指定します。また検索されたレコードの個数をカウントする場合はcountを利用します。
//登録されているデータベースを表示  show dbs local (empty) nginx 0.</description>
    </item>
    
    <item>
      <title>TinyCDB vs QDBM vs MemcachedのPerformance比較</title>
      <link>https://yutakikuchi.github.io/post/201304190433/</link>
      <pubDate>Fri, 19 Apr 2013 04:33:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201304190433/</guid>
      <description>[KVS] : TinyCDB vs QDBM vs MemcachedのPerformance比較 NoSQLデータベースファーストガイド
作者: 佐々木達也出版社/メーカー: 秀和システム発売日: 2011/04メディア: 大型本購入: 8人 クリック: 859回この商品を含むブログ (29件) を見る
Index   Standalone KVSのPerformance比較 Performanceの結果 MachineSpec TinyCDB  Feature Setup Write Performance Read Perfomance  QDBM  Feature Setup Write Performance Read Perfomance  Memcached  Feature Setup Write Performance Read Perfomance     Standalone KVSの性能比較  TinyCDB - a Constant DataBase 
データベースライブラリ QDBM 
memcached - a distributed memory object caching system</description>
    </item>
    
    <item>
      <title>地域データの構造化テキストを公開しました。</title>
      <link>https://yutakikuchi.github.io/post/201304150124/</link>
      <pubDate>Mon, 15 Apr 2013 01:24:22 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201304150124/</guid>
      <description>[Python] : 地域データの構造化テキストを公開しました。 [asin:B003VWCBXI:detail]

はじめに  読み仮名データの促音・拗音を小書きで表記するもの - zip圧縮形式 日本郵便 
駅データ 無料ダウンロード 『駅データ.jp』 
最近は専らデータの整形屋になっている@yutakikucです。今日は日本郵便株式会社と駅データ.jpに掲載されているデータを利用して、地域データを構造化テキストとして作成します。ここでの目的は上のリンクで公開されている地域データの形式がCSVだったり、WebAPIのXMLだったり、データが1箇所にまとめられていなかったりと少し不便を感じたので構造化テキストを作成することにしました。出力形式は名前ベースのYAMLです。尚、下で公開したデータの利用は全て自己責任でお願い致します。
  PyYAMLの利用  YAML形式をGenerateする言語はpythonを利用します。pythonのYAMLライブラリのPyYAMLを使うのでその設定方法を書いておきます。import yamlが実行出来れば設定が完了しています。
$ wget &#34;http://pyyaml.org/download/pyyaml/PyYAML-3.10.zip&#34; $ unzip PyYAML-3.10.zip $ cd PyYAML-3.10 $ sudo python setup.py install $ python Python 2.6.6 (r266:84292, Sep 11 2012, 08:34:23) [GCC 4.4.6 20120305 (Red Hat 4.4.6-4)] on linux2 Type &#34;help&#34;, &#34;copyright&#34;, &#34;credits&#34; or &#34;license&#34; for more information.  import yaml    地域名データ/郵便番号のYAML化  県名、市区町村のデータの冗長化を防ぐためにYAMLのkeyとして設定しデータに重複が発生しないようにしています。以下が簡単なDataFormatとSampleDataです。日本郵便のサイトからDataをDownloadしてYAMLファイルを生成するのを下のPythonで自動的にやってくれますが、僕が試しに実行してみたところ処理に5分程掛かってしまい少し重たく感じます。生成したファイルをGithubに上げました。</description>
    </item>
    
    <item>
      <title>カッコイイWordPressを構築するためのたった3つの手順</title>
      <link>https://yutakikuchi.github.io/post/201304100545/</link>
      <pubDate>Wed, 10 Apr 2013 05:45:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201304100545/</guid>
      <description>[WordPress] : カッコイイWordPressを構築するためのたった3つの手順 
はじめに  新しく始める自社サイトをWordPressで簡単に作って、lolipopの月額105円プランでの運用を企んでいる@yutakikucです。lolipopのサーバはredhat系 + Apacheの設定でWordPressの導入も簡単に出来るということで、自分のLocalPC(CentOS)で環境を作って開発し、ちゃんと挙動を確認してからlolipop上で動作させるための準備をこのエントリーでは目的としています。
利用料金 / サービス - ロリポップ！  サーバー環境について / サービス - ロリポップ！ 
  Step1. WordPress Install  LocalPC 以下の作業は全て自分のPC(CentOS)で行なっています。lolipopでの作業ではないので注意して下さい。CentOSのVersionは6.3です。
$ cat /etc/redhat-release CentOS release 6.3 (Final)  yum install WordPressを動かすためにはPHP Mysql Apacheを必要とするので、以下のpackageをyum installします。
$ sudo yum install php php-mysql php-mbstring mysql httpd -y  Apache/Mysql Start ApacheとMysqlを起動します。serviceコマンドでstartをするだけです。
$ sudo service mysqld start Starting mysqld: [ OK ] $ sudo service httpd start Starting httpd: httpd: Could not reliably determine the server&#39;s fully qualified domain name, using localhost.</description>
    </item>
    
    <item>
      <title>Yahoo!を退職します。</title>
      <link>https://yutakikuchi.github.io/post/201304040851/</link>
      <pubDate>Thu, 04 Apr 2013 08:51:05 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201304040851/</guid>
      <description> [起業] : Yahoo!を退職します。 
4/30で退職します。  いつもWeb技術のネタでお世話になっています、菊池佑太(@yutakikuc)です。
この度、新卒入社以来6年間お世話になったYahoo!を4月30日付けで退職することになりました。昨日が最終出社日でした。
在籍中はモバイルプラットフォーム開発と行動ターゲティング広告の精度向上を目的とした研究開発に努めました。これと言って特に秀でた能力が無い私でも沢山の案件で責任者を担当させていただき、お世話になったYahoo!の皆様に対しては言葉では言い表せないほどの感謝の気持ちでいっぱいです。僕の最後を見送って頂いた方々、Officeの天井に何度も激突するような胴上げを2回もして頂き本当にありがとうございました。
  Yahoo!は今どんな会社か？  Yahoo!はとても良い会社だと断言出来ます。ここではYahoo!に興味を持たれている方も沢山いらっしゃると思うので個人の見解の一部を紹介します。軽い参考程度に。
2012年の途中から経営陣が新体制となり、社内の状況も大きく変わりつつあります。新体制でとても魅力的なところは社長に宮坂学さんが就いたところです。会社全体の目指すべきところを定期的に社員にわかりやすく説明してくれますし、社員一人一人を大切にして少しでも働きやすい環境を目指そうとする心意気がとても強く伝わって来ます。確かに過去においてはYahoo!の成長に必要な超有力人材を失ってしまったことは否めませんが、宮坂さんが目指す会社は確実に良い方向に変わっています。これにより社内の人達の働く意識が向上し、社外からもどんどん優秀な人が集まって素晴らしいサービスを爆速で開発していくことに強く期待しています。
ヤフー新経営陣「スマホ時代のNext Big Thingを作るため、”ならず者”たちを解き放つ」【キーパーソンインタビュー】 - エンジニアtype 
『Yahoo! JAPAN』CMO村上臣に聞く、「爆速」を生む組織マネジメントの極意【連載:BizHack】 - エンジニアtype 
本人の希望を考慮した配属を慎重に考えてくれる良い会社だと思います。少なくとも僕はやりたいことを的確に伝えていたこともあって、新卒配属/異動希望の両方を専門に携わるチームレベルで正確に配属していただきました。他の会社ではここまでは叶えてくれないという事をよく聞き、人事部の方々へは改めてお礼を述べたいと思っています。本当にありがとうございました。
超優秀な人材が会社を引っ張っています。一緒にお仕事をさせていただいた中では広告配信研究部門/Yahoo!研究所の方々のポテンシャルが凄まじく、成果物に対する数値の伸びもビックリするものでYahoo!のレベルの高さを再確認しました。同じチームだった2つ年下の上司は僕と比較できないほどの判断力/統率力/信頼力といったリーダシップの素養を持った素晴らしい方でした。このような方に会えたことは人生の貴重な財産です。広告配信以外にもYahoo!は誇れる技術が沢山あり、個人的に今後も注目したいのは日本語処理、検索PF、YOLP(地図)といった部門です。これらの部門はYahoo!デベロッパーネットワークでAPI外部公開も積極的に進めています。Yahoo!デベロッパーネットワーク 
当然課題もあります。サービス面ではスマートフォンポータルとしての苦戦。組織や経営面では技術やチームマネジメントをできる人が圧倒的に不足している状況であること、注力点を絞れず常にリソース不足になってしまうこと、不必要な社外テクノロジーの買いに走ること等。でも、きっとこれら問題の解決方法を導き出し今後も素晴らしい課題解決エンジンYahoo!を提供し続けてくれることを信じています。
  僕と一緒にWebの未来を考えませんか？  僕の今後について書きます。
Yahoo!を辞めるのは個人の力でWeb業界の道を切り拓きたいと考えたからです。他に理由はありません。コンペで何度かアイディア賞を頂いた新サービスの発想力と高度なエンジニアリングを活かし、Userには使いやすく喜ばれるサービスを、情報提供者に様々な面でおいしいサービスを。この2つを徹底的に追求して行きたいと考えています。その一つのサービスの方向性としてはユーザの行動履歴からの興味推定や生活圏の特定を行い、それにマッチした情報を配信する仕組みを作っていこうとしています。履歴やコンテンツはFacebook上のものを利用する予定です。Userの位置情報って本当に大切ですよね。Graph API - Facebook開発者 
急な話の展開になりますが、ここで一番強く訴えたいこと、それは一緒にWebの未来を考えてくれる仲間を絶賛大募集中というお知らせです。最初に積極的に関わりたいのは学生さん達です。今までも大学院生の研究開発の支援をしていて、将来有望な人材への投資や協力が重要だということを常々感じていましたし、何よりも僕は今若い人たちの積極的な意見が欲しい！ 今後は実際のWebサービスを一緒に作り、お互いの良いところを出し合いながら双方にメリットがある形を目指して行きたいと思っています。
まだ新しく作る社名すら決まっていないような準備段階なので、新サービスネタと基盤技術以外はゼロからのスタートアップということになります。本当に駄目元で書きますが、もしそれでも僕のやりたいことについてもっと詳しく知りたいという方はgmail.com&#34;yuta.kikuchigmail.com(を@にして)、@yutakikuc、Facebookまで超気軽にご連絡ください。Webを通して価値あるものを世の中に提供して行きましょう。
新しい会社ではチャレンジングな事、生きるためにお金を稼ぐ事の２つを並行して進めなければいけないと考えており、今日は前者に対する考えだけを紹介させていただきました。別の機会にお金を稼ぐ点についても考えを書きたいと思います。
  </description>
    </item>
    
    <item>
      <title>急上昇Buzzword抽出器</title>
      <link>https://yutakikuchi.github.io/post/201303250819/</link>
      <pubDate>Mon, 25 Mar 2013 08:19:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201303250819/</guid>
      <description>[Python] : 急上昇Buzzword抽出器 Buzzword抽出  先日Yahooさんの検索ランキングがリニューアルされたこともあり、今流行っているBuzzwordを自動抽出可能なプログラムを作成します。Buzzwordを抽出する対象はYahoo検索ランキング、Googleトレンド、Naverトピックワードランキング、kizasi、TweetBuzzです。これらのメディアから現在の検索、ブログ、twitterでのBuzzwordを把握出来ます。
 急上昇ワード ピックアップ - Yahoo!検索データ  Google トレンド  NAVER トピックワードランキング  kizasi.jp:ブログから、話題を知る、きざしを見つける  TweetBuzz - いま、Twitterで人気のサイトがまるわかり     BuzzwordSample  Buzzword抽出器により出力したデータは以下の内容になりました。定性的な評価から一番旬なデータで量を持ったページはYahoo! SearchRankingだと思います。Google Trendは1日のデータが1件ずつで過去1週間のデータを記録しているので、Yahooに比べると劣ってしまいます。またブログやTweet情報はやや新鮮さに欠けるのと現在のTrendを表している単語が少ないように感じました。
DataFormat Lavel : data1 \t data2 \t data3 .....  Yahoo! SearchRanking YahooBuzzword : 菅野美穂 オサート 河西里音 福岡大仏 武井咲 ガンバ大阪 トキハソース 青山草太 高橋愛 北総鉄道 ロアッソくん  Google Trend GoogleBuzzword : 堺雅人 仲里依紗 南海トラフ プエルトリコ Wbc ローマ法王 メタンハイドレート ＷＢＣ 野球  Naver TopicwordRanking NaverBuzzword : 平愛梨 ももいろクローバ.</description>
    </item>
    
    <item>
      <title>業種別企業の平均年齢と年収の辞書データを公開しました</title>
      <link>https://yutakikuchi.github.io/post/201303180838/</link>
      <pubDate>Mon, 18 Mar 2013 08:38:51 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201303180838/</guid>
      <description>[自然言語処理] : 業種別企業の平均年齢と年収の辞書データを公開しました 平均年齢と年収の辞書データ  企業別の平均年齢と年収のデータをネットで探していたのですが、リストとしてまとまっているものが無かったので作成しました。以前作成した業種別企業名辞書の企業コードを基にYahoo!ファイナンスから平均年齢と年収のデータを引き当てます。当然ですが、Yahoo!ファイナンス様のサーバ負荷が高まらないように引当時にはsleepを入れるという優しさを忘れてはイケません。
※下で公開しているデータの利用は全て自己責任でお願い致します。
業種別企業名辞書データを公開しました - Yuta.Kikuchiの日記 
Yahoo!ファイナンス - 株価やニュース、企業情報などを配信する投資・マネーの総合サイト 
  平均年齢と年収データ  DataFormat [業種名] 企業Code \t 上場市場 \t 企業名 \t 平均年齢 \t 平均年収  Github 全データは以下のURLにまとめてあります。
Data/corps_age_income.txt at master · yutakikuchi/Data · GitHub 
 SampleData 4344 東証1部 ソースネクスト(株) 33.7歳 6,050千円 4674 東証1部 (株)クレスコ 34.5歳 5,490千円 4676 東証1部 (株)フジ・メディア・ホールディングス 44.3歳 15,100千円 4684 東証1部 (株)オービック 34.3歳 7,380千円 4687 東証1部 ＴＤＣソフトウェアエンジニアリング 34.9歳 5,790千円 4689 東証1部 ヤフー(株) 34.</description>
    </item>
    
    <item>
      <title>洒落乙でResponsiveなWordPressのDesignをまとめた人気サイトのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201303120811/</link>
      <pubDate>Tue, 12 Mar 2013 08:11:24 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201303120811/</guid>
      <description>[WordPress] : 洒落乙でResponsiveなWordPressのDesignをまとめた人気サイトのまとめ WordPressデザインブック3.x対応
作者: エビスコム出版社/メーカー: ソシム発売日: 2011/09メディア: 単行本購入: 4人 クリック: 122回この商品を含むブログ (7件) を見る
まとめサイトのまとめ  過去1年間で公開された洒落乙なWordPressのDesignをまとめた人気サイトのまとめを書いておきます。有料/無料の両方があります。
本家  WordPress › Free WordPress Themes    日本語のまとめサイト  2012年 : 無料で使えるクオリティの高いWordPressテーマ 100 « nanomal  【WordPress】最新版！ハイクオリティすぎる無料テーマ集27選 - NAVER まとめ  これが無料!?!?圧倒的にクオリティーが高い、Wordpressテーマ３３選 | Hibilog　|　世界一周と日常と考察ブログ  Wordpressデザインテンプレート  厳選！高品質なのに無料で使用できるレスポンシブ対応のWordPressのテーマのまとめ -2012年版 | コリス  レスポンシブデザインに対応した完成度の高い無料WordPressテーマ七個 | room402.biz  無料のワードプレス テーマ、wordpressテンプレート配布サイト 『WordPress Theme』 ― 海外で人気のWordPressテーマ、ワードプレステンプレートを日本語化して無料配布！  無料で使えるレスポンシブのWordPressブログテンプレート１０選  レスポンシブなWordpressテーマ制作のためのスターターテーマまとめ - K&#39;conf  ジャンル別・おすすめWordPress無料テーマ集 - NAVER まとめ    海外のまとめサイト  40+ Free Responsive WordPress Themes  Best Of 2012: 50 Free WordPress Themes - noupe  Free WordPress Themes | NewWpThemes.</description>
    </item>
    
    <item>
      <title>Perlの衰退議論について一言言っておくか</title>
      <link>https://yutakikuchi.github.io/post/201303100457/</link>
      <pubDate>Sun, 10 Mar 2013 04:57:18 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201303100457/</guid>
      <description>[programming] : Perlの衰退議論について一言言っておくか プログラミング言語を作る
作者: 前橋和弥出版社/メーカー: 技術評論社発売日: 2009/06/20メディア: 大型本購入: 9人 クリック: 211回この商品を含むブログ (25件) を見る
Perl衰退に関する最近の話   なぜ国内でPerlが急速に萎んだのか  「なぜ国内でPerlが急速に萎んだのか」という記事を読んで - サンプルコードによるPerl入門  最近熱が上がっているPerlの衰退の話についてですが、そもそもこーゆー議論に意味ってあるんですかね？どうも信者とそうでない人の言い合いとしか周りからは見れません。僕はPerl信者でも無ければ、Perlを憎んでもいない立場の人間です。以下では個人的に気になったPerlの現状を数値で見直し、Perlを嫌う理由、Perlの使いどころというのを書きたいと思います。
  Perlの現状を数値で調査した  CPANのPackage数はPypiに抜かれている The Comprehensive Perl Archive Network - www.cpan.org 
PyPI - the Python Package Index : Python Package Index 
Perlをみんなが使った理由としてCPANの充実があったと思います。CPANで検索すれば何かしらライブラリが見つかり、Perlスクリプトの中でuseすれば簡単に利用できたのですごく重宝されています。少し前まではCPANの方がPypiよりもPackage数が多かったことを確認していたのですが、ここ最近では単純なPackage数の比較ではPypiが勝っています。2013/3/10現時点でのPackage登録数はCPANが27033、Pypiは28848です。authorの数も気になるところですが、CPANは10470、Pypiは不明でした。


 Document数で他言語に劣る Google先生にDocument数を聞いてみました。Queryが全てProgramming系の内容に一致したということが前提ですが、Document数でもPython、PHPやRubyに劣っていました。(PHPさんの圧勝には笑いました。)
   Lang   Documents     Perl   102,000,000     Python   159,000,000     Ruby   258,000,000     PHP   19,630,000,000</description>
    </item>
    
    <item>
      <title>CentOSでNginxのログをFluentdを使ってMongodbにリアルタイムで格納する</title>
      <link>https://yutakikuchi.github.io/post/201302200824/</link>
      <pubDate>Wed, 20 Feb 2013 08:24:53 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201302200824/</guid>
      <description>[MongoDB] : CentOSでNginxのログをFluentdを使ってMongodbにリアルタイムで格納する ハイパフォーマンスHTTPサーバ Nginx入門
作者: Clement Nedelcu,長尾高弘出版社/メーカー: アスキー・メディアワークス発売日: 2011/04/21メディア: 大型本購入: 2人 クリック: 714回この商品を含むブログ (23件) を見る
このエントリーで試したいこと  Nginxのログをfluentdを使ってMongoDBに格納することをリアルタイムで行うことを試してみたいと思います。参考にしたサイトは以下のものです。
fluentd  Fluentdのドキュメントへようこそ ― fluentd 0.10 documentation  Fluentdで始めるリアルタイムでのログ有効活用 （1/4）：CodeZine  Introduction of ‘fluentd’ « NAVER Engineers&#39; Blog  fluentd を利用した大規模ウェブサービスのロギング  dstatの結果をfluentdで取得して、WebSocketで送りつけるリアルタイムリソース監視アプリを作ってみた。 - from scratch  #fluentd でアクセスログからメトリクス生成/リアルタイム監視するための設定例 - tagomorisのメモ置き場      Nginxの導入  install Nginxをyumコマンドでinstallします。
$ sudo rpm -ivh http://nginx.org/packages/centos/6/noarch/RPMS/nginx-release-centos-6-0.el6.ngx.noarch.rpm $ sudo yum install nginx -y $ yum installed list | grep nginx nginx.</description>
    </item>
    
    <item>
      <title>Mecab Pythonを使ったTF・IDFによるWikipediaの重要単語抽出</title>
      <link>https://yutakikuchi.github.io/post/201302150823/</link>
      <pubDate>Fri, 15 Feb 2013 08:23:40 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201302150823/</guid>
      <description>[自然言語処理] : Mecab Pythonを使ったTF・IDFによるWikipediaの重要単語抽出 入門 自然言語処理
作者: Steven Bird,Ewan Klein,Edward Loper,萩原正人,中山敬広,水野貴明出版社/メーカー: オライリージャパン発売日: 2010/11/11メディア: 大型本購入: 20人 クリック: 639回この商品を含むブログ (44件) を見る
TF・IDF計算  自然言語処理の勉強としてTF・IDFによる重要単語の抽出をwikipediaのデータに対して試してみます。TF・IDFを一言でまとめると、とある単語の重要度を出現頻度から計算する手法です。計算結果は重みを表します。TFは単語の出現数(Term Frequency)、IDFは総文書数 / 単語が出現する文書の総数の対数(Inverted Document Frequency)、TFIDFはその積になります。数式にすると以下のようになりますが、Webを検索してみると人によって計算の仕方が異なっています。思想としてはTFは大きい値であればあるほど重みが大きくなるし、IDFは出現頻度がドキュメント総数に対して少なければ価値のある単語となります。
: Number Of Occurrences Of i In j
: Number Of Documents Containing i
: Total Number Of Documents
今回はMecab Pythonを使って自力でTF・IDFを計算します。PythonのNLTKライブラリを利用するとTF・IDFの計算を簡単に出力してくれたりもします。てっとり早くやりたい人はそちらを使ってみると良いでしょう。
nltk.text.TextCollection 
  Mecab Python  形態素解析器で有名なMecabをPythonから利用できるようにします。Mecab本体、Mecab-ipadic、mecab-pythonの3つをinstallします。mecab-pythonのinstallの時に&#34;mecab-config: コマンドが見つかりません&#34;のように怒られたらsetup.pyのmecab-configをmecabをinstallした時のlocalディレクトリを指定するように修正するとinstallできます。
// mecab本体 $ wget http://mecab.googlecode.com/files/mecab-0.99.tar.gz $ tar -xzf mecab-0.99.tar.gz $ cd mecab-0.99 $ .</description>
    </item>
    
    <item>
      <title>業種別企業名辞書データを公開しました</title>
      <link>https://yutakikuchi.github.io/post/201302100932/</link>
      <pubDate>Sun, 10 Feb 2013 09:32:43 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201302100932/</guid>
      <description>[自然言語処理] : 業種別企業名辞書データを公開しました Web解析Hacks ―オンラインビジネスで最大の効果をあげるテクニック &amp; ツール
作者: Eric T. Peterson,株式会社デジタルフォレスト,木下哲也,有限会社福龍興業出版社/メーカー: オライリー・ジャパン発売日: 2006/11/08メディア: 単行本（ソフトカバー）購入: 3人 クリック: 78回この商品を含むブログ (21件) を見る
企業名辞書  業種と企業名の辞書データが欲しかったんでYahoo!FinanceのデータをCrawlして作りました。帝国データバンクや四季報のデータが使えると良かったんですが、Crawlできそうに無かったので諦めました。残念ながら2600社ほどのデータしか集まっておらず、個人的にはもっといろんなデータが欲しいです。他に良い方法をご存知の方いらっしゃいましたらご連絡いただけると幸いです。
Yahoo!ファイナンス - 株価やニュース、企業情報などを配信する投資・マネーの総合サイト 
TDB企業サーチ | 帝国データバンク[TDB] 
会社四季報オンライン 
  企業名辞書データ  GithubURL githubに上げました。※利用する場合は全て自己責任でお願いします。
Data/corps.txt at master · yutakikuchi/Data · GitHub 
 DataFormat [業種名] 企業Code \t 上場市場 \t 企業名 \t 紹介文   SampleData [電気・ガス業] 9501 東証1部 東京電力(株) 福島第一原発事故による巨額賠償負担や廃炉費用で経営悪化。政府が出資、一時公的管理下に 9502 東証1部 中部電力(株) 電力３位で中部財界の雄。通信など新規事業に出資。１１年５月、国の要請で浜岡原発の運転停止 9503 東証1部 関西電力(株) 東京電力と並ぶ業界の雄。原発依存度高い。情報通信など展開。原発設備利用率低下で経営悪化 9504 東証1部 中国電力(株) 石炭火力５割と高い。電源は瀬戸内集中。原発増強計画だったが、震災で新規原発は稼働延期 9505 東証1部 北陸電力(株) 北陸３県に供給。原子力の設備利用率高かったが、現況は石炭火力主体。水力比率も高い 9506 東証1部 東北電力(株) 東北６県、新潟へ供給。震災で原発４基停止に加え、火力発電所も複数被災。大口ガス卸売りも 9507 東証1部 四国電力(株) 発電所は瀬戸内側に立地集中、原子力の比率大。保有する伊方原発１〜３号は全基停止中 9508 東証1部 九州電力(株) 九州財界の雄。産業向け比率が高い。通信事業も育成。玄海、川内の原発６基はすべて停止中 9509 東証1部 北海道電力(株) 原子力・石炭火力の比重大。需要構造は冬ピーク型。５月上旬の泊３号機点検入りで全原発停止 9511 東証1部 沖縄電力(株) 沖縄本島と周辺約４０島に電力供給、民生用比率が８割占める。電源は石炭と石油火力が主体 9513 東証1部 Ｊ−ＰＯＷＥＲ ０４年に政府が民営化で株放出、電力卸が主。電源は石炭火力と水力中心、大間原発の建設再開 9514 マザーズ (株)ファーストエスコ 省エネ支援と木質バイオマス発電が２本柱。バイオ発電は日田で自社発電所、白河は運営受託 9531 東証1部 東京ガス(株) 都市ガス最大手。原料天然ガス化先鞭、海外ガス田開発も。地域冷暖房注力、新エネ開発も着手 9532 東証1部 大阪ガス(株) 京阪神地盤。都市ガス２位。営業力強い。コージェネ推進。燃料電池用触媒など技術力に定評 9533 東証1部 東邦ガス(株) ガス業界３位。愛知、岐阜、三重の３県が営業地域。ＬＰＧも強い。コージェネ事業を推進 9534 東証1部 北海道ガス(株) 札幌、小樽、函館が地盤の地方都市ガス大手。石狩にＬＮＧ基地建設中、ガス調達先多様化 9535 東証2部 広島ガス(株) 中国地方で都市ガス供給首位。契約戸数はＬＰガス含め６０万戸強。工業用コージェネにも注力 9536 東証1部 西部ガス(株) 都市ガス大手、需要家数、販売量で全国４位。福岡市、北九州市が主要地盤。燃料電池開発に力 9537 東証2部 北陸ガス(株) 地方ガス大手。新潟、長岡、三条地区に都市ガスを供給。原料は県内産天然ガスとＬＮＧの２本柱 9539 東証2部 京葉瓦斯(株) 都市ガス中堅。市川、船橋、松戸など千葉県西部の市街地地盤。東京ガス、東京電力から原料融通 9540 名証2部 中部ガス(株) 豊橋、豊川、浜松、磐田が地盤。区域内の天然ガス化完了。グループにサーラコーポレーション 9541 東証2部 大多喜ガス(株) 千葉県内房地盤に都市ガス販売。県内産天然ガス等が原料の２割弱。関東天然瓦斯開発の子会社 9542 東証2部 新日本瓦斯(株) 日本瓦斯の関連会社で北本市、桶川市、久喜市等を供給地域とする都市ガス会社。ＬＰＧ販売も 9543 東証1部 静岡ガス(株) 静岡市地盤で工業用に強み。販売量国内５位。清水ＬＮＧ基地活用し周辺地域へ卸供給を拡大 9544 東証2部 東日本ガス(株) 日本瓦斯グループ。都市ガス中堅。取手、我孫子周辺が地盤。ＬＮＧは東ガス、東電から調達   CrawlCode #!</description>
    </item>
    
    <item>
      <title>スタートアップを目指す人は必読！起業成功マニュアルの後半を読んでまとめを書きました</title>
      <link>https://yutakikuchi.github.io/post/201302040834/</link>
      <pubDate>Mon, 04 Feb 2013 08:34:03 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201302040834/</guid>
      <description> [起業] : スタートアップを目指す人は必読！起業成功マニュアルの後半を読んでまとめを書きました 完全網羅 起業成功マニュアル
作者: ガイ・カワサキ,三木俊哉出版社/メーカー: 海と月社発売日: 2009/05/29メディア: 単行本（ソフトカバー）購入: 10人 クリック: 110回この商品を含むブログ (25件) を見る
前回の続き  前回の続きを記述します。振興組織に必要な具体的な方法論が載っています。個人的に後半で面白いと感じたのは第七章の資金調達の項目でした。
スタートアップを目指す人は必読！起業成功マニュアルの前半を読んでまとめを書きました - Yuta.Kikuchiの日記 
  第六章 人材採用の奥義  起業の要諦6  採用では表面的なことではなく以下を重視する。  その人は自分が必要とすることができるか。 事業の意義に賛同しているか。 必要とする強みが有るか。  試用期間を決めてなんとかなりそうかどうかを双方が検討できるようにする。   「A」プレイヤーを雇う  AプレイヤーはBプレイヤーを雇い、BプレイヤーはCプレイヤーを....を繰り返しZプレイヤーまでを雇ってしまうと起業には能無しが増加してしまう。 CEOがやるべきことは自分より優秀な経営陣を雇うこと。 CEOは自分たちより役割を果たす人がいることを認め、一度認めたら採用する勇気が必要。 おかしな人を雇わない原則  その人がどんなプロジェクトをまとめたかを分析する 大企業での成功は新興企業での成功を保証しない 期待や今までの環境との違いを口にする 身元紹介で察する 候補者に迷った時は人が知っている人を選ぶ  互いに同じスキルを持った人間ではなく補完しあうような多様なスキルを持つ人間が必要。   無意味な条件は無視する  組織で失敗した経験を持つ人を採用する。ただし失敗ばかりをしていた人を除く。 学位の有る人でなくても良い。 同じ業界や部門、役割での経験が両刀の剣。 大きな弱みがあっても、際立った強みがある人を探す。   ありとあらゆる策を講じる  取締役やスーパースターを使って良い人材を勧誘する。 自社で働いたことを功績としてもらう。   全ての意思決定者に売り込む  新興企業でへの就職は反対する人が多いので、配偶者、親、友だちなどの意思決定者の関心に応えられるような協力が必要。 内定通知を早く出し過ぎない。 採用する人の嘘を見抜く。   直感をダブルチェックする  直感による影響を避けるための手順  面接前に募集するポジションに必要な心構え、知識、正確、経験での話を組み立てておく 仕事に関する具体的な話をする 台本に拘る 成功体験や学習体験など具体的な内容を絞る 記憶に頼らずメモをとる 身元を紹介する  予防措置をとった上で直感に従う   スタンフォードショッピングセンターテストをする  どこかのショッピングセンターで偶然採用者に出会った時に駆け寄って挨拶をしたい人だけを選ぶ。 人生は短いので、生まれつき気に入らない人間と仕事をしている暇は無い。   一次評価期間を設ける  出来ない人の解雇を難しいと考えていると、出来る人の解雇をしなければならない可能性が増える。 パフォーマンス目標を決めた上で一次評価期間を設けて、その人の出来をチェックする。 3ヶ月間の試用期間でパフォーマンスを見極める。   これでおしまいと思わない  もっとその人のことを調べ、新興企業に向き/不向きの判断をする。 一流企業に勤めた人はよほどのことが無い限り新興企業に適さない。 その人が元の勤務先を辞め、自分の会社に入ったからといって終わらない。毎日が従業員との新たな契約。     第七章 資金調達の奥義  起業の要諦7  売り込みや資金調達の一部である。 有意義で長続きをする事業であり、社会にとって価値がある事業をしているかということ。   事業を立ち上げる  意義を見出すこと、世の中を変えること、お金のためでなく世界がよりよい場所にしたから事業をする。 事業に成功したら投資家が積極的に資金を出すか、もしくは彼らの資金など必要なくなる。 何事も最初は不可能だが、起業家とはまさに人が不可能だと思ったことをやる。   口を聞いてもらう  一回の打ち合わせで契約が成立するのは夢。 意思決定者の注意を引き付けるために信頼出来る人に口を聞いて貰う必要がある。 投資家に他の投資家の紹介をしてもらう。 弁護士や会計士、PR会社を選ぶ時は能力意外の人脈にも注目する。 投資家の投資先を調べて、起業家と知り合いになる。 投資家は大学の先生の言葉に弱い。   「トラクション」を示す  投資家は投資の対象となるチーム、技術、販売の実績を重視する。 トラクションは企業の場合は収益。 会社を始める資金が無ければ見込み客とコンサルティング契約を結ぶ。 製品/サービスが未完成の場合は実地テストやパイロットサイト、テストのための客先開拓をする。実地テストのつてが無いと資金調達には苦労する。   「きれいな体」にする  投資家のほどんどは取引しない理由を探している。 2000の事業計画のうち、10が出資を受け、1つが札束を生む。 投資家が考える欠陥はいかのようなもの。  別の雇用主からの訴訟、創業者に属するコア技術、他社への特許侵害 少数の創業者での大部分の所有、株式の希薄化、頑固な投資家による支配 CXOレベルの人間が適正に欠けたり、経験不足や刑法犯など。また法律違反や税金の不払い    全てを開示する  投資家には前に倒産したことなど簡単に知られてしまうので、罪を告白する。 賢明な投資家ならこの告白を評価する。大切なのは失敗したことではなく、失敗に学び再挑戦しようとしていること。   敵を作る  賢明な投資家は市場が無く競争相手がいないこと、創業者自身が同じような企業が他に存在することを知らないという結論に達する。 競争相手の存在は市場存在の証。競争相手のことを知っていれば情報収集は怠りが無い。 自社と競合他社の何ができて何ができていないかを整理する必要がある。 自らの弱みと強みをオープンにする。また競合他社との表では意味のない比較パラメータを設定しないこと。 競争相手が全くいないというのは成功の見込みが無い組織となる。   新しい嘘をつく  投資家には自分以外の企業も話をしている。 投資家に嘘をつくのであれば出来る限り新しい嘘をつく。   計略にはまらない  投資家の引掛け問題に対しても正しく応える必要がある。   猫を群れさせる  起業家はNoとは言えないし、投資家もあからさまな拒絶は好まない。 猫一匹をしっかり確保することが重要。 投資家を口説くことは客観的で数値化可能な有力情報を提供することではない。それは分析作業であると同時に恋人とのデートに近い。 投資家は貴方のことを見ている。 売り込み後の連絡、質問の受け答え、有力な顧客との契約、目標クリアによる驚き、他の優良投資家の動きなど。 改善が有ることが前提で粘り強く活動していると猫も群れるかもしれない。改善が無いと厄介者になる。   自分が関わろうとしている世界を理解する  ベンチャーキャピタリストのベール  自身の事業について自分以上に知っているわけではない。 一流の投資家を捕まえても成功が保証されるわけではない。投資家は掛けをたくさん打つが、そのほとんどは失敗すると踏んでいる。 外部資金を調達した時から自身は支配権を失う。 投資家が何かをしてくれるという期待をあまり持たないこと。失望もしないで済む。      第八章 パートナーシップの奥義  起業の要諦8  業務提携によって大事なのはキャッシュフローや売上が増えて、コストが減るという事。   スプレッドシート上の理由で提携する  組織の財務予測を変えるために提携する。 マスコミを喜ばせるだけの提携はしてはならない。   現場の人達に受け入れられるようにする  上層部が合意して記者会見をするというシナリオではなく、ミドル層とボトム層がその提携に対して理解をしていないければならない。   弱みをごまかすのではなく、強みを更に強化する  お互いの強みを際立たせる事。   win-winの関係を結ぶ  片方だけに有利な取引は長続きしない。 ミドル層やボトム層に提携を支持されたければ双方が勝ったと思われなければならない。 提携では人の道こそが何よりも大切。   文章でフォローする  文章は話し合いの先ではなく、後に作成する。   終了条件を入れておく  終了条件を入れておくと両当事者たちが安心でき、かえって契約が長持ちする。 優れた提携の当事者は本気でリソースを投入する。双方にとって重要な提携だからこそ抜けづらいようなパートナーを築く。     ブランド構築の奥義  起業の要諦9  ブランド学  ブランディングとはマーケターが扱う理解不能なまじない。 ブランディングとは4P。Product,Place,Price,Promotion。  ブランド構築に必要なのは人々に伝染しやすいこと、試しやすいこと、口コミを重視、コミュニティを築くこと。 伝染性の鍵となるもの。クール、有効、独特、常識はずれ、感動的、不快、贅沢、無料修理などのサポート  障壁を低くする  操作性の複雑さを減らす方法。  マニュアルを読まなくても分かるようにする マニュアルが良ければ良いほど口コミブランディングにも有効に効く 写真や図表を使う 父母に試してただの人が理解できるIFを目指す。  価格の高さも障壁になる。LEXUSはドイツ車より値下げをした。 乗り換えコストを下げる。   コミュニティを築く  エバンジェリスト採用の狙いは製品/サービスをめぐるコミュニティの構築。 コミュニティで顧客サービス、技術サポート、顧客同士の交流機会を提供。 コミュニティの構築、活動支援、カンファレンスを開くなどをする。ブランド創出、維持をする上でコニュニティ構築はもっとも安上がりな手段。コミュニティが勝手にできるのを待っていたは行けない   人間味を出す  温もりがあるブランドの方が良い。 若者をターゲットにすると温かみがあるブランドを作らざるを得ない。 自分たちのことを真面目に考えないとお客さんも真面目に受け取ってはくれない。 マーケティング資料に顧客を登場させる会社は人間味を感じる。 恵まれない人たちに手を差し伸べることは道徳的な責任を果たすだけでなく、ブランドを高める。   パブリシティを重視する  広告はブランドの維持、拡大には役立つかもしれない。それを打ち立てるのはパブリシティである。 自分が何かすごいものを作り出し、障壁を低くして人々の手に行き渡らせる、これを人々が噂をしてマスコミが記事にする。 友が力になるかどうか分からないうちから友情を結ぶことが大切。 自分の話がマスコミの相手にふさわしいかどうかを判断する。次に自分の分野をカバーしている記者を見極める。さらに一つの重要な条件を満たすときだけその記者に売り込みをかける 常に本当の事を言っていれば調子の良い悪い時に関わらず、マスコミは信頼してくれる。 記者が良い記事を書けるように情報を提供する。   実行を言葉にする  従業員の全てが実行を言葉に出来るようにする。      第十章 事業拡大の奥義  起業の要諦10  事業拡大が難しい理由が２つ  とある目的でデザインした製品を誰が何の目的で購入するか分からない。 振興組織のサービス/製品は簡単に買ってもらえない。    百花斉放、百家争鳴を歓迎する  種を沢山蒔くと何かが根を張り、花を咲かせる。花を咲かせた市場を育てる。また何故そこで花が咲いたかを見極める。 変なこだわりは捨てて予期せぬ顧客や用途を受け入れる。   リードジェネレーションの正しい手法を選ぶ  見込み顧客獲得のトップ5  小規模セミナー スピーチ 出版 積極的な人脈づくり 業界団体への参加    側近を取り込む  意思決定者の側近をいかにして取り込めるか。 側近を理解する、買収しようとしない、共感する、文句を言わない。   無神論者ではなく不可知論者を探す  事業拡大の聖杯は推薦人たる顧客を探すこと。 自分の宗教を否定せず製品やサービスの存在を考慮に入れてくれる不可知論者を探すべき。 不可知論者を喜ばせるためにはかつて出来なかった事を出来るようにしてあげる。   見込み客に語らせる  サービスを購入してもいいという人たちは最終判断をするために必要な物を教えてくれる。 見込み客に対して良い質問をする準備ができていないといけない。 見込み客に対して黙っていられない。耳を傾けられないというのはNG。 メモは記憶の助けになり、書き留めるほど関心が高いという心象を相手に与えることが出来る。 自分の製品をわかっていないのでニーズに合わせることができないというのは、もってのほか。 顧客にどうすれば商品を買ってくれるかを語ってくれるのであれば絶対に耳を傾けたい。   実際に試してもらう  新興組織が顧客を引き付けるためには製品/サービスを試せるようにするのが一番。 試してもらう事業のやり方  GMは本当の試乗ができるように24時間試乗プログラムで家に帰ってもらうということを実施した。 SalesForceはソフトウェアを30日間無料で提供し、一旦使用すると既にデータを入力をしているので他社製品にスイッチしにくい。    安全で容易な第一歩を提供する  狭い範囲でリスクの低い限られた方法で製品/サービスを使ってもらう。 一つの地域や部門、プロジェクト、お試し期間、簡単なサポート。 退会処理を簡単に出来るようにするなど、会社に対する最後の印象を良くすることができる。   拒絶から学ぶ  事業拡大の提案は拒絶されるが、そこから学ぶことが多い。   事業拡大のプロセスを管理する  全員に事業拡大を奨励し続ける。 顧客ごとに目標を設定する。 先行指標をチェックする。 真の成果に報いる。     第十一章 気高き事業遂行の奥義  起業の要諦11  社会を顧みず自分の利益を重んじる事業は大きくならない。 優れた持続可能な組織を作りたければ従業員に高いモラルや倫理基準を求める。 気高く倫理的な組織であるためには多くの人々を支援する、正しい行いをする、社会に還元するの３つ。   多くの人々を援助する  自分の助けにならない人を助ける。  ひょっとしたら彼らは何時の日か自分を助けるかもしれない 自分の説が正しかったときのために、しっかりとポイントを稼いでおきたい 同胞を助けることで内在的喜びが得られる    正しい行いをする  合意の精神を遵守する、見合う額を支払う、大切な事を重んじる   社会に還元する  キャピタルゲインにこだわらない。更なるお金を貯めこむことではなく、社会への還元である。 社会還元はお金だけでなく、時間や専門知識、精神的な支援wお提供すること。 重要なのは受けた恩を返すこと、見返りを期待して恩を着せるのではなく。     </description>
    </item>
    
    <item>
      <title>JavaでMapReduceを書くことが出来ない問題児がPigのデータ構造を調査しました</title>
      <link>https://yutakikuchi.github.io/post/201301280755/</link>
      <pubDate>Mon, 28 Jan 2013 07:55:29 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201301280755/</guid>
      <description>[Hadoop] : JavaでMapReduceを書くことが出来ない問題児がPigのデータ構造を調査しました Programming Pig
作者: Alan Gates出版社/メーカー: O&#39;Reilly Media発売日: 2011/09/29メディア: Kindle版この商品を含むブログ (1件) を見る
本当はJavaで書きたい。けどコンパイルや多段MapReduceは面倒なので  まずは僕の面倒くさがりな性格とプログラミング言語の話。10年前はJavaでWebアプリを書いていましたが、就職してScript言語をばりばり使っていた時期が長く続いたのでJavaから遠ざかってしまいました。もともとJavaのコンパイルが嫌いで、環境を整えたり直ぐに動作確認ができなかったり。スピードを求められる単純作業がその面倒な事によって時間が削られることを嫌っています。(自分でも良くないことだと思っていますので、今後は時間が有るときにJavaを書いてみます)JavaMapReduceは柔軟であり速度的にも速いことは知っています。ただMapReduceで複雑なデータのJoinやデータ集計を行うためには多段のMapReduceを書かざるを得ないこともあり、単純集計のコードを複数管理することもまた面倒に感じられてしまいます。
という事で、僕はHadoop集計ではPigを使うことにしました(笑) PigはSQLチックなLatinでデータをパズル的に組み合わせてデータ集計が可能です。速度的には当然JavaMapReduceよりは遅くなってしまいますが(たしか平均で1.5倍ぐらい遅くなる)、一つのScriptで柔軟なデータ組み合わせができます。
Pigの良いところはたったの数行で強力なデータの組み合わせが表現できること、悪いところは1行の表現が難しく自分の直感とは全く異なる挙動をすることがあるので細かくDESCRIBEを用いてデータの形式を確認する必要があります。以前に自分のメモとしてPigの内容を紹介をしました。今日はデータの格納と組み合わせのTipsについて紹介したいと思います。
 10分でHadoop-Pigの基本文法を理解する - Yuta.Kikuchiの日記  PigでHadoopをより便利に使う！PigでのMapReduceまとめ - Yuta.Kikuchiの日記  Hadoop Oozie設定からPigのPythonUDFを利用するまでのまとめ - Yuta.Kikuchiの日記     Index   Pigのデータ構造調査  Latinの日本語ドキュメント DataFormatの確認にDESCRIBEを利用する tuple,bagの中身を参照する InputからOutputしたいDataFormatを考える JOINとCOGROUPでは参照の仕方が異なる JOIN/COGROUPのBY属性にnull項目があるときは気をつける STOREした時のtuple/bag/mapは文字列として保存される。再度LOADする時はPig形式のデータで読み込みたい時はASで指定する  その他  文法をCheckしてlocalで実行し、問題なければHDFS上でMapReduceさせる Reducerの数を調節したい STOREするDataをgzip圧縮したい     Pig データ構造調査  Latinの日本語ドキュメント Pigに関する日本語のドキュメントは次のページを見ると詳しく載っています。
Pig Latin の基本 
 DataFormatの確認にDESCRIBEを利用する /* input */ John 17 Men youtube John 17 Men yahoo Kate 18 Women facebook Kate 18 Women ebay Kate 18 Women ebay Tom 19 Men googleまずはDESCRIBEを覚えると良いです。DESCRIBEは変数(alias)がどういったDataFormatなのかをコンソールに出力してくれます。Pig内部のData操作はDESCRIBEで出力してみないと良くわからないことが多いです。変数の中身を確認するにはDUMPで結果を出力するか、STOREで結果ファイルパスに格納することができますが、LOADのデータサイズが大きいと実行に時間がかかってしまうのと複雑なPigは変数にどの変数の一部が格納されているのかが分かりづらくなってしまうので、開発中はDESCRIBEで変数のDataFormatだけをDebug出力して、コードをどんどん書いていくと良いでしょう。下のInputとSampleのPigコードに対してDESCRIBEさせた結果を見てみます。group_dataは名前、年齢、性別、閲覧Domainで共通GROUP化しています。下の説明だけで奥が深くなってしまうのですが、group_dataには共通項として設定したname/age/gen/domainがtuple形式のgroupとして、グルーピングされたdata部分がbag形式として保存されます。groupというデータ形式はGROUPを使った場合に付与されるデータ群、tupleは( )で表現される単純なデータの入れ物、Bagは{ } で表現される色々な種類のデータの入れ物としてまずは覚えておくといいと思います。</description>
    </item>
    
    <item>
      <title>スタートアップを目指す人は必読！起業成功マニュアルの前半を読んでまとめを書きました</title>
      <link>https://yutakikuchi.github.io/post/201301150833/</link>
      <pubDate>Tue, 15 Jan 2013 08:33:08 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201301150833/</guid>
      <description> [起業] : スタートアップを目指す人は必読！起業成功マニュアルの前半を読んでまとめを書きました 完全網羅 起業成功マニュアル
作者: ガイ・カワサキ,三木俊哉出版社/メーカー: 海と月社発売日: 2009/05/29メディア: 単行本（ソフトカバー）購入: 10人 クリック: 110回この商品を含むブログ (25件) を見る
はじめに  どこかのブログに起業をしたい人は必読って書いてあったんで気になって読んでみました。かなり意訳しているところもありますが本の前半部分の内容紹介をします。ソーシャルメディアに取り上げてもらえれば後半も書きますよー。プレゼンの場合は聞き手の最年長の年齢を２で割ったフォントサイズを使いなさい、映画マトリックスのモーフィアス役を確保するなど面白い話が盛りだくさんなので、是非購入して読むことをお勧めします。
  第一章 エンジン始動の奥義  起業の要諦  起業の要諦Top10リストの5項目。  意義を見いだす。組織の意義を見いだす。 標語(マントラ)を決める。意義を基に短いマントラを決める。 走り出す。売り込み、企画、計画ばかりにこだわらず、まずは走り出す。 ビジネスモデルを明らかにする。 ビジネスモデルが無いと短命に終わる。 マット(MAT)を織る。 目標となるマイルストーン(MileStones)、ビジネスモデルの前提仮説(Assumpritions)、達成すべきタスク(Task)。    意義を見いだす  例えば以下のような事が意義。  世界をよりよい場所にする 生活の質を向上。 悪を正す。 優れたものの終わりを防ぐ。    マントラを決める  ミッションステートメントという顧客への奉仕の言葉を考えるのは苦痛。 マントラは短く心地よくなければならない。   走り出す  立派な計画を立てて成功した者などいない。 常に販売ありき。 実行の敵は熟考。 高い目標を持ち、大きな成功を目指す。 成功した企業は二人以上のチームが必要。 サービスを作ったとき、熱烈ファンとアンチがいてOK。 自分が欲しいものを作る。 現在の組織ではできないことをやる。 出来る事を証明する。 よりよいやり方を実現する。 プロトタイプ全力で作って、市場で適用させる。 見込み客に対して修正するのではなく、気に入っているお客に対して修正する。神の恩寵があるから自分が変わるという精神が必要。   ビジネスモデルを明らかにする  誰がお金を持っていて、それをどうやって自分の財布に入れるかの答えを見つけること。 成功起業は特定の市場を決めて、追って他の分野に応用する事で規模を拡大している。 成功しているビジネスモデルを利用や真似をする。 ビジネスモデルのヒントは女性に聞く。女性はキラー遺伝子が無いので、他の企業を葬りたいなどの考えが無い。   MATを織る  重視すべきマイルストーンは以下の7つ。ここに8割は注力する。  コンセプトの裏付け 具体的な設計を完了 プロトタイプを完成 資金調達 テストサービスを提供 最終的なサービスを提供 収支達成  主な仮説は以下の通り  サービスの評価指標 市場規模 粗利益 販売員辺りの訪問(電話)回数 顧客展開率 販売サイクルの長さ 投資収益率(ROI) 出荷単位あたりのテクニカルサポート数 売掛金、買掛金の支払いサイクル 補償要件 部品や補給品の価格 顧客のROI  オフィスを借りる、サプライヤーを探す、会計/給料システムを整備、法的書類提出、保険加入等のタスクを実行する。     第二章 ポジショニングの奥義  優位性の把握  顧客にとってはサービスをひいきすればどんなメリットがあるかが重要。 顧客のために何をすべきか。顧客が自分たちがリーダーである事を証明する。 従業員が自信を付けて、自分の限界を破ってそれを楽しむ事ができるようにする。 特定の顧客をターゲットとする。 ポジションの基礎を固める。風呂敷を広げたりはしない。 競合相手と同じようなポジションであってはいけない。ただし競合相手がいないや全く無能であることはほとんどない。   汝、ニッチより始めよ  マイクロソフトのような会社を作るためにはニッチからスタートさせ、成功したら場所を移す。 ニッチが集まれば市場支配をすることは可能。   ネーミングで妥協しない  アルファベット順で早いものを採用する。XやZを避ける。 数字は使わない、音の響きが区別でき、論理的、流行を追わないなどの工夫が必要。   個人に関連づける  ポジショニングは個人に関わる時の方が効果が大きい。 見込み顧客視点ではサービスのニーズを想像する手間が掛からない。   反対語テスト  競合のサービスと反対の表現であれば独自性がある。そうでなければ無力。 陳腐で無意味、おなじみの形容詞で表されのが自分たちのサービスだけとは限らない。    メッセージを組織中に行き渡らせる  組織のポジションに対して全ての社員に共通の認識を。   流れに身を任せる  市場で自分のポジションをコントロールできない。 市場は自分のありのままのポジションを見つけてくれる。     第三章 売り込みの奥義  企業の要諦3  我売り込む、ゆえに我あり 素早いスタート、事業の妥当性を説明、聞き手の反応を確かめ、うまく行くまで何度でも説明する。   最初の一分で自分を説明  まずは自分が何をしているのかを説明し、聞き手に事業の詳細を理解させる。 聞き手が自分の素性を想像することを避ける。自己紹介は簡潔で分かりやすく。   それで？の声に答える  起業家は自分に常に「それで？」とささかく人をイメージし、耳を傾けるべき。 自分が述べていることの重要性は自明とは限らない。   聴衆を知る  優れた売り込みの基本は打ち合わせの前に行う調査。 スポンサーには以下のような質問をしておく。  自分の組織で知りたい3つの重要事項 MTGの機会を持ちたいと思った理由 MTG前に問題や質問、地雷に備えておくべきか。 MTGに参加する人の最長年齢  以下のような聴衆の情報を集める  組織のミッションステートメント、起源、資金元、設立者。 エグゼクティブのこれまでの所属組織 現在の取り組み    10 / 20 / 30のルール  10枚のスライド、時間は20分、30ポイントのフォント。   10枚のスライド  売り込みの目的は商談成立ではなく、関心を促す事。 スライドの枚数が少ないほど、アイディアは聞き手を引きつける。 10枚のスライド以外では技術やマーケティング、顧客、重要戦略を記述し、詳しい説明を求められた時に使う。   20分  1時間の予定の場合はプレゼンテーションを20分、話し合いの時間を40分とする。   30ポイントのフォント  最年長投資家の年齢を2で割ったフォントサイズを使う。 読むためのスライドではなく、聴衆を引きつけるスライドにする。 人が文章を読むスピードは話す事よりも速いため、聞き手が先に文章を読んでしまうと話を聞いてくれない。   舞台を整える  スタートに失敗したら取り返す事が出来ない。 最悪の事態が発生したときの事を考えて、予備を準備する。 最初に口にすべき言葉は以下のようなもの。  どれぐらいお時間いただけますか？　時間をオーバーしないように聞き手の時間を尊重 私がお伝えできる3つの最重要事項はなんでしょう？ 前もって知っておくべきだが再確認しても問題ない 一通りご説明してから質問を受け付けるという形で宜しいでしょうか？    話すのはひとりだけ  売り込みはCEOが8割話す。 チームメンバはCEOを助けてはいけない。助けは団結力の無さを示している。分からない点についてはCEOが追ってご連絡しますと言うべき。   空想をかき立てる  聞き手に空想の流れや連鎖を意識させる。そうすると調査結果を引用するよりもずっと効果がある。   高さ1000フィートにとどまる  雲の上の世界ではなく、地上のように緊張を強いられる場面でもない1000フィートを目指す。   黙ってメモを取り、まとめ、繰り返し、フォローアップする  黙ってメモを取ると売り込みの場で評価される。 メモを取る事で以下のような印象を与える  頭が良い 話を書き留める価値がある 学びたい意欲 真面目な姿勢  会議の終わりに聞いた内容をまとめて復唱し、正しいかどうかを確認することが出来る 売り込みの約束をフォローアップすることができる   一から書き直す  会議の数が増えてプレゼンの修正を繰り返した場合、原型をとどめなくなってしまう。全体のメッセージが分かりづらくなる。 会議が10回程度の数になったらバージョンを上げて、一から書き直す。   絶えず売り込みを  回数が内容をはぐくむ。 売り込みは臨機応変ではなく、練習。     第四章 事業計画作成の奥義  起業の要諦4  日々の業務にはMATが役立つ。 組織が成功するのは優れた事業計画ではなく、実行力。   作成は正しい理由で  投資家の心が前向きな時は事業計画書は補強する素材に過ぎない。 投資家の心が後ろ向きな場合は事業計画書で心変わりするのは難しい。 投資家は投資対象組織の精査を行う時に事業計画書が必要。 創業チームの一致協力で事業計画書を作りチームがまとまる。 今まで軽視してきた事に対しての振り返り。 創業チームに欠落していたものが見える。   計画は売り込みの後  事業計画書は売り込みを詳しくしたもので、逆ではない。売り込みさえ巧く行けば事業計画もうまくいく。 投資家に拒絶されるか受け入れられるかは売り込みであり、事業計画ではない。 売り込みは内容が少ないから事業計画より修正がしやすい。 売り込みに対する反応はすぐに貰える。 事業計画が無くても、もしかしたら資金調達ができるかもしれない。   エグゼクティブサマリーを重視する  投資家向けの売り込みに必要な10枚のスライドの内容  タイトル 問題 解決策 ビジネスモデル サービスの目玉 マーケティング・販売 競合 経営陣 財務予測と重要指標 現状、既存成果、スケジュール、資金の用途   上の10項を基に事業計画を作成する。エグゼクティブサマリーは事業計画の中核。   読みやすくする  短ければ短いほど読んでもらえる。 書き表すのは一人にした方が良い。 財務予測は簡潔にまとめる。投資家が気にするのはキャッシュフロー。 顧客、事業所数、再販業者数等の重要指標を書く。 財務予測の指標となる数値の前提条件を書く。   適正な数字を示す  財務予測には5年後までが求められる。 以下は投資家が知りたい事。  資金が使われる間の毎月、翌年の四半期毎、利益が出るまでの毎年の数字が見たい。 生み出したキャッシュフローで自立できるまでにいくら必要なのか。 損益計算書、バランスシート、キャッシュフロー計画書。 黒字化までに必要なキャッシュ。 結論を導くために利用された仮説、ビジネスモデル、市場規模、価格、チャネル、粗利益、資本集約度などが重要。    慎重に計画し、緊急的に行動する  慎重な戦略策定プロセスは過去のデータや技術ロードマップ、競合分析を綿密に利用する。実績のある成熟企業に有効。 緊急的な戦略策定プロセスは将来が不透明で適切な戦略策定が難しい時に利用する。新興部門に有効。 投資家は慎重な計画を求める。 未来の事が逐一分かっているような書き方で、現実に直面したら日和見的な対応をする。 多くの組織が途中でビジネスモデルを変更している。 慎重な計画を作ったのに失敗するのが一番良く無い。成功さえしてしまえば、計画通りでなくても誰もが気にしない。    第五章 自己資本経営の奥義  起業の要諦5  起業家がベンチャーキャピタルから出資を得る事が出来る可能性は晴れの日にプールで雷に打たれる確率に等しい。それは言い過ぎで、確率はそこまで高く無い。 たいていの起業家は粗食に耐えながら事業をどうにか発掘、運営していかなければならない。 資金に恵まれない創業当初の日々を如何に乗り切るか。 ベンチャーキャピタルからの資本調達ができないことで可能性を小さく限定するのではな無い。ヒューレットパッカード、デル、マイクロソフト、アップル、ebayのいずれも最初は自己資本投資による経営。   利益よりもキャッシュフローに注意する  自己資本経営ビジネスモデルは以下の特徴を持つ。  最初の必要資本が少ない 販売サイクルが1ヶ月未満 支払い期間が1ヶ月未満 リピート販売がある 口コミで伝わる  以下のような製品/サービスが自己資本経営のターゲット市場。  人々が既に必要と分かっている事 自動的な説得力により、問題と解決方法を購入者が納得しサービスを購入する 市場トレンドが障害を取り除く事 認められているサービスに便乗    ボトムアップ方式で予測する  トップダウン方式の予測だと市場が十分に大きければ、簡単に成功できると思い込んでしまう。 自己資本投資家はトップダウン方式を採用しない、トップダウン=倒産である。彼らはボトムアップ方式を採用して現実的な変数を利用し、スタートする。   まずは出荷、それからテスト  自己資本経営の大きな特徴の一つは、サービスをすぐに市場に出す。 出荷、修正、出荷、修正のサイクルを繰り返す。 すぐにサービスを出す事は売り上げがすぐに手に入る、実際のフィードバックが得られるなどの長所がある。 逆にクオリティに問題があるとイメージが悪くなるなどの短所がある。 すぐにサービスを出すかどうかを判断するための問いかけは以下のようなもの。  競合に勝っているか。 ダメージを最小限にするために限定地域や市場に投入できるか。 進んで実験台になってくれる物分かりの良い顧客はいるか。 サービスは意義を出し、ビジョンを叶えているか。 顧客のニーズを満足させているか。 現状のサービスは顧客に危険や害を及ぼさないか。 十分なテストを行ったので、実際のテストが必要か。    実績あるチームは諦める  経験は無いが可能性の詰まった活力あふれる若者を雇う。 無知は幸福で、力を与えてくれる。 サービスモデルを採用することは自己資本経営に役立つ。 研究開発に対してお金を払ってもらえるのは一時的な戦略でなくてなくてはならない。   形式ではなく、機能を重視する  サービス提供者を見極めるためのヒント。  必要なことを専門とする会社を選ぶ。 惜しみない出費が正しい場合もあると理解する。 仕事をしてもらっている紹介者に尋ねる。 料金、支払いスケジュール、報酬等なんでも交渉する。 気に入らなければ個人や事務所を変える。人生は短いから気に入った相手と働こう。    闘いの場を選ぶ  闘いの場を選び限定する。 誰にでも出来る事をやってお金を稼ごうとしないこと。   直販する  再販業者を間に挟んでしまうと以下のような問題がある。  顧客から乖離してしまう。できるだけ顧客の生の声を聞くべき。 利幅が薄くなる。 顧客に製品を届けるのに時間がかかる。  以上の理由で顧客に直販すべき。   市場リーダーを基準にポジショニングする  世間で既に受け入れられているやり方を基準にしたポジショニング、競争相手のブランド認知を利用する。 例えばレクサス。メルセデスやBMW並みのクオリティで価格が3割安い。 市場リーダーを基準としているので、PRやマーケティングのコストが大幅に削減できる。 市場リーダーとコスト、利便性、デザイン、信頼性、スピード、性能、選択の幅、顧客サービス、地理的な位置などの差別化を図る。 市場リーダを基準にする場合は以下の条件が必要。  リーダーの価値が常に基準となることを持ち続ける。 リーダーが自分の立場を蝕むということが無い。 競合よりも正真正銘勝っている。    赤いピルを飲む  現実と向き合う10の問いかけ  サービスを市場に出す準備がいつできるか。 本当のオペレーションコストはいくらか。 資金がいつ尽きるか。 見込み客のどれぐらいがクロージングに至か。 売掛金のどれぐらいが回収可能か。 競合のサービスは自分のサービスに無いどんな特性があるか。 従業員で戦力になっていないのは誰か。 株主価値最大化のために手を尽くしているか。 意義を見いだすために自分の組織は何をしているか。 組織のリーダとしての手腕はどの程度か。    モーフィアス役を確保する  真実を伝送するモーフィアスのタスクは以下の通り。  サービスに欠陥があると指摘する研究設計担当 事業対応できないと指摘するオペレーション担当 お金の使い方を指摘する財務担当 誤った価値観を教え込んでいると指摘する倫理担当    人員を抑えてアウトソースする  頭数を減らせば良いというだけではない。以下のような問題に直面せざるを得ない。  長期契約を結んでしまった余分なスペース。 余分な家具類やコンピュータ。 解雇があるというトラウマ。 解雇された人に残るトラウマ。 別のタイプの人材を採用したくなる。 組織の内部崩壊を世間に知らせてしまう。  人手不足の短期的な解決策は沢山の機能をアウトソースすること。しかし研究開発、マーケティング、販売と言った戦略的な機能はアウトソースしてはいけない。 自己資本で経営したければ人はあえて少なめにする事。   取締役会を設置する  優れた指導者は常に必要。 お金が取締役を引きつける重要な要素ではなく、サービスの革新性、事業の意義、自身の個性など。 お金を掛けない取締役会を作れば製品/サービスの伝導スキルの証明となる。 優秀な取締役会は資金集めの助けとなる。   大きい事にこだわる  起業家がはした金を惜しんで大局を失うと自己資本は失敗する。 起業家が対処すべき大きな事、小さい事は以下の通り。 小さい事の例はオフィススペース、家具調達、コンピュータ、事務機器、事務用品、名刺、レターヘッド等の準備。 大きい事は製品サービスの開発、販売、販売代金の回収。   実行する  実行の奥義を紹介。  目標を決めて、共有する。 進捗を測る。 責任者を明確にする。 成果を出した人には報いる。 問題が解決するまでやりきる。 モーフィアスの現実的な発言に注目する。 実行の文化を築く。       </description>
    </item>
    
    <item>
      <title>10分でHadoop-Pigの基本文法を理解する</title>
      <link>https://yutakikuchi.github.io/post/201301070827/</link>
      <pubDate>Mon, 07 Jan 2013 08:27:10 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201301070827/</guid>
      <description>[Hadoop] : 10分でHadoop-Pigの基本文法を理解する Hadoop MapReduce デザインパターン ―MapReduceによる大規模テキストデータ処理
作者: Jimmy Lin,Chris Dyer,神林飛志,野村直之,玉川竜司出版社/メーカー: オライリージャパン発売日: 2011/10/01メディア: 大型本購入: 4人 クリック: 254回この商品を含むブログ (16件) を見る
はじめに  年末から使い続けているPigについて勉強した事をまとめていきます。主に以下のDocumentを参照しています。PigのDocumentでLatinを日本語で詳しく紹介しているものが見当たらなかったので、そういった目的でこの記事を参照されている方のお役に立てれば光栄です。
 Getting Started  Pig Latin Basics  PigTutorial - Apache Pig - Apache Software Foundation  Pig Latin の基本  Hadoop Oozie設定からPigのPythonUDFを利用するまでのまとめ - Yuta.Kikuchiの日記  PigでHadoopをより便利に使う！PigでのMapReduceまとめ - Yuta.Kikuchiの日記     Pig Tutorial  tutorial.tar.gzのdownloadとscriptの実行 tutorialのサンプルファイルが入っているtutorial.tar.gzをdownloadして解凍します。解凍したら作成されるディレクトリに移動します。試しにlocalmodeでscript1-local.pigを実行してみます。実行すると結果のディレクトリがlocalに作成されて中身を確認する事ができます。実行したサンプルはexecite-small.logという検索Queryログをtab区切りでparseして、Queryのngramを抽出します。抽出したngramの頻度に対してscoreを付けて、頻度の高いngramを出力しています。では以下では文法を確認して行きます。
$ wget -O tutorial.tar.gz &#34;https://cwiki.apache.org/confluence/download/attachments/27822259/pigtutorial.tar.gz?version=1&amp;modificationDate=1311188529000&#34; $ tar xzf tutorial.tar.gz $ cd pigtmp $ ls -rw-r--r--.</description>
    </item>
    
    <item>
      <title>無駄無駄無駄無駄無駄ァ！ - The Lean StartUpから学ぶ無駄の無い起業プロセス - </title>
      <link>https://yutakikuchi.github.io/post/201301030718/</link>
      <pubDate>Thu, 03 Jan 2013 07:18:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201301030718/</guid>
      <description> [開発手法] : 無駄無駄無駄無駄無駄ァ！ - The Lean StartUpから学ぶ無駄の無い起業プロセス - Running Lean ―実践リーンスタートアップ (THE LEAN SERIES)
作者: アッシュ・マウリャ,渡辺千賀,エリック・リース,角征典出版社/メーカー: オライリージャパン発売日: 2012/12/21メディア: 単行本（ソフトカバー）購入: 3人 クリック: 14回この商品を含むブログ (18件) を見る
The Lean StartUp  
すみません、タイトルは釣りです。正月で時間があったのでThe Lean StarUpを読みました。StartUpにおける構築-計測-学習のプロセスを短く回す事に寄って時間の無駄を無くす事を目的とした本です。量は360ページありますが分かりやすい話なので、1日あればスラスラ読む事が出来ると思います。私個人が凄く忘れやすい人間であり、本を毎回捲るのが面倒であるため要点を箇条書きでまとめておきます。
リーンスタートアップまとめ - NAVER まとめ 
  はじめに   スタートアップのほとんどが失敗する。 正しいやり方で進めるからこそ成功する。 IMVU開発ではユーザの意見を取り入れなかった。 顧客から望みを聞くわけではなく、顧客の望みを中心とする意思決定を科学的に行う。 リーンスタートアップの5原則  アントレプレナーはあらゆるところにいる 企業とはマネジメント 検証による学び 構築-計測-学習 革新会計     スタート   とにかくやってみようという方針では混乱を招くことが多い。 リーンスタートアップでは検証による学びを単位として進歩を計測する。 スタートアップの目標はできるかぎり早く、作るべきモノ(顧客が欲しがりお金を払ってくれるモノ)を突き止めること。 スタートアップに目的が存在する。  目的地 = vision visionを実現するためのstrategyを採用する strategyから生み出される成果物がproduct  visionはめったに変えないが、strategyとproductはチューニングを行う。 スタートアップとはとてつもなく不確実な状態で新しい製品やサービスを作り出さなければならない人的組織である。 大企業は製品を少しずつ改良し顧客を満足させる持続的イノベーションを得意とし、画期的な新製品を出す破壊的なイノベーションは不得意な場合がある。 企業の経営陣が従業員がイノベーションを進めることができるような環境を作らなければならない。    学び   検証による学びは進捗を的確に図るための方法。 無駄を発見し、それを体系的に無くしていく方法を学べばトヨタのようにリーンな企業となり、業界をリードする立場に立てる。 リーンにおける価値とは顧客にとってのメリットを指し、それ以外は全て無駄だと考える。 スタートアップの場合は顧客が誰か、顧客の価値が何かを見出すことができない。 顧客はこう望んでいるはずと自分の考えを正当化することは簡単。 実験から新しいことに気づきそれを元に当たらし実験をまた考える。 実験は最初の製品である。    舵取り   構築-計測-学習のトータルサイクルを最小にすることが重要。 実用最小限の製品を作り、革新会計による計測を行う。 トータルサイクルを終えたときに方向転換するか、当初戦略を維持するかの問いに直面し、仮説に一つでも誤りがある場合は新しい戦略的仮説に方向転換する必要がある。 類例と反例から答えが得られていない問いが明らかになる。 成功と失敗を分ける鍵は計画のうまく行っている部分と道を誤っている部分を見つける先見性と能力、ツールをアントレプレナーが持っていて、戦略を状況に順応させられるか。 持続的イノベーションの場合、現地・現物主義で顧客のニーズを確認できるがスタートアップの場合はどの仮説から検証すべきかぐらいしか分からない。 実用最小限の製品は事業仮説を検証するためのもの。 誰が顧客なのかが分からなければ、何が品質なのかも分からない。 顧客が気にするのは自分にとって良いか悪いか。 実用最小限の商品を作るときは求める学びに貢献しない機能やプロセス、労力は全て削除すること。 スタートアップは競合他社に直面するので、勝つためには他よりも速いスピードで学ぶしか道は無い。 スタートアップがやらなければならないこと  現状を的確に計測し評価で明らかになった厳しい現実を直視する 理想に現実の数字を近づける方法が学べる実権を考案する  革新会計は以下の3段階  実用最小限の製品を作成 ベースライン状態から理想状態へのエンジンチューニング 方向転換するか辛抱するか  独立した顧客グループの数値を比較するコホート分析の導入。 異なるバージョンの製品を同時に顧客に試してもらうスプリットテストの導入。 虚栄の評価基準は捨てて行動につながる評価基準を採用する。 虚栄の評価基準は人の弱さにつけ込む。数値が上がると行動が改善をもたらしたと考え、数値が下がると自分以外の誰かのせいにしたがる。 ヴォディズンは実用最小限の製品作成のスピードアップを図り、方向転換時にはレガシーな商品を捨てた。 スタートアップに残された時間は資金から資本年少率を出すことではなく、ピボットを後何回できるかで測る。 ピボットには勇気が必要。ピボットに遅れてしまうのは以下の3つの理由  虚栄の評価基準から偽の認識を導き出してしまう 仮説が曖昧だと完全な失敗が無くなり、完全な失敗が無いから根本的な見直しが無い 失敗を認めると士気が下がる  アントレプレナーは失敗する気概を持たないといけない。 スタートアップの場合は定期的に会議を開いて方向転換か辛抱かを検討すべき。 ピボットとは作ったものや学んだものを再利用してもっとも優れた方向を見つけること。 アーリーアダプタとメインストリームで求めるものが異なる。 ズームイン型ピボット  一機能と考えていたものがメインになる。  ズームアウト型ピボット  ズームインの逆で製品全体を一機能と考える。  顧客セグメント型ピボット  顧客層を変更する  顧客ニーズ型ピボット  顧客と自分たちが問題解決したいことを別に発見し、方向転換する  プラットフォーム型ピボット  アプリケーション-プラットフォーム間の方向転換  事業構造型ピボット  企業自体が高利益率・少量の複合システムか低利益率・大量の大量操業に分けられ、これらの事業構造を切り替えること  価値補足方ピボット  企業が生み出す価値を貨幣化や収益モデルとして捕らえる  成長エンジン型ピボット  スピードアップや利益向上を実現するため  チャネル型ピボット  ソリューションを他のチャネルで提供する  技術型ピボット  同じソリューションを他の技術で実現する     スピードアップ   一つ一つのプロセスに要する時間が全く同じ場合でもバッチサイズが小さいほうが効率的になる。 バッチサイズを小さくすれば構築-計画-学習のフィードバックループを競合他社より短いサイクルで回せる。 プッシュ方式をプル方式にする効果とバッチサイズを縮小する効果を持つ。 構築-計測-学習のトータルサイクルを計画するときは逆順で考える。 トヨタのすごいところは史上最高レベルの学ぶ組織を作り上げた点。 持続的な成長とは過去の顧客行動が新しい顧客を呼び込むこと。 過去の顧客が新しい顧客を呼び込む形式は以下の4つ  口コミ 利用効果 有料広告 リピート購入・利用  成長エンジンには3つ存在する。 粘着型成長エンジン  新規顧客の契約速度が解約速度を上まれば成長する  ウィルス型成長エンジン  顧客一人が何人の顧客を連れてくるかが係数となる  支出型成長エンジン  顧客の獲得に再投資できる金額の売り上げに占める割合が一致している場合は成長速度は同じ。 成長速度を上げたければ顧客あたりの売り上げを増やすか顧客の獲得コストを下げる。  3種類の全ての成長エンジンを並行してモデル化するのはややこしい作業。 時間のために品質を犠牲にしてはならない。 問題に対して「なぜ？」という疑問を5回ブレークダウンして真因を見つけて正すことができる。 5回のなぜを繰り返して5回の誰になって悪者を探してしまうのは良くない。 人間ではなくプロセスに問題があったことを見つける。 5回のなぜの学び促進には分野ごとに責任者を置くと良い。 スタートアップは守備範囲において自由に開発、マーケティングする権限が無くてはならない。 チームは全部門をカバーするメンバで構成する。 チームは小さくなくてはならない。 成果にはアントレプレナーの利害がかかっていなければならない。 組織メンバが自己防衛に走る環境ではイノベーションは生まれない。 イノベーションが自由に行えるsandboxを用意する。 新しい製品を開発した創造性のあるマネージャーが部門や資源の管理を行うケースが多く、新しいイノベーションが生み出しづらい。 社員一人一人の得意分野を理解して、製品の段階にあわせて担当者をバトンタッチする。 イノベーションサンドボックスはチームと親組織を守るための仕組み。 人間の時間を乱用するのは創造性と可能性の過失無駄罪。 定量的な目標を設定することではなく、目標を達成するための方法を整えること。    </description>
    </item>
    
    <item>
      <title>Hadoop Oozie設定からPigのPythonUDFを利用するまでのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201212291432/</link>
      <pubDate>Sat, 29 Dec 2012 14:32:53 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201212291432/</guid>
      <description>[Hadoop] : Hadoop Oozie設定からPigのPythonUDFを利用するまでのまとめ Hadoop 第2版
作者: Tom White,玉川竜司,兼田聖士出版社/メーカー: オライリージャパン発売日: 2011/07/23メディア: 大型本購入: 9人 クリック: 182回この商品を含むブログ (24件) を見る
Oozie  OozieとはHadoop MapReduceのジョブ管理システムの事でMapReduceの定期処理化や複数のMapReduceの実行し結果を一つにまとめるなど一連の処理フローとして定義することができる優れものです。Oozie自体はJava/Tomcatで作られているようです。Oozieを動かすために開発者は以下のものを用意しなければなりません。
   ファイル   必須   記述方式   用途   設置場所     MapReduceプログラム   必須  Java,Streaming,Pig/Hive等   MapReduce実行   HDFS     workflow   必須   xml   OozieJobの実行   HDFS     coordinator   定期化する場合必須  xml   Ooziejobの定期化   HDFS     properties   必須  設定ファイル   Ooziejobの実行パラメータファイル   Oozie client   Oozieを動かすためのパラメータファイル以外は全てHDFSのパスに設置する必要があります。workflowがMapReduceの一連処理の流れを定義し、coordinatorは定期処理をしたい場合に定義するxmlファイルです。propertiesはJobTracker,NameNodeやOozieのjobに渡すパラメータの設定ファイルです。</description>
    </item>
    
    <item>
      <title>PigでHadoopをより便利に使う！PigでのMapReduceまとめ</title>
      <link>https://yutakikuchi.github.io/post/201212170830/</link>
      <pubDate>Mon, 17 Dec 2012 08:30:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201212170830/</guid>
      <description>[Hadoop] : PigでHadoopをより便利に使う！PigでのMapReduceまとめ Hadoop Hacks ―プロフェッショナルが使う実践テクニック
作者: 中野猛,山下真一,猿田浩輔,上新卓也,小林隆出版社/メーカー: オライリージャパン発売日: 2012/04/25メディア: 単行本（ソフトカバー）購入: 3人 クリック: 156回この商品を含むブログ (8件) を見る
Pig  HadoopのMapReduceを独自で記述するのは手間が掛かります。それらの手間を出来るだけ緩和させるための便利なツールとしてDSL形式の処理フローを定義する事でMapReduceを実行するHiveやPIgというものが存在します。HiveとPigはライバルブロジェクトのようで、本日紹介するPigはYahoo!が開発しているミドルウェアになります。Hiveについては以前簡単に紹介をしたので以下のリンクを参考にしてください。PigLatinという手続き型の文法でDataのload/filter/join/sort/group/join/limit/storeなどの処理を組み合わせ、inputからoutputまでの一連のMapReduceを定義する事ができます。JavaやStreamingでのMapReduceでは目的のデータを抽出するために多段MapReduce処理を記述する事もありますが、PigやHiveを使うと1回の実行で済んだりします。
Hadoopをより便利に使う！HiveでのMapReduceまとめ - Yuta.Kikuchiの日記 
  Pigの実行  Install CentosでInstallする内容については以前まとめたので、詳しくはそちらを参照してください。
CentOSでHadoopを使ってみる - Yuta.Kikuchiの日記 
必要なyum installは以下のものになります。
$ yum install hadoop-0.20 -y $ yum install hadoop-0.20-conf-pseudo -y $ yum install hadoop-pig -y  PigLatin Pig Latin Basics 
PigLatinとはPigの便利な文法のことです。ここでは代表的なLatinをいくつか紹介します。
   文法   役割       LOAD   データのファイルシステムから読み込み     STORE   データをファイルシステムに保存する     DUMP   結果をコンソールに出力する     FILTER   条件を指定しデータのフィルタリングを行う       MATCHES   データを正規表現の条件でフィルタリングする     FOREACH   繰り返しデータ変換を行う     SPLIT   データのを分割する     JOIN   データの結合を行う     GROUP   データのグループ化を行う     ORDER   データをソートする     DISTINCT   データの重複を排除する     LIMIT   データの出力件数を制限する     SAMPLE   データのサンプリングを行う     LocalModeで実行 Pigはlocalmodeとmapreducemodeをそれぞれ実行で切り分ける事が出来ます。localmodeはjobを投げるサーバの1台で、データのLoad/Storeもローカルサーバのものに対して行います。mapreducemodeはHDFS上のデータを利用し、全Hadoopクラスタに対して分散処理を行うモードになります。localで実行したい場合は-x localというオプションを起動時に設定します。pigコマンドを実行すると対話コンソールが出力されます。</description>
    </item>
    
    <item>
      <title>3ヶ月間Hadoopを使ってみて学んだ事</title>
      <link>https://yutakikuchi.github.io/post/201211261210/</link>
      <pubDate>Mon, 26 Nov 2012 12:10:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201211261210/</guid>
      <description>[Hadoop] : 3ヶ月間Hadoopを使ってみて学んだ事 Hadoop 第2版
作者: Tom White,玉川竜司,兼田聖士出版社/メーカー: オライリージャパン発売日: 2011/07/23メディア: 大型本購入: 9人 クリック: 182回この商品を含むブログ (24件) を見る
Overture  BigData解析という仕事をやり始めて半年、Hadoopを業務で使い始めて3ヶ月以上が経過したのでここで今までの業務での知識をまとめてみたいと思います。先日参加したWebDBForum2012でも各種企業がBigData(主にログ)からユーザの趣味思考や特徴などを解析して表示システムへのFeedBackや企業戦略などに活かしている報告があり、Hadoopなどの分散処理技術や今後は更にリアルタイムでBigDataを使うためのミドルウェアが出てくることが予想され、そこに精通した人間が求められるようになってくると思います。
第5回 Webとデータベースに関するフォーラム (WebDB Forum 2012) 
  Hadoop  Hadoop - Wikipedia 
Hadoopの説明を簡単に。大量のデータを扱うために従来のスタンドアロンで処理を行うのではなく、大量のマシンに処理を分散させて解析スピードを劇的に改善することを目的としたフレームワークです。例えば200GほどのApacheのAccessLogからUserのCookie、Refererや利用しているUserAgent/Deviceなどをデータを1日毎に集計しようとした場合、1台スクリプトで計算していたら翌日の集計に間に合いません。しかしこれをHadoopの分散処理を使えば1-2時間で処理を完了させることが出来ると思います。
merit / demerit Hadoopを利用するmerit/demeritを箇条書きで書きます。
 merit  大量のデータを複数台で分散し、解析処理を高速化。 スケールアウトが比較的楽にできる。 面倒な分散処理は全てHadoopが行ってくれる。Hadoopユーザは解析処理に専念できる。 導入実績が豊富で信頼ができる。  JAVAが苦手なユーザでもStreamingを使う事でPerl/Python/Rubyなどの言語でMapReduceを記述する事が可能。    demerit  逐次処理を行う場合には不向き。 リアルタイム性を求める処理には不向き。 多段のMapReduce処理を重ねないと集計が出来ないケースがある。 MapReduceのテクニックを学ばなければならない。    ServerStructure Hadoopのクラスタはmaster/slaveの二つに分けられます。masterとなるのは1台のサーバでそこを起点にslaveノードに対してTask,Dataが割り当てられます。master/slaveのサーバの内部でもMapReduce処理を行うサーバ、HDFSのファイルを管理するサーバに分けられ、それぞれに対してJobTracker/TaskTracker/NameNode/DataNodeと名前が定義されています。
 JobTracker  HadoopClientからjobを受け取る。Slaveサーバに対してTaskを分割する。  TaskTracker  Masterサーバから受け取ったTaskを実行する。  NameNode  実際のデータがDataNodeのどこに格納されているのかを管理するメタデータサーバ。  DataNode  NameNodeによって管理されている実際のデータ。ブロックと呼ばれる単位で格納されている。ブロックはデフォルトで64MByte。ファイルは設定に応じてreplication管理されている。  以下は概要図となりますが、Clientから投げられたHadoopのJobをMasterのJobTrackerが受け付けてSlaveのTaskTrackerに流します。Clientから投げられたHadoopのJobのINPUT_PATHはNameNodeに問い合わせを行い、どのDataNodeにデータが格納されているのかを取得します。JobTrackerはTaskTrackerに対してDataNodeのファイルを指定し、TaskTrackerが実際のファイルを取りに行くような仕組みです。</description>
    </item>
    
    <item>
      <title>Mahoutを使ったNaiveBayesによる機械学習</title>
      <link>https://yutakikuchi.github.io/post/201211130837/</link>
      <pubDate>Tue, 13 Nov 2012 08:37:41 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201211130837/</guid>
      <description>[機械学習] : Mahoutを使ったNaiveBayesによる機械学習 入門 ソーシャルデータ ―データマイニング、分析、可視化のテクニック
作者: Matthew A. Russell,奥野陽（監訳）,佐藤敏紀（監訳）,瀬戸口光宏（監訳）,原川浩一（監訳）,水野貴明（監訳）,長尾高弘出版社/メーカー: オライリージャパン発売日: 2011/11/26メディア: 大型本購入: 18人 クリック: 779回この商品を含むブログ (42件) を見る
BigDataでの機械学習  膨大なデータに対して機械学習を行いたい時にlocalの端末一台では処理の時間が掛かりすぎてしまいます。学習、モデル作成、予測のそれぞれの処理を高速で行うための一つのSolutionがHadoop上で機械学習をしてしまうことだと思います。Hadoop上で機械学習をするための便利なライブラリとしてJAVAベースのMahoutがあります。この記事ではMahoutによるNaiveBayes分類学習を中心としたMahoutデータの生成と使い方について紹介します。
Machine Learning With Hadoop - Yuta.Kikuchiの日記 
  NLTKによる分かち書き  NLTK Install Pythonの自然言語処理ライブラリのnltkを利用して分かち書きを行います。nltkのセットアップは非常に簡単で以下のコマンドを実行するだけです。Installing NLTK ― NLTK 2.0 documentation 
$ python -V Python 2.6.6 $ wget &#34;http://pypi.python.org/packages/2.6/s/setuptools/setuptools-0.6c11-py2.6.egg#md5=bfa92100bd772d5a213eedd356d64086&#34; $ sudo sh setuptools-0.6c11-py2.6.egg --prefix=/usr/ $ sudo easy_install pip $ sudo pip install -U numpy $ sudo pip install -U pyyaml nltk  Mecab Install 形態素解析器で有名なMecabをPythonから利用できるようにします。Mecab本体、Mecab-ipadic、mecab-pythonの3つをinstallします。mecab-pythonのinstallの時に&#34;</description>
    </item>
    
    <item>
      <title>lookコマンドによる二分探索が速すぎて見えない</title>
      <link>https://yutakikuchi.github.io/post/201210290836/</link>
      <pubDate>Mon, 29 Oct 2012 08:36:38 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201210290836/</guid>
      <description>[Linux] : lookコマンドによる二分探索が速すぎて見えない Linuxコマンドブック ビギナーズ 第2版 コマンドブックシリーズ
作者: 田谷文彦,三澤明出版社/メーカー: ソフトバンク クリエイティブ発売日: 2007/04/11メディア: 単行本 クリック: 3回この商品を含むブログ (2件) を見る
grep vs look  数GByte容量の圧縮ファイルから特定の文字列を検索したい場合があります。一度きりのgrep検索処理であればそれほど気にする事はありませんが、System処理で何度も検索をするようなケースでは処理に時間がかかってしまいます。今日はsortされたファイルに対してlookという2分探索コマンドを利用するとgrepより高速に検索が可能ということを調べたいと思います。
  lookコマンドの活用  lookは通常の場合辞書ファイルからスペルを確認するために利用されます。例えばmorpholoと先頭一致する単語一覧を取得したい場合は$ look morpholoと実行します。単語一覧の辞書データは/usr/share/dict/wordsに配置されています。
$ look morpholo morphologic morphological morphologically morphologies morphologist morphologists morphology morpholoical辞書ファイルを見るとわかりますがlookを使うには予めデータをsortしておく必要があります。これはlookの内部処理で2分探索を行っているためだと思われます。lookは上の辞書だけでなくgrepと同じように引数に指定したファイルから検索を行う事ができます。lookのusageは以下のようになっています。
look 使い方: look [-dfa] [-t キャラクタ] 文字列 [ファイル]   圧縮ファイルへの検索  検索手段 圧縮ファイルに対する文字列検索の手段で思い当たるものは以下のものです。以下のそれぞれを検証して行きます。
 zgrepで検索 zcatで出力 + grepで検索 ファイル解凍 + grepで検索 ファイル解凍 + sort + lookで検索   検索対象ファイル作成 手元に手頃なgzファイルが無かったので16進数の文字列を改行で出力するスクリプトで作成します。以下のスクリプトで生成されるログファイルは5.</description>
    </item>
    
    <item>
      <title>Machine Learning With Hadoop</title>
      <link>https://yutakikuchi.github.io/post/201210150838/</link>
      <pubDate>Mon, 15 Oct 2012 08:38:00 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201210150838/</guid>
      <description>[機械学習] : Machine Learning With Hadoop Machine Learning: A Probabilistic Perspective (Adaptive Computation and Machine Learning series)
作者: Kevin P. Murphy出版社/メーカー: The MIT Press発売日: 2012/08/24メディア: ハードカバー購入: 1人 クリック: 26回この商品を含むブログを見る
Big DataのMachine Learning  Daily数百ギガバイトのAccessLogからDataMiningに必要なFeatureをかき集めるのにスタンドアロンの端末で処理を行うには時間が掛かりすぎます。バッチ処理で1日以内にUserのAccessLogを整形、必要な部分を取り出してDataMining/Machine Learningに掛けて、Userが利用するSystemに反映して行こうと考えると最初のバッチ処理で利用できる時間はそれほど多くありません。処理時間改善のためにHadoopを使い複数台のマシンに大量のログデータとバッチ処理を分散させる仕組みはBig Dataを扱う人の中では常識として利用されています。今日はBigDataをMachineLearningさせたいときの方法について調べた内容を載せます。Apache Mahout、PigのUDF、独自MapReduceの3つのうちどれかを使う事になりそうです。
Apache Mahout  Hadoop上で実行可能な機械学習ライブラリ。ファイルをDownloadして展開するだけで利用可能。一番お手軽だが未実装のAlgorithmもあり、公開されているPatchファイルを当てるなどの対応が必要になる場合がある。 Apache Mahout: Scalable machine learning and data mining  Algorithms   Classification Clustering Pattern Mining Regression Dimension reduction Evolutionary Algorithms Recommenders / Collaborative Filtering Vector Similarity    Pig  Pig専用のScriptを書く事でMap/Reduceが可能。データのjoinなどもできる。Hiveのライバルプロジェクト。MachineLearningを行う場合はuser-defined functions (UDFs)を書く必要がある。MachineLearningのUDFsはほとんどWeb上に公開されていない。TwitterはPigを使って機械学習している。 Welcome to Apache Pig!</description>
    </item>
    
    <item>
      <title>Pythonのscikit-learnでRandomForest vs SVMを比較してみた</title>
      <link>https://yutakikuchi.github.io/post/201210120820/</link>
      <pubDate>Fri, 12 Oct 2012 08:20:41 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201210120820/</guid>
      <description>[機械学習] : Pythonのscikit-learnでRandomForest vs SVMを比較してみた Random Forest
メディア: ペーパーバック クリック: 27回この商品を含むブログ (1件) を見る
Random Forest  Random Forestとは  Random forest - Wikipedia  Random forests - classification description  機械学習の方法論の一つで決定木ベースの集団学習アルゴリズムを取り入れたものです。説明変数の依存が少ないことや学習が高速であることが特徴として挙げられています。英語サイトの方で特徴として紹介されているRFの内容について記述します。
 Features  大きなデータに対して効率よく処理される。 変数の削除をすることなく入力した数千の変数を扱う事ができる。 どの変数が分類に対して重要なのかを計算して与えてくれる。 木の構築処理中に一般的なエラーの偏りの無い計算を生成する。 高い割合でデータが誤っている時に誤りのデータを計算し、精度を保つ効果的な手法を持っている。 アンバランスなデータが与えられたクラス群の中でエラーのバランスに対する手法を持っている。 生成された木は今後他のデータに適用させるために保存する事ができる。 変数とクラスタリング間の関係性に関する情報を計算する。 クラスタリングに利用される隣接するケースを計算する。 上の特性はラベリングやクラスタリングされていないデータやはずれ値に対しても拡張する事ができる。 変数の相互作用を発見するための実験的な方法を推薦する。   Remarks  RFではoverfitは存在しない。 RFは複数の木として処理できるし、それは処理速度が速い。 5万のデータと100の変数を持ったデータに対して、100個の木に割り当て、800Mhzのマシンで11分で処理が終わる。 RFではCross-Validationをする必要がない。out-of-bag (oob) エラー計算がその代わりとなる。   テキストデータ分類器の比較  http://mjin.doshisha.ac.jp/R/200905_70.pdf このPFD-PaperによるとRFがマクロ平均のF値において最も精度が高いと言われていますが、どんなデータに対しても精度が高いとは言えないと思います。画像引用 : 図6 : 10のテーマにおけるマクロ平均のF1 値の平均プロット

 SVMについて SVMには以前に記事を書いたので以下を参照してください。</description>
    </item>
    
    <item>
      <title>線形予測の機械学習ツールliblinearで効果最大化のための最適な定数Cを探る</title>
      <link>https://yutakikuchi.github.io/post/201210090830/</link>
      <pubDate>Tue, 09 Oct 2012 08:30:06 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201210090830/</guid>
      <description>[機械学習] : 線形予測の機械学習ツールliblinearで効果最大化のための最適な定数Cを探る Machine Learning for Hackers
作者: Drew Conway,John Myles White出版社/メーカー: Oreilly &amp; Associates Inc発売日: 2012/02/28メディア: ペーパーバック クリック: 63回この商品を含むブログを見る
liblinear   LIBLINEAR -- A Library for Large Linear Classification  10秒で設定可能なlibsvmで機械学習を行う - Yuta.Kikuchiの日記  R言語でSVM(Support Vector Machine)による分類学習 - Yuta.Kikuchiの日記  今日はliblinearを用いた機会学習の話です。今まではSVMを利用するときはkernelオプション付きのR言語のSVM/libsvm/svm-lightを利用していましたが、学習データが多い時に計算時間が何時間も掛かる事に不便を感じていました。そこでSVMのツールについて色々と調べてみたところ、線形予測に特化したliblinearの存在を知りました。公式のDocumentにもlibsvmとliblinearでの線形予測での処理時間が桁違いにliblinearの方が優れていることが記述されています。以下にliblinearの特徴を記述します。
 liblinearはinstanceや特徴が100万桁のデータを線形分離するためのtoolであり以下をサポートしています。  L2正則化の分類  L2-loss linear SVM, L1-loss linear SVM, and logistic regression (LR)  L1正則化の分類  L2-loss linear SVM and logistic regression (LR)  Support Vechtor RegressionのL2正則化  L2-loss linear SVR and L1-loss linear SVR   正則化とはOverfittingを回避するために罰則項を与える事です。種類としてはL1,L2,L1L2の3つが良く利用されるもので精度とスパース性によって異なります。L1は精度が低くスパース性が高い、L2は精度が高くスパース性が低い、L1L2は両方を取り入れ精度を高く保ちながらスパース性を高くすることです。</description>
    </item>
    
    <item>
      <title>そろそろ本気で機械学習の評価方法について学習するよ</title>
      <link>https://yutakikuchi.github.io/post/201209100835/</link>
      <pubDate>Mon, 10 Sep 2012 08:35:34 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201209100835/</guid>
      <description>[機械学習] : そろそろ本気で機械学習の評価方法について学習するよ Machine Learning for Hackers
作者: Drew Conway,John Myles White出版社/メーカー: Oreilly &amp; Associates Inc発売日: 2012/02/28メディア: ペーパーバック クリック: 63回この商品を含むブログを見る
機械学習の評価方法について学習  機械学習初心者ですが最近業務で本格的に触り始めています。少し前までSmartPhoneのWebAppliを作ることを専門職としていたので機械学習の領域は未知な事が非常に多く、用語の意味ですら十分に理解できていません。今日は機械学習の評価方法を中心に学習(勉強)した内容を記録して行きます。例えばPrecision/Accuracy/Recallの言葉の違いやROC曲線,AUC評価などの技法といったものが話の中心になります。初心者視点で書いていますので専門性がありません。間違い等ありましたらご指摘ください。また以前にもはじめての機械学習という本のサマリーを書いたのでそちらも参照していただけると嬉しいです。
初めての機械学習理論 - Yuta.Kikuchiの日記 
  用語定義  初めに慣れない用語が多いので、その意味を定義します。
   用語   意味     K-Fold Cross-Validation   標本をK子に分割してK-1個を学習データ、1個を評価データとして扱い、K回検定を行い推定の平均を得る     Leave-One-Out Cross-Validation   標本から1つの事例を取り出して評価データとし、残りを学習データとする。全事例が1回は評価となるように検定を繰り返す。    Accuracy   正解率のこと。予測結果全体と、答えがどれぐらい一致しているかを判断する指標。計算式は下記を参照。     Precision   適合率のこと。予測を正と判断した中で、答えも正のもの。計算式は下記を参照。     Recall  再現率のこと。答えが正の中で、予測が正とされたもの。計算式は下記を参照。       F-measure   F値のこと。予測精度の評価指標。PresicionとRecallの調和平均。計算式は下記を参照。     ROC曲線   Receiver Operating Characteristicのこと。縦軸にTrue Positive、横軸にFalse Positiveの割合を2次元プロットして点を線で連結した曲線     AUC   Area Under the Curveのこと。ROC曲線の曲線よりしたの面積。分類器の精度評価に使う。    マイクロ平均   Nセットのテストをする場合、テストを合計してから評価値を計算。計算式は下記を参照。     マクロ平均   Nセットのテストをする場合、各セットを計算してからそれらを平均する計算。計算式は下記を参照。     True Positive   正しくPositiveと判断。予測が正解しているのでOK。     False Positive   誤ってPositiveと判断。予測が不正解なのでNG。     False Negative  誤ってNegativeと判断。予測が不正解なのでNG。     True Negative   正しくNegativeと判断。予測が正解しているのでOK。       -   事実が1   事実が-1     予測が1   True Positive(TP)   False Positive(FP)     予測が-1   False Negative(FN)   True Negative(TN)   AccuracyとPrecisionは相関関係になり、PresicionとRecallは逆相関の関係になるのが一般的です。PresicionとRecallともに予測が正しい事の指標ですが、数式で表すと分母が予測ベースか事実ベースかが異なります。どっちだっけという判断が非常に難しいです。</description>
    </item>
    
    <item>
      <title>joinコマンドが便利過ぎて生きるのが辛い</title>
      <link>https://yutakikuchi.github.io/post/201209070848/</link>
      <pubDate>Fri, 07 Sep 2012 08:48:01 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201209070848/</guid>
      <description>[Linux] : joinコマンドが便利過ぎて生きるのが辛い Linuxシステムプログラミング
作者: Robert Love,ロバートラブ,千住治郎出版社/メーカー: オライリージャパン発売日: 2008/04/16メディア: 大型本購入: 5人 クリック: 181回この商品を含むブログ (29件) を見る
結合  Unix/Linuxの標準コマンドで2つのファイルの共通keyで連結することができます。共通keyでの結合にはjoinコマンドを利用します。joinによりSQLのinner joinに近いことがコマンドだけで出来てしまいます。今までテキスト処理をコマンドで行う事が少なかったのでjoinの活用方法を知りませんでしたが、今回調べた内容を記録します。似たコマンドとしてpasteというものもあり、こちらは同じ行数の内容を単純に結合します。そちらについても簡単に紹介します。
  join  join前にsort joinコマンドを利用する場合は2つのファイルがそれぞれ結合するフィールドでsortしておく必要があります。sortしないで実行すると期待通りの結果が得られません。
 joinコマンドオプション    オプション   役割     -1 n   File1のn番目のフィールドを用いてjoinする     -2 n   File2のn番目のフィールドを用いてjoinする     -a File   ファイルにあるペアにならなかった行を通常の出力に追加     -e string   入力にFieldがなかった場合はそれに対応する出力フィールドをstringにする     -i, --inore-case   キーを比較する時に英大文字小文字の違いを無視     -j n   -1 n ,-2 nと同じ     -o Field-list   出力のフォーマットにField-listを用いる       -t char   入力/出力フィールド区切り文字にcharを指定     -v File   ペアにならなかった行だけを出力       先頭カラムでjoin $ cat AA.</description>
    </item>
    
    <item>
      <title>Support Vector Machinesを用いた「魔法少女まどか☆マギカ」人物予測モデル</title>
      <link>https://yutakikuchi.github.io/post/201209040835/</link>
      <pubDate>Tue, 04 Sep 2012 08:35:36 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201209040835/</guid>
      <description>[自然言語処理] : Support Vector Machinesを用いた「魔法少女まどか☆マギカ」人物予測モデル 言語処理のための機械学習入門 (自然言語処理シリーズ)
作者: 高村大也,奥村学出版社/メーカー: コロナ社発売日: 2010/07メディア: 単行本購入: 13人 クリック: 235回この商品を含むブログ (39件) を見る
人物予測モデル  記事のタイトルがだいぶ固い内容になっていまいましたがやりたい事はとても簡単です。過去に発せられたまど☆マギ台詞の形態素を学習し、予測モデルを作成します。その後に未分類の形態素のデータセットを与えた時にどれだけ人物のラベル付けが正しく行われたかを評価します。予測モデルの対象となる人物は鹿目まどか/暁美ほむら/美樹さやか/キュゥべえ/佐倉杏子/巴マミの合計6名です。機械学習にはSVMを利用します。先に実験の結果をお伝えしておくと、台詞の形態素ベクトルでは十分なマルチラベリングができていません。それでもこの記事が気になる方は読み進めてください。処理手順の詳細は以下の通りです。
 まど☆マギ台詞の収集。 発言者の1行台詞を形態素解析し、形態素IDと形態素出現回数をベクトル化。 TrainデータとPredictデータを分離する。 SVMを利用したTrainデータの学習。Model作成。 Kfold-Cross-Validation実施。Modelの評価。    まど☆マギ台詞の収集  魔法少女まどか☆マギカ　WIKI - ネタバレ考察/台詞集 
上のWIKIの台詞を利用させてもらっています。※承諾は得ていません。
Pythonコードで各登場人物の台詞を取得します。Webページのスクレイピングによる抽出です。実行すると各登場人物ファイルにデータを落とし込みます。
#!/usr/bin/env python # -*- coding: utf-8 -*- import sys,re,urllib,urllib2 urls = { &#39;http://www22.atwiki.jp/madoka-magica/pages/131.html&#39; : &#39;madoka.txt&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/57.html&#39; : &#39;homura.txt&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/123.html&#39; : &#39;sayaka.txt&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/130.html&#39; : &#39;mami.txt&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/132.html&#39; : &#39;kyoko.txt&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/56.html&#39; : &#39;kyube.txt&#39; } opener = urllib2.</description>
    </item>
    
    <item>
      <title>10秒で設定可能なlibsvmで機械学習を行う</title>
      <link>https://yutakikuchi.github.io/post/201208290841/</link>
      <pubDate>Wed, 29 Aug 2012 08:41:30 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201208290841/</guid>
      <description>[機械学習] : 10秒で設定可能なlibsvmで機械学習を行う Support Vector Machines (Information Science and Statistics)
作者: Ingo Steinwart,Andreas Christmann出版社/メーカー: Springer発売日: 2008/08/29メディア: ハードカバー クリック: 17回この商品を含むブログを見る
libsvm   LIBSVM -- A Library for Support Vector Machines  R言語でSVM(Support Vector Machine)による分類学習 - Yuta.Kikuchiの日記  前回RでのSVMを簡単に紹介しましたが、今日はlibsvmを利用したirisの分類学習を行いたいと思います。libsvmは導入がめちゃくちゃ簡単なところが売りだと思います。zipをlibsvmサイトからdownloadして展開してgmakeで設定完了です。
設定 $ wget &#34;http://www.csie.ntu.edu.tw/~cjlin/cgi-bin/libsvm.cgi?+http://www.csie.ntu.edu.tw/~cjlin/libsvm+zip&#34; $ unzip libsvm-3.12.zip $ cd libsvm-3.12 $ gmake $ ls -rw-r--r-- 1 yuta yuta 1497 1月 31 2012 COPYRIGHT -rw-r--r-- 1 yuta yuta 72186 4月 1 07:17 FAQ.</description>
    </item>
    
    <item>
      <title>R言語でSVM(Support Vector Machine)による分類学習</title>
      <link>https://yutakikuchi.github.io/post/201208270835/</link>
      <pubDate>Mon, 27 Aug 2012 08:35:47 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201208270835/</guid>
      <description>[機械学習] : R言語でSVM(Support Vector Machine)による分類学習 サポートベクターマシン入門
作者: ネロクリスティアニーニ,ジョンショー‐テイラー,Nello Cristianini,John Shawe‐Taylor,大北剛出版社/メーカー: 共立出版発売日: 2005/03メディア: 単行本購入: 8人 クリック: 135回この商品を含むブログ (41件) を見る
SVMとは  Support Vector Machineの略で教師あり学習に分類されます。線形、非線形の識別関数があり現在知られている多くの学習モデルの中では最も優れた識別能力があるとされています。いわゆる2値分類を解くための学習モデルであり、線形しきい素子を用いて分類器を構成します。訓練データにおける各データ点と距離が最大になるマージン最大化という基準で線形しきい素子のパラメータを学習させます。シンプルな例は与えられたデータ集合を全て線形に分離する事です。SVMはカーネルトリックという非線形の分離も可能としており、この部分でも優れた性能を発揮する事が分かっています。この記事ではR言語に備わっているデータを利用してSVMによる分類学習を行います。途中でNeuralNetwork、NaiveBayesとの比較も簡単に行います。
  R言語でSVM  設定 R言語でSVMを利用するにはkernlabというパッケージを必要とします。最初にinstallします。またlibrary関数でkernlabを読み込みます。
$ sudo R  install.packages( &#34;kernlab&#34; )  library( kernlab )  学習データ/予測データを作成 R言語に標準で入ったテストデータにIrisというものがあります。Irisを辞書で調べてみると以下のようにアヤメのことを差しています。「 アヤメ科アヤメ属の単子葉植物の総称。アヤメ・ハナショウブ・カキツバタなど。一般にはジャーマンアイリス・ダッチアイリスなどの園芸種をいう。」(Yahoo!辞書から引用)。Irisデータは4行のデータであり、蕚片の長さ/幅、花びらの長さ/幅で種別を定義しているデータです。ここでは蕚片の長さ/幅と花びらの長さ/幅を説明変数、種別を目的変数と呼ぶ事にします。トレーニングデータの説明変数を学習させ、評価データの説明変数から目的変数がどれに分類されるかを評価します。まずはIrisデータの50%をトレーニングデータ、残りの50%を予測を行うデータに分類します。Irisのデータは150行のデータなのでそれぞれ75行分のデータが格納されます。
#irisのデータの行数を取得  rowdatarandom_idsrandom_ids [1] 148 35 114 6 26 129 58 92 20 138 147 107 110 41 88 11 137 52 142 [20] 17 38 55 139 132 21 8 4 49 125 12 84 77 101 122 40 1 25 37 [39] 87 83 61 111 18 5 7 113 56 93 109 3 74 82 134 118 33 42 130 [58] 76 70 103 136 116 106 65 19 16 30 75 143 54 98 60 121 45 94 #学習データを作成  iris_trainingiris_training Sepal.</description>
    </item>
    
    <item>
      <title>R言語を用いた自己回帰モデルによる株価予測を試してみた</title>
      <link>https://yutakikuchi.github.io/post/201207300844/</link>
      <pubDate>Mon, 30 Jul 2012 08:44:41 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207300844/</guid>
      <description>[機械学習] : R言語を用いた自己回帰モデルによる株価予測を試してみた 一番売れてる株の雑誌ZAiが作った「株」入門 改訂版
作者: ダイヤモンド・ザイ編集部出版社/メーカー: ダイヤモンド社発売日: 2009/03/27メディア: 単行本購入: 5人 クリック: 71回この商品を含むブログ (13件) を見る
株価予測  欧州の経済不安により円高/日本株安が深刻になっています。トレーダーとしてはこのBigWaveを見過ごす訳にはいかないですが、「もうはまだなり、まだはもうなり」という言葉があるように投資のタイミングは非常に難しいものです。ここでは投資理論を語るのではなく、機械学習で株価を予測する事を試してみます。今回採用する予測Modelは自己回帰Model(AR)です。ARは時系列データ解析によく用いられます。AR処理はR言語のar関数を用います。
  AR(AutoRegressive)Model  ARModel - 自己回帰モデル ARModelは時系列解析に利用されます。時間と一緒に変動する値に対してARModelを適用し、時系列データに隠れた何かしらの情報を導きだす事を目的としています。AR(p)モデルは次のような式で定義されます。
ここでは時刻tで得られた時系列、はモデルパラメータ、は定数項、は誤差項となります。定数項は単純化式では削除されることが多いようです。数式から分かるようにが過去のデータに依存していおり、これが回帰と呼ばれる原因になっています。
自己回帰移動平均モデル - Wikipedia 
 MAModel - 移動平均モデル MAModelは時系列データの平滑化を行う手法です。誤差による平滑化が行われます。MA(q)は次のような式で定義されます。
ここでは時刻tで得られた時系列、は誤差項、はモデルパラメータとなります。
移動平均 - Wikipedia 
 ARMAModel - 自己回帰移動平均モデル ARMAModelはARModelとMAModelを組み合わせたモデルです。ARMA(p,q)は以下の数式で定義できます。
 その他 ここでは詳しく紹介しませんが、その他ARIMA、ARFIMA、ARCH/GARCHなどの時系列解析Modelが存在します。
ARCHモデル - Wikipedia 
   R言語でのARModel練習  UKgas R言語に標準で入っているイギリスの1960年〜1986年の四半期毎のガス使用量のデータでARModelの予測練習をしてみます。UKgasのデータは次のものです。また時系列データをplotしてみます。
 UKgas Qtr1 Qtr2 Qtr3 Qtr4 1960 160.1 129.7 84.</description>
    </item>
    
    <item>
      <title>初めての機械学習理論</title>
      <link>https://yutakikuchi.github.io/post/201207230844/</link>
      <pubDate>Mon, 23 Jul 2012 08:44:46 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207230844/</guid>
      <description>[機械学習] : 初めての機械学習理論 はじめての機械学習
作者: 小高知宏出版社/メーカー: オーム社発売日: 2011/04/22メディア: 単行本（ソフトカバー）購入: 6人 クリック: 99回この商品を含むブログ (9件) を見る
はじめての機械学習  はじめての機械学習という本を読んで学んだことをまとめます。自分で理解した言葉としてまとめています。原文とは異なる可能性があります。またその他自分で勉強した内容についても紹介します。
 機械学習とは パラメータ調整による学習 帰納的学習 教示的学習 進化的手法による規則学習 ニューラルネット 機械学習ライブラリ その他用語    機械学習とは   「生物」以外の「機械」が学習を行う事。 過去のデータやとある局面のデータを学習して新たな局面に当てはまる有効な知識構成を「汎化」と呼ぶ。 機械学習はゲーム研究での適用が始まりで、人口知能と人間の対戦だった。 評価関数の評価値が高くなるようなパラメータ調整が必要。=パラメータ調整による機械学習。 数値だけでなく形などの構造も学習可能で、具体的な事例から一般知識を抽出する学習を「帰納的学習」と呼ぶ。 与えられた原理や法則から具体的な事例を導く学習を「演鐸的学習」と呼ぶ。 生物集団の進化モデルをベースにした学習を「遺伝的アルゴリズム」と呼ぶ。 生物の個体が環境との相互作用によって知識を獲得するモデルを「強化学習」と呼ぶ。 強化学習は環境からの報酬に従い、報酬を最大にする事が目的。 データマイニングやテキストマイニングにも機械学習が用いられる。 生物の神経組織の挙動モデルにより情報処理を行う仕組みを「ニューラルネットワーク」と呼ぶ。 この本での学習の一覧と概略は以下の通り。     学習   概略     パラメータ調整学習   時系列データから知識抽出       帰納的学習   暗記学習中心     教科学習   データ分類規則の学習     遺伝的アルゴリズム   規則的な学習     ニューラルネット   パーセプトロン型のニューラルネットワーク        パラメータ調整による学習   パラメータ調整学習は与えられた学習データを自動的に調整。 学習データの数値を数式に決定することを回帰分析という。例として最小二乗法などがある。 時系列データの周期性や規則性を求めるケースに適用できる。 気温の周期性を求める場合、学習データ(真のデータ)、予測結果、予測の真否を○×で表にまとめる等すると評価が分かる。    帰納的学習   Webサイト上のデータを大量に収集する場面にはテキストマイニングが有効。 テキストマイニングするには自然言語処理を必要とする。自然言語処理の流れは以下の通り。  文抽出 形態素解析 構文解析 意味解析 談話理解  英文は単語がスペースで区切られているので形態素に分解するのは楽。日本語は大変。 構文解析では生成文法に基づき文の構造を記号として置き換える。置き換えた内容を名詞や動詞句として判定する。 意味解析は形態素と構文から判断。 談話理解は上の処理を踏まえて文全体で判定。 n-gramというn個の記号や文字の並びから文の特徴を抽出する。 n-gram全体の個数を表にまとめて上位を見ると特徴が分かる。 n-gramの考え方はテキストの特徴を表す指標のtf-idfにも関連する。 英語の場合n-gramは冠詞(the)や接続詞(and)が多くなる。 tf-idfとはある文章中の出現文字列が文章の特徴をどれだけ表しているかを表現する手法。  tf = term frequency idf = inverse document frequency tfはその文章中の出現回数、idfは一般文章全体の割合。idfの値が大きいと出現頻度が少ない事を示す。     教示学習   教示学習は教師あり学習とも呼ばれる。教師を無しに指示を受けずに学習を進める手法を教師無し学習と呼ぶ。 教師あり学習  効率で精密な学習。 教師が正確な学習を教示。 学習データに現れない状況への対応ができない。 学習を汎化することができない。  教師無し学習  学習の汎化や学習データに依存しない学習が可能。 誤った学習をする可能性がある。   教示学習の適用例としてはカテゴリ分類がある。人間にどのカテゴリに属するかを示してもらい知識を学習する。 分類知識の例としては電子メールのスパム判定がある。 分類知識は分類システムの動作を決定する知識関数。 知識分類は命題のYes or Noの判断木という知識の木構造で表現することができる。 判断木は論理式よりも記述が冗長化することがあり、論理式と比較してデータ構造が大きくなってしまう。 他の知識表現としてプロダクションシステムがあり、AならばBというルールを用いた表現。 分類知識の学習は成功しない場合があることを前提にすべき。 判断木の機械学習アルゴリズム  学習セットが空ならば終了。 学習セットの要素が全て単一カテゴリに属するならば終了。 学習セットをサブセットに分類する処理を再帰的に繰り返す。 属性が無く分類が終わっていなければ手続き終了。  プロダクションシステムではif 条件式 then カテゴリという式を当てはめる。 特定の分類知識を使って学習データをセットを分類した場合、正しく分類された場合と壮麗外の場合を調べる事が可能。得点を与えて評価。 ランダム生成に基づく分類知識獲得アルゴリズム  学習データセットの読み込み 乱数に寄る分類知識生成 分類知識の評価      進化的手法による規則学習   教示的な学習は探索空間が膨大でどのあたりに有用な知識が存在しているかが不明な場合に有効。探索範囲が明確である場合は、系統的な探索を行う方が有利。 人工知能の研究では縦型、横型、最良優先、最適化経路探索、Aアルゴリズム、A*アルゴリズムなどがある。 ランダム探索で一定の方向性を与える方法として焼きなまし法というものがあり、ランダムさを示すパラメータを初期値では高く設定し、そのパラメータを少しずつ修正して徐々に効率の良い探索点を探すこと。探索を1点とするのではなく複数の探索点を同時に調べる粒子群最適化法、蟻の群れの挙動を模擬することで探索を進める蟻コロニー最適化法などがある。 進化的計算のなかでも遺伝的アルゴリズム(Genetic Algorithm, GA)は研究が進んでいて、探索空間の複数の探査点を同時に処理して行く。 遺伝的アルゴリズムにおける評価関数を適応度関数と呼ぶ。選択にはルーレット選択、ランク選択、トーナメント選択など様々な方法がある。 選択された遺伝子は子孫を作る事ができ、複製や一部改変することを交叉(crossover)、突然変異(mutation)がある。 遺伝的アルゴリズムでは最良会を求める代わりにまずまずの結果を与える解を素早く求める事を目的としている。 採用する遺伝的アルゴリズムの一般的な方法としてSimple GA(SGA)というものがある。 SGAの処理手順  遺伝子プールの初期化 交叉 突然変異 結果の出力 繰り返し  エリート保存は世代交代で親世代のエリート遺伝子を子供世代にそのまま残すこと。子世代の結果が親世代と比較して低下しないようにする。    ニューラルネット   生物の神経細胞やネットワーク挙動にヒントを得た機械学習システム。生物の神経網と明確に区別したい場合は人工神経路網、人工ニューラルネット等と呼ぶ事もある。 神経細胞をモデル化したニュールセル(ニューロン、人工ニューロン、ニュール素子)をノートとして用い、複数のニューロセルを結合してネットワークを構成する。 ニューロセルは複数の入力を待ち、それぞれに特定の重み付けをした上で足し合わせをする。足し合わせた結果から閾値を引いて値を求める。数式で書くと次のよう。 xは入力、wは重み、vは閾値。  uの値を適当な関数fに適用してニュール素子の出力zを獲得する。fは出力関数。出力関数はステップ関数やシグモイド関数のようになる。シグモイド関数は以下のように示される。  ニューロセル単体でも情報処理が可能だが、ニュールセルを複数結合してニューラルネットを構成することで更に高度な情報処理機構を実現することが可能。ネットワークの最終的な出力が計算されることをフィードフォワードネットワーク(Feed Foward Network)または階層的なネットワークと呼ぶ。ニューロセルが自分自身にフィードバックして入力の一部になることをリカレントネットワーク(Recurrent Network)と呼ぶ。リカレントネッワークのうち、ニューロセルが互いに双方向に結合しているものをポップフィールドモデル(Hopfield Model)と呼ぶ。 パーセプトロンはフォードフォワード型のネットワークで特定の形式を持ったニューラルネット。パーセプトロンは3層の階層構造をもったニューラルネット。入力層→中間層への加重や閾値は乱数で、中間層→出力層への加重や閾値は学習で決まる。 パーセプトロンでは入力層から中間層の結合荷重を変更しなくても、中間層から出力層への結合荷重を適切に選ぶことで論理積/論理和等の動作を行う事ができる。しかし中間層→出力層の調整では排他的論理和(XOR)に対応する出力はできない。入力層→中間層の荷重値によっては排他的論理和も実現が可能。 学習データセットを与えて出力誤差が小さくなるように結合荷重と閾値を調整する。結合荷重と閾値の学習にはへブの学習則(Hebbian learning rule)を用いる。へブは頻繁に信号を伝達するシナプスの結合がより強化される。正しい結果を与える回路はより結合荷重を大きくして誤った結果を与える回路の結合荷重は小さくする事でネットワークとして学習が可能。 パーセプトロンの学習手続き  適当な終了条件まで以下を繰り返す。 学習データセットの中の一つの例としてについて以下を計算する。 を用いて中間層の出力を計算する。 を用いて出力層oを計算する。 出力層のニューロセルについて以下を計算する。   パーセプトロンの線形分離不可問題を回避して階層型のニューラルネットをより幅広い対象について学習を行うためには出力層に加えて中間層の結合荷重を調整する必要がある。バックプロパゲーション(back propagation、誤差逆伝播) バックプロパゲーションの学習手続き  適当な終了条件まで以下を繰り返す。 学習データセットの中の一つの例としてについて以下を計算する。 を用いて中間層の出力を計算する。 を用いて出力層oを計算する。 出力層のニューロセルについて以下を計算する。  中間層のj番目のニューロセルについて以下を計算する。  中間層j番目のニューロセルのi番目の出力について以下を計算する。      機械学習ライブラリ  有名な機械学習ライブラリを列挙します。</description>
    </item>
    
    <item>
      <title>BehaviorTargeting調査レポート</title>
      <link>https://yutakikuchi.github.io/post/201207170832/</link>
      <pubDate>Tue, 17 Jul 2012 08:32:59 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207170832/</guid>
      <description>[AD] : BehaviorTargeting調査レポート 行動ターゲティング広告 ページビュー神話の終焉
作者: 渡辺健太郎出版社/メーカー: インプレスジャパン発売日: 2007/12/01メディア: 単行本（ソフトカバー）購入: 2人 クリック: 66回この商品を含むブログ (7件) を見る
Index   BehaviorTargetingとは WebにおけるBehaviorTargeting MicroAdによる詳しい説明 海外のResearch Introduction to Computational Advertising Google Twitter FaceBook Yahoo NHN/livedoor Amoad その他ターゲティングに関する情報    BehaviorTargetingとは   BT広告とは【ビヘイビアターゲティング広告】 - 意味/解説/説明/定義 ： IT用語辞典  BehaviorTargetingとはユーザの行動から趣味指向を分析しそれにあったTargetingを行うことです。もう少し詳しく説明するとユーザの過去のWebサイト閲覧や滞在時間、動画/音楽の視聴、検索キーワード入力、広告クリック、GPSなどの現在地情報、ID登録時の年代性別、Web以外でのクレジットカード決済などのデータを基にユーザの顕在/潜在ニーズを推定し、それに見合った商品や広告の提案を行う事です。AmazonのAffinity Item(関連商品)やGoogleAdsenseもBehaviorTargetingによる商品と言えます。今日はBehaviorTargetingについて海外でのResearchや日本国内企業のTargeting手法に着いて調査した内容をまとめたいと思います。
  WebにおけるBehaviorTargeting   インタレスト カテゴリやユーザー属性カテゴリはどのように判別するのですか？ - AdSense ヘルプ  Ads Preferences Manager - Google 一般的にはWebにアクセスするブラウザのCookieと過去の行動から推定された趣味指向/興味カテゴリを紐付ける手法が用いられます。当然の事ながらCookieベースの手法なのでユーザ自身がCookieを削除してしまうと趣味指向/興味カテゴリとの紐付けも解除されます。後でも詳細を説明しますがGoogleAdsenseはユーザ自信が自分が興味を持っているカテゴリを編集することができ、Cookieに対して新たに編集内容を保存をします。ここでユーザに正確な情報を設定してもらえれば推定というよりは確定要素としてTargetingが可能です。
  MicroAdによる詳しい説明  MicroAdの行動ターゲティング特集の説明が素晴らしく詳しいので今回参考にさせていただきました。コンテンツの内容は広告主をメインとしているようです。</description>
    </item>
    
    <item>
      <title>JSONを見やすく表示するにはPythonの-mjson.toolを使うと良いよ</title>
      <link>https://yutakikuchi.github.io/post/201207110838/</link>
      <pubDate>Wed, 11 Jul 2012 08:38:50 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207110838/</guid>
      <description>[javascript] : JSONを見やすく表示するにはPythonの-mjson.toolを使うと良いよ Python クックブック 第2版
作者: Alex Martelli,Anna Martelli Ravenscroft,David Ascher,鴨澤眞夫,當山仁健,吉田聡,吉宗貞紀出版社/メーカー: オライリー・ジャパン発売日: 2007/06/26メディア: 大型本購入: 11人 クリック: 423回この商品を含むブログ (85件) を見る
JSONを見やすく表示する  WebツールやExtension  JSONLint - The JSON Validator.  Online JavaScript beautifier  JSON整形サービス  Chrome ウェブストア - JSONView  {&#34;Compile&#34;:[&#34;C&#34;,&#34;C++&#34;,&#34;Objective-C&#34;],&#34;Script&#34;:[&#34;JavaScript&#34;,&#34;PHP&#34;,&#34;Perl&#34;,&#34;Python&#34;]}JSONはJavaScriptのObjectを文字列化したもので、1行にまとまってしまうため非常に見づらいです。「JSON 整形」というキーワードでぐぐると上のようにWebサービスからChrome/FirefoxのExtensionなどが出てきます。今回はサーバサイドでJSONをechoする処理を書いていて、ツールを用いる事無く簡単に見やすくする方法を調べていたのですが、Pythonの-mjson.toolを使うとコマンドライン上で整形できることがわかりました。
 Python pretty-print-json  linux - How to pretty-print JSON script? - Stack Overflow  Stack Overflowに How to pretty-print JSON script?というコラムがあり、そこでPythonの-mjson.toolを指定する方法を知りました。例えば以下のようなPerlコードがあって、実行処理に対してパイプ(|)でpython -mjson.toolを指定するだけで整形してくれます。下では生のJSON文字列と整形した内容を比較していますが、見やすさは歴然だと思います。特にHashを表す{}と配列を表す[]がそれぞれ分かりやすいかと。
#!/usr/bin/perl use strict; use warnings; use JSON; my %array = (); push( @{$array{Script}}, ( &#39;JavaScript&#39;, &#39;PHP&#39;, &#39;Perl&#39;, &#39;Python&#39; ) ); push( @{$array{Compile}}, ( &#39;C&#39;, &#39;C++&#39;, &#39;Objective-C&#39; ) ); print encode_json( \%array ); $ perl echo_json.</description>
    </item>
    
    <item>
      <title>R言語のことは知らん。だがCRANパッケージのrimageを使ってLennaタン全身画像を解析してみる</title>
      <link>https://yutakikuchi.github.io/post/201207090829/</link>
      <pubDate>Mon, 09 Jul 2012 08:29:12 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207090829/</guid>
      <description>[CentOs] : R言語のことは知らん。だがCRANパッケージのrimageを使ってLennaタン全身画像を解析してみる 
はじめに  前回R言語の導入について記事を書いたらそこそこ反応があったので今日も書きます。CentOSでR言語を使ってみたことのまとめ - Yuta.Kikuchiの日記 
10代の頃はmatlabを使って画像の特徴抽出ということをやっていました。特徴とは輝度やフィルターを通して取得可能なエッジの事です。今日はR言語のrimageを使って画像処理をしてみたいと思います。
  環境設定  fftw,fftw-develのinstall R言語のrimageを使う前にCentOSのfftw,fftw-develのパッケージをinstallします。yumレポジトリに無いようなのでrpmforgeから取得します。rpmforge.repoを修正してrpmforgeからの取得を有効化します。有効化したら再度yum installします。
$ sudo yum install fffw-devel -y 0 packages excluded due to repository priority protections Setting up Install Process No package fffw-devel available. Nothing to do $ sudo vim /etc/yum.repos.d/rpmforge.repo [rpmforge] name = RHEL $releasever - RPMforge.net - dag baseurl = http://apt.sw.be/redhat/el5/en/$basearch/rpmforge mirrorlist = http://apt.sw.be/redhat/el5/en/mirrors-rpmforge #mirrorlist = file:///etc/yum.repos.d/mirrors-rpmforge enabled = 1 protect = 0 gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rpmforge-dag gpgcheck = 1 $ sudo yum install fftw-devel 110 packages excluded due to repository priority protections Setting up Install Process Resolving Dependencies -- Running transaction check --- Package fftw-devel.</description>
    </item>
    
    <item>
      <title>CentOSでR言語を使ってみたことのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201207040836/</link>
      <pubDate>Wed, 04 Jul 2012 08:36:00 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207040836/</guid>
      <description>[CentOs] : CentOSでR言語を使ってみたことのまとめ Rクックブック
作者: Paul Teetor,大橋真也,木下哲也出版社/メーカー: オライリージャパン発売日: 2011/12/22メディア: 大型本購入: 9人 クリック: 61回この商品を含むブログ (13件) を見る
はじめに  統計的処理と結果のグラフ化をするために学生時代はmatlabを利用していました。matlabは行列演算に優れ使い易い言語だったのですが、一番の難点はMathWorks社の商用製品である事です。しかも高い。MathWorks 日本 - MATLAB / Simulinkによる数値計算 - マスワークス公式日本語サイト  matlabと同様の機能をもつ(互換性は無い)scilabというFreeの言語もありますが、イマイチ流行っている感はありません。Home - Scilab WebSite  Freeの言語で統計処理をやるのはR言語が主流のようなので、それに習ってR言語を使って行きます。今日はR言語/RCommander周りの説明を行います。多少癖がある言語なので学習が面倒ですが慣れると優れものと聞くので頑張って勉強します。The R Project for Statistical Computing 
  CentOSへのInstall  環境 CentOS5.7、64bit環境、Core2Duoです。
$ cat /etc/redhat-release CentOS release 5.7 (Final) $ cat /proc/cpuinfo cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 23 model name : Intel(R) Core(TM)2 Duo CPU E7600 @ 3.</description>
    </item>
    
    <item>
      <title>これだけ覚えれば安心！ネット広告に関する重要指標と算出方法のまとめ</title>
      <link>https://yutakikuchi.github.io/post/201207030838/</link>
      <pubDate>Tue, 03 Jul 2012 08:38:24 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207030838/</guid>
      <description>[AD] : これだけ覚えれば安心！ネット広告に関する重要指標と算出方法のまとめ 図解入門ビジネス 最新 ネット広告&amp;モバイル広告がよーくわかる本―効果的な集客のための基礎知識 (How‐nual Business Guide Book)
作者: 佐藤和明出版社/メーカー: 秀和システム発売日: 2010/03メディア: 単行本購入: 2人 クリック: 36回この商品を含むブログ (4件) を見る
はじめに  CVR、CPC、PPC.... ネット広告のKPI(Key Performance Indicator)には3文字のアルファベットが良く利用されます。ベテランの先輩達は会社の会議で当たり前のようにアルファベットで表現されていますが、初心者には理解が及びません。そこで自分で学習したKPI指標を以下に簡単にまとめていきます。
  Index   表示系  PV Imps  User系  UU UB / UDB  KPI系  KPI CV CVR CTR ROI ROAS CPC CPM eCPM CPA CPI / CPO Reach Frequency グロス/ネット売り上げ  その他 Links    表示系  PV(Page View)  Pageがどれだけ表示されたかという指標です。 PVは1アクセスで1カウントされます。 だいたい1人のUserが複数のページにアクセスするので、UUよりもPVが多くなるのが一般的です。 Yahoo JAPAN!</description>
    </item>
    
    <item>
      <title>Makefileの書き方</title>
      <link>https://yutakikuchi.github.io/post/201207020838/</link>
      <pubDate>Mon, 02 Jul 2012 08:38:29 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201207020838/</guid>
      <description>[C++] : Makefileの書き方 GNU Make 第3版
作者: Robert Mecklenburg,矢吹道郎(監訳),菊池彰出版社/メーカー: オライリージャパン発売日: 2005/12/01メディア: 大型本購入: 4人 クリック: 115回この商品を含むブログ (33件) を見る
利用するケース  C/C++を書いた時に複数ファイルから実行ファイルを生成するときやライブラリをIncludeする場合コンパイルのオプションが複雑になります。複雑なオプションを毎回コマンドラインで入力するのではなく、Makefileというコンパイルのオプションルールを記載してmake/gmakeコマンドにて実行ファイルを生成すると便利です。今回はMakefileの簡単なルールについて紹介します。
  Makefileの基本  基本ルール C++ソースのコンパイルにはg++を利用します。例えばhello.cppというファイルをコンパイルする場合は$ g++ hello.cpp -o helloと実行するとhelloという実行ファイルが生成されます。これをMakefileを使って書くと次のようになります。Makefileを作成したらgmakeとコマンドを実行するだけです。
hello: hello.cpp #ターゲット: 依存ファイル g++ -Wall hello.cpp -o hello #実行コマンド clean: rm -f *.o hello書式を簡単に説明すると、1行目に生成したいターゲットファイル名: 依存ファイル、2行目に生成するための実行コマンドを記載します。実行コマンドの先頭にはTabを入力する必要があります。viなどではControl-V、Tabとして入力すると識別されると思います。Tabではなくspaceを入れてしまうと「Makefile:2: *** 分離記号を欠いています. 中止.」とエラーが出力されてしまうので注意が必要です。
上のMakefileをgmakeコマンドで実行するとhelloという実行ファイルが生成されます。生成された実行ファイルを消去したい場合はgmake cleanと実行するとrm -f *.o helloの箇所が実行されます。g++の-Wallオプションですが、全ての警告オプションを結合してくれるもので一番厳密に文法をチェックします。
$ ls drwxr-xr-x 2 yuta yuta 4096 6月 30 11:40 . drwxr-xr-x 6 yuta yuta 4096 6月 30 11:20 .</description>
    </item>
    
    <item>
      <title>C&#43;&#43;でマルチスレッドプログラミングを試してみたことのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201206260841/</link>
      <pubDate>Tue, 26 Jun 2012 08:41:53 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201206260841/</guid>
      <description>[C++] : C++でマルチスレッドプログラミングを試してみたことのまとめ C++プログラミング入門
作者: グレゴリーサティア,ダウグブラウン,Gregory Satir,Doug Brown,望月康司,谷口功出版社/メーカー: オライリー・ジャパン発売日: 2001/11メディア: 単行本購入: 9人 クリック: 147回この商品を含むブログ (29件) を見る
Index   Word  排他制御 アトミック性 クリティカルセクション fork マルチプロセス/マルチスレッド mutex semaphore Pthreads スレッドセーフ スピンロック ACID  pthread API fork semaphore mutex Attention links    Word  初歩的な単語ですが、マルチスレッドプログラミングに関するものを簡単にまとめます。
排他制御 複数のタスクが処理を並行して同一の資源にアクセス可能な場合、データの整合性が合わない事を避けるために他のプロセスの処理を排除すること。相互排除（mutual exclusion）ともいう。最大k個のタスクが資源にアクセス可能な事をk-相互排除と呼ばれる。
 アトミック性 複数の操作が不可分であり、切り離せない事。システム上の他の操作から見てアトミック性を持つ操作は全てが完了したか/全てが失敗したかの状態として観測できない事。全てが失敗したと観測された場合は処理前の状態にロールバックしないとならない。
 クリティカルセクション 複数のタスクの同時アクセスを可能とした時に、データの不整合が発生すると破綻する部分の事。クリティカルセクションでは排他制御を行い、データのアトミック性を保証する。
 fork UNIX系のシステムコールで親プロセスから子プロセスを生成すること。forkはプロセスを新たに生成するのでマルチスレッドとは異なりマルチプロセスである。
 マルチプロセス/マルチスレッド マルチプロセスはそれぞれのプロセスが独立したメモリ空間を使用してプロセス毎の平行した処理が可能。マルチプロセスはプロセス間通信や共有メモリを使ってデータのやり取りができる。それに対してマルチスレッドは同一のメモリ空間に対して複数のスレッドを作成して平行処理が可能。メモリを共有しているのでマルチスレッドはデータのやり取りがスレッド間でできる。
 mutex Mutual Exclusion(相互排他)の略で排他制御を行う場面でアトミック性を保証するための同期仕様。広義にはsemaphoreの一種と言える。semaphoreは同一クリティカルセクションに対して複数のタスクが操作可能であり、mutexでは一つしか操作可能とならないので完全なる排他制御。P操作(ロック)、V操作(アンロック)を持ち排他制御を行う。
 semaphore 複数のタスクが同一資源にアクセスする上限を規定する時に用いる。同時アクセス1とした場合はmutexとほぼ同じことになり、これをバイナリセマフォと呼ぶ。処理としては共通領域にセマフォ領域を確保し、任意の最大k個のアクセスを許可する。セマフォ領域でタスクがP操作、V操作により処理実行中/完了フラグのON/OFFを行う。
 Pthread POSIX標準のスレッドのこと。スレッド生成や操作のAPIが用意されている。C言語のデータ型や関数をpthread.</description>
    </item>
    
    <item>
      <title>はてなダイアリーにFacebookのLikeBoxを設置する方法</title>
      <link>https://yutakikuchi.github.io/post/201206220820/</link>
      <pubDate>Fri, 22 Jun 2012 08:20:35 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201206220820/</guid>
      <description>[Web] : はてなダイアリーにFacebookのLikeBoxを設置する方法 フェイスブック 若き天才の野望 (5億人をつなぐソーシャルネットワークはこう生まれた)
作者: デビッド・カークパトリック,小林弘人解説,滑川海彦,高橋信夫出版社/メーカー: 日経BP社発売日: 2011/01/13メディア: ペーパーバック購入: 33人 クリック: 2,842回この商品を含むブログ (240件) を見る
イイネ！してくれた人を知るために  このブログへのアクセスリファラーを見てみるとFacebookからの誘導がいくつかあります。誰かがFacebookでシェア、このブログ上でイイネ！ボタン押下をしてくれているんですが、誰がイイネ！してくれたのかが分かりません。イイネ！してくれた人を知るため、また更なるイイネ！を取得するためにLikeBoxを設置します。
  やること  やることは3つです。
 Facebookページ作成 LikeBox設置モジュール作成 はてなダイアリーにLikeBoxモジュールを貼付け    Facebookページ作成   
 http://www.facebook.com/pages/create.php からFacebookページの作成を行います。 まずはカテゴリを選びます。私はアーティスト、バンド、有名人カテゴリを選択しました。 次にプロフィール画像を投稿します。 最後にFacebookページのURLを登録します。今回は http://www.facebook.com/yutakikuc こういったURLで登録しました。 上の右写真のようなページが作成されます。    LikeBox設置モジュール作成   
 http://developers.facebook.com/docs/reference/plugins/like-box/　からLikeBoxの設置モジュールを作成します。 Facebook Page URLの項目に上で登録したFacebookページのURLを記載します。 設置したいLikeBoxのWidthやHeighといったパラメータを入力します。 一番下にあるGet Codeというボタンを押します。 Like BoxのプラグインコードというPopUpが出てきます。HTML5、XFBML、IFRAMEが選択できます。HTML5のコードそのままはてなダイアリーに貼付けるとエラーが出てしまうので、ここではIFRAMEを選択します。 IFRAME定義をはてなダイアリーで読み込めるように変換します。IFRAMEを選択した時にiframe srcの先頭にhttp:を付ける事を忘れないようにしましょう。 なぜか付与されていないです。 変換は便利なサイトがありました。はてなダイアリーに任意のiframeを貼り付ける - daily gimite  以下は変換したモジュールです。 scriptsrc=&#34;</description>
    </item>
    
    <item>
      <title>Subversionでの管理対象外設定について</title>
      <link>https://yutakikuchi.github.io/post/201206110832/</link>
      <pubDate>Mon, 11 Jun 2012 08:32:43 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201206110832/</guid>
      <description>[programming] : Subversionでの管理対象外設定について 実用 Subversion 第2版
作者: C. Michael Pilato,Ben Collins-Sussman,Brian W. Fitzpatrick,宮本久仁男(監訳),朝枝雅子,浜本階生出版社/メーカー: オライリージャパン発売日: 2009/07/27メディア: 大型本購入: 6人 クリック: 45回この商品を含むブログ (19件) を見る
管理対象外設定  Subversion(SVN)でプログラムのソースを管理する時に、管理対象外のファイル/ディレクトリとして設定したい時があります。今日はその設定方法の紹介になります。
svn propset 管理対象外にするためにはsvn propsetというコマンドを使います。propsetにてsvn:ignore属性を設定します。 propsetについて詳しい情報が欲しい場合は$ svn help propsetを実行します。以下にFile/Directoryの両方をSVN管理対象外にする設定を記述します。
 Fileに対しての設定  文法  svn propset svn:ignore -F   svn propset svn:ignore --file    (例) ignoreFileというファイルをカレントDirectoryにおいてSVN管理対象外とする $ svn propset svn:ignore -F IgnoreFile .  Directoryに対しての設定  文法  svn propset svn:ignore *   (例) IgnoreDirの全ファイルに対してSVN管理対象外とする $ svn propset svn:ignore * IgnoreDir    設定確認  svn status 管理対象外と設定したFile/Directoryの状態を確認するにはsvn statusコマンドを実行します。使い方の詳細が知りたい場合はsvn help statusを実行します。</description>
    </item>
    
    <item>
      <title>より良いプレゼンを行うための５つの改善点</title>
      <link>https://yutakikuchi.github.io/post/201206050830/</link>
      <pubDate>Tue, 05 Jun 2012 08:30:22 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201206050830/</guid>
      <description>[presentation] : より良いプレゼンを行うための５つの改善点 スティーブ・ジョブズ 驚異のプレゼン
作者: カーマイン・ガロ,外村仁解説,井口耕二出版社/メーカー: 日経BP社発売日: 2010/07/15メディア: 単行本（ソフトカバー）購入: 126人 クリック: 3,690回この商品を含むブログ (298件) を見る
前書き  
六本木でWebプログラマをやっています@yutakikucです。いつもは技術のネタしか書きませんが、今日は技術セミナー発表の反省点を書きます。この半年間で大人数に対してプレゼンする機会を2回頂き、どちらも多くの失敗点があった事が後から分かりました。プレゼン直後は自分的にも良い発表が出来たと思い込んでいたのですが、撮影された動画を見て愕然としました。客観的に振り返る事は次回のプレゼン機会に有効に活かせる点が沢山見つかります。自分が話す動画を見るのはとても恥ずかしい事ですが、動画を撮影しもらって内省すると良いでしょう。今回の内省する点は以下の5つに絞ります。どれも超基本的な事のように思われますが、意識しても難しい事だと感じました。
 プレゼンの対象が曖昧 プレゼンの勉強不足 聴衆を引きつけていない 「えっと」、「あのー」と言ってしまう 表現が的を得ていない    プレゼンの対象が曖昧  
&#34;誰に対して何を話したいのか？&#34;
誰に向けたプレゼンなのかを最初から意識しないとプレゼン資料やシナリオの方向性が大きく異なります。プレゼンに求められた要件と対象レベルを明確にする事が重要です。僕は今回対象者が曖昧だったため、数回資料を作り直す必要がありました。発表前に色々な人からFeedBackをもらい、最終的には対象を初中級者向けと定義しました。自分が思っているほど他人は理解していない、分かりやすい資料じゃないと理解してくれない事を意識する必要があります。
 404 Blog Not Found:惰翻 - プレゼンをイカす10のtips  誰もがスティーブ・ジョブズになれる!? プレゼン成功5つの秘訣 人前での表現がうまい人は何が違うのか｜マーケットが見える！人のココロをつかむセオリー｜ダイヤモンド・オンライン  ASCII.jp：第9回　誰でも必ず上達する！　勝てるプレゼン技術とは｜独立すれば、キラリと光る　SEのための起業塾     プレゼンの勉強不足  
&#34;プレゼンの型は何か？&#34;
今まで意識していた事としてはトップダウン(まずは結論から喋って後から理由付け)で話す事、不必要な言葉は全て削る事の2点だけでした。それ以外にも多くのプレゼンメソッドは存在します。自分でどういった方法が良いかを考えるだけでなく他者の良いプレゼンの型を学ぶ必要があります。実際にプレゼンが巧い人の発表を聞いて技術を盗むのも一つの手段ですね。色々な人がWebでプレゼンの極意について紹介しているので次回から参考にしたいと思います。
 プレゼンハック 〜プレゼン改善のための10個の小技〜 | IDEA*IDEA  良いプレゼンを作る5つのポイント教えます : ライフハッカー［日本版］  すごいプレゼン資料まとめ - NAVER まとめ  最強プレゼンテーションの10のコツ | SEO Japan  プレゼンテーション・パターン (Presentation Patterns)     聞き手を引きつけていない</description>
    </item>
    
    <item>
      <title>初心者から見たPerl言語I/Fは気持ち悪いが、たった一行の記述が素晴らしく強力な件について</title>
      <link>https://yutakikuchi.github.io/post/201206040834/</link>
      <pubDate>Mon, 04 Jun 2012 08:34:22 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201206040834/</guid>
      <description>[Perl] : 初心者から見たPerl言語I/Fは気持ち悪いが、たった一行の記述が素晴らしく強力な件について プログラミングPerl〈VOLUME1〉
作者: ラリーウォール,ジョンオーワント,トムクリスチャンセン,Larry Wall,Jon Orwant,Tom Christiansen,近藤嘉雪出版社/メーカー: オライリー・ジャパン発売日: 2002/09メディア: 単行本購入: 8人 クリック: 245回この商品を含むブログ (130件) を見る
追記  ※2012/06/10 toku_bassさんからご指摘頂いた内容を載せておきます。
 shebangの定義は #!/usr/bin/env perlの方が都合が良い。 連想配列の定義でkeyを&#39;&#39;や&#34;&#34;で囲まないとエラーになる。  use strict無しでは&#39;&#39;や&#34;&#34;が無くてもそのまま実行されてしまいますが、use strictを使うと確かにエラーになる事を確認しました。よって必ずkeyは&#39;&#39;や&#34;&#34;で囲むべきです。  #!/usr/bin/env perl use strict; my %hash = ( &#39;key0&#39;, 0, &#39;key1&#39;, 1, &#39;key2&#39; ,2 ); foreach( sort keys %hash ) { print &#34;Key = $_Value = $hash{$_}\n&#34;; }   arrayの連番定義は1..9等を使うと良い。 文字列の定義の場合はqw/ foo bar hoge /を使うと良い。&#39;&#39;などで囲むのが面倒なので。 #!/usr/bin/env perl use strict; my @array = ( 1 .</description>
    </item>
    
    <item>
      <title>標準入出力プログラミング</title>
      <link>https://yutakikuchi.github.io/post/201205280833/</link>
      <pubDate>Mon, 28 May 2012 08:33:38 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201205280833/</guid>
      <description>[CentOs] : 標準入出力プログラミング 入門Unixオペレーティングシステム
作者: ジェリーピーク,ジョンストラング,グレーストディノ,Jerry Peek,John Strang,Grace Todino,羽山博出版社/メーカー: オライリージャパン発売日: 2002/12メディア: 単行本 クリック: 5回この商品を含むブログ (6件) を見る
標準入力  今日は簡単なメモ書きです。標準入力の使い方について少しだけ話します。
プログラム中で容量があるデータを一度ファイルに落として、その落としたファイルに対して別のプログラムを呼び出すような処理がかかれているのを時たま目にします。見る度に良い方法ではないと感じます。例えば1processで1fileを生成するような場合は、processに比例してfile数とDisk圧迫が重なります。重要なシステムでログファイルを記録するなどの使い方ではない限りfileを中継する方法は辞めましょう。代わりに実行コマンドへの標準入力で対応する方が効率的です。
標準入力とはUnix/Linuxで広く用いられているプログラムへの入力方法で、Keybord入力のコマンドライン引数、プログラムから別のプログラムへ入力する方法などを意味します。BigData処理のHadoopStreamingも標準入出力を利用しています。
  標準出力  標準入力によるデータ受け取りを行うプログラムに対して標準出力により受け渡しを行います。標準出力はプログラム内部でprintされる結果であったり、単純なUnix/Linuxコマンドのechoだったり、ヒアドキュメントで作る事も可能です。標準入力に複雑なデータ構成を必要とする場合はプログラム言語で記述、それ以外はechoやヒアドキュメントで十分だと思います。Sampleを以下に記述します。
プログラム言語での出力 ただprintするだけです。2列のデータを作ります。列の表現をタブ(\t)で、行を改行(\n)で表現します。このファイルをoutput.plとします。
#!/usr/bin/perl use strict; use warnings; print &#34;title\tdetail\nこんにちは\tこんにちはYutaKikuchiさん\nこんばんは\tこんばんはYutaKikuchiさん&#34;; title detail こんにちは こんにちはYutaKikuchiさん こんばんは こんばんはYutaKikuchiさん  echoコマンドでの出力 echoは代表的なUnix/Linuxの出力コマンドです。-eオプションを利用してタブや改行によるデータを出力します。
$ man echo オプション -n 行末の改行を行わない。 -e string 中の、バックスラッシュでエスケープされた文字の解釈を有効にする。それぞれの意味は以下の通り: \a アラート (ベル) \b バックスペース \c 行末の改行を出力しない \f フォームフィード (form feed) \n 改行 (newline) \r 復帰 (carriage return) \t 水平タブ \v 垂直タブ \\ バックスラッシュ \0nnn アスキーコードが nnn (8 進) の文字 $ echo -e &#34;</description>
    </item>
    
    <item>
      <title>PHPのマルチスレッドプログラミングを使ってシステム処理を爆速化するお話し</title>
      <link>https://yutakikuchi.github.io/post/201205210841/</link>
      <pubDate>Mon, 21 May 2012 08:41:56 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201205210841/</guid>
      <description>[PHP] : PHPのマルチスレッドプログラミングを使ってシステム処理を爆速化するお話し パーフェクトPHP (PERFECT SERIES 3)
作者: 小川雄大,柄沢聡太郎,橋口誠出版社/メーカー: 技術評論社発売日: 2010/11/12メディア: 大型本購入: 32人 クリック: 1,065回この商品を含むブログ (60件) を見る
Intro  会社に入社して5年が経ち、4月から新しい部署で働いています。最近はプログラミング言語の学習としてC++/JAVA/Perl/R言語、理論の勉強として機械学習をやっています。平行して少しずつ勉強しているのでblogの記事内容も多種多様になってきています(笑)。新しい事をやる時は一つの事に集中して勉強したいのですが、直近は業務で成果を残さないと相手にされないので学習がforkします。ということで強引な繋ですが今日はforkの話をします。業務で必要になったPHPの処理爆即化に向けてマルチスレッドプログラミングを試してみました。pcntlにより親プロセスから子プロセスを作成してforkさせます。出来たところまでの成果を以下にまとめました。
PHP: PCNTL - Manual 
  Source Build  pcntlというPHPのマルチスレッドプログラミングはdefaultでは使えないようです。phpソースをbuildする時に--enable-pcntlを付ける必要があります。以前にmcrypt関数を使う時にもオプションをつけて--with-mcrypt=/usr/local/libを付けたりlibmcryptをインストールしないといけないことがあったので、標準で使えるようにして欲しいですね。
 $pid = pcntl_fork(); if ($pid == -1) { die(&#39;fork できません&#39;); } else if ($pid) { // 親プロセスの場合 echo &#34;parent process \n&#34;; pcntl_wait($status); // ゾンビプロセスから守る } else { // 子プロセスの場合 echo &#34;child process \n&#34;; } $ php pcntl.php Fatal error: Call to undefined function pcntl_fork()次にphpソース取得とコンパイル、インストール手順を書きます。</description>
    </item>
    
    <item>
      <title>Perlの多次元連想配列をJSONエンコードする方法</title>
      <link>https://yutakikuchi.github.io/post/201205160829/</link>
      <pubDate>Wed, 16 May 2012 08:29:42 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201205160829/</guid>
      <description>[Perl] : Perlの多次元連想配列をJSONエンコードする方法 初めてのPerl 第5版
作者: Randal L. Schwartz,Tom Phoenix,brian d foy,近藤嘉雪出版社/メーカー: オライリージャパン発売日: 2009/10/26メディア: 大型本購入: 22人 クリック: 293回この商品を含むブログ (41件) を見る
はじめに  業務でPerlを使う事になったので少しずつ学習した事を書いて行きます。この記事での目的は連想配列を多次元で定義して、その配列をjson_encodeします。
超簡単 Perl一問一答学習帳 - Yuta.Kikuchiの日記 
  配列  定義 配列の定義には@を使います。@は配列を示す宣言ですが、定義の仕方は色々とあります。以下では()括弧、添字、qw演算子での指定について書きます。qw演算子とは空白での区切りでlistにしてくれるものです。
#!/usr/local/bin/perl -w use strict; use warnings; # @を使う my @sample = ( &#39;a&#39;, &#39;b&#39;, &#39;c&#39; ); # $を使う my @sample2; $sample2[0] = &#39;a&#39;; $sample2[1] = &#39;b&#39;; $sample2[2] = &#39;c&#39;; # qwを使う my @sample3 = qw/a b c/;   取得 各要素を取得するにはfor,foreach,whileなどの繰り返し文が使えます。Perlの特徴的な記述とし$#配列名とすると配列の最後の添字番号を取得する事が出来ます。またforeachで括弧に@配列を入れると自動的に$_に要素を展開してくれます。スゴい事にPerlは繰り返しを使わなくても全要素を表示する事ができます。print @配列名またはprint &#34;</description>
    </item>
    
    <item>
      <title>魔法少女まどか☆マギカN-Gram</title>
      <link>https://yutakikuchi.github.io/post/201205140829/</link>
      <pubDate>Mon, 14 May 2012 08:29:57 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201205140829/</guid>
      <description>[機械学習] : 魔法少女まどか☆マギカN-Gram 小説 魔法少女まどか☆マギカ (まんがタイムKRノベルス)
作者: 原作:Magica Quartet,文:一肇(ニトロプラス),イラスト:蒼樹うめ・シャフト出版社/メーカー: 芳文社発売日: 2012/05/08メディア: 単行本（ソフトカバー）購入: 1人 クリック: 78回この商品を含むブログ (14件) を見る
まどマギN-Gram抽出  魔法少女まどか☆マギカのN-Gramを抽出したデータを記載します。目的はテキスト機械学習用のデータ抽出です。N=3〜7で試してみました。台詞のデータは以下のサイトをPythonでスクレイプ、N-Gramの解析にはC++で行いました。
魔法少女まどか☆マギカ　WIKI - トップページ    まどマギ台詞  台詞をスクレイピングするプログラムを記載します。プログラムをmadmagi_scrape.pyとして保存します。Pythonプログラムを実行するとmadmagi.txtというファイルができるので中身を確認します。
#!/usr/bin/env python # -*- coding: utf-8 -*- import sys,re,urllib,urllib2 urls = ( &#39;http://www22.atwiki.jp/madoka-magica/pages/170.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/175.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/179.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/180.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/200.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/247.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/244.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/249.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/250.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/252.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/241.html&#39;, &#39;http://www22.atwiki.jp/madoka-magica/pages/254.html&#39; ) f = open( &#39;./madmagi.txt&#39;, &#39;w&#39; ) opener = urllib2.build_opener() ua = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/534.</description>
    </item>
    
    <item>
      <title>Hadoop MapReduceのExamplesで分散grep、WordCount、randomwriter、sort、join、数独、円周率計算を試してみる</title>
      <link>https://yutakikuchi.github.io/post/201205100828/</link>
      <pubDate>Thu, 10 May 2012 08:28:29 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201205100828/</guid>
      <description>[Hadoop] : Hadoop MapReduceのExamplesで分散grep、WordCount、randomwriter、sort、join、数独、円周率計算を試してみる Hadoop徹底入門
作者: 太田一樹,下垣徹,山下真一,猿田浩輔,藤井達朗,濱野賢一朗出版社/メーカー: 翔泳社発売日: 2011/01/28メディア: 大型本購入: 14人 クリック: 668回この商品を含むブログ (43件) を見る
Try  この記事はHadoopのMapReduce examples.tarで以下の事を試した記録です。とても初歩的な事を書いています。
 sudoku ： 数独 Pi Estimator ： 円周率計算  WordCount ： 単語数カウント grep ： 文字列検索 randomwriter ： 文字列生成 sort ： sorting join ： データ結合 この記事では以下のサイトを参考にしました。中でもHadoop入門 IBMが一番詳しくて分かりやすいと思います。
 Overview (Hadoop 0.20.2 API)  FrontPage - Hadoop Wiki  CDH3_Quick_Start_Guide_u3.pdf Hadoop入門 IBM また今までHadoopに関連した以下の記事を書きました。何かの参考にしていただければと思います。
 CentOSでHadoopを使ってみる - Yuta.Kikuchiの日記  Hadoopをより便利に使う！HiveでのMapReduceまとめ - Yuta.Kikuchiの日記  「魔法少女まどか☆マギカ」の台詞をJavaScriptでMapReduceしてGoogle Chart APIでグラフ出力したよ！ - Yuta.</description>
    </item>
    
    <item>
      <title>Apache Mahout 機械学習Libraryを使って「魔法少女まどか☆マギカ」の台詞をテキストマイニングしてみた</title>
      <link>https://yutakikuchi.github.io/post/201205031659/</link>
      <pubDate>Thu, 03 May 2012 16:59:32 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201205031659/</guid>
      <description>[Hadoop] : Apache Mahout 機械学習Libraryを使って「魔法少女まどか☆マギカ」の台詞をテキストマイニングしてみた Mahout in Action
作者: Sean Owen,Robin Anil,Ted Dunning,Ellen Friedman出版社/メーカー: Manning Pubns Co発売日: 2011/10/28メディア: ペーパーバック購入: 4人 クリック: 81回この商品を含むブログ (10件) を見る
Index   Information &amp; Links Apache Mahout  Abouc Apache Mahout Mahout has machine learning libraries Mahout Download / Setting  Madmagi Words  Scraping Word MA Mecab MA HDFS PUT  Clustering Theory  TF/IDF K-Means Canopy Clustering  Word Vector Clustering Graph Display  Required JAR Sample Graph Image     Information &amp; Links  この記事は「魔法少女まどか☆マギカ」の台詞を機械学習によりClusteringした内容についての記録です。Clustering/Graph Image出力などの実験/検証が不十分であるため後日再挑戦しますができたところまで公開します。以下はこの記事で参考にしたリンクです。</description>
    </item>
    
    <item>
      <title>C&#43;&#43;最速マスター その4</title>
      <link>https://yutakikuchi.github.io/post/201205022121/</link>
      <pubDate>Wed, 02 May 2012 21:21:49 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201205022121/</guid>
      <description>[C++] : C++最速マスター その4 C++プログラミング入門
作者: グレゴリーサティア,ダウグブラウン,Gregory Satir,Doug Brown,望月康司,谷口功出版社/メーカー: オライリー・ジャパン発売日: 2001/11メディア: 単行本購入: 9人 クリック: 147回この商品を含むブログ (29件) を見る
Index   inline iterator  reverse_iterator const_iterator  pair cast  static_cast dynamic_cast const_cast reinterpret_cast  Class  Inner Class Local Class Annonymous Class  etc  size_t     inline  inlineは関数に付ける修飾子です。inline void echoError() {}のように設定すると関数をインライン展開します。インライン展開は呼び出しもとにコードそのものを埋め込んでしまうようなイメージです。これにより関数呼び出し処理の時間削減を行います。inlineはサイズの大きい関数またアドレス取得するような関数には適用ができません。
#include  using namespace std; // inline関数 inline int max( int x, int y ) { return ( x  y ) ?</description>
    </item>
    
    <item>
      <title>Javaを最速でマスターするための予備知識7項目</title>
      <link>https://yutakikuchi.github.io/post/201204270829/</link>
      <pubDate>Fri, 27 Apr 2012 08:29:04 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201204270829/</guid>
      <description>[Java] : Javaを最速でマスターするための予備知識7項目 Java: The Good Parts
作者: Jim Waldo,矢野勉,笹井崇司出版社/メーカー: オライリージャパン発売日: 2011/02/24メディア: 大型本購入: 3人 クリック: 148回この商品を含むブログ (36件) を見る
index   1.Compile  javac Making Jar  2.Class  Relations Basic Class Nested Class Static Nested Class Inner Class Local Class Anonymous Class  3.NameSpace 4.Annotation 5.Generics 6.Extension for 7.Variable Arguments Links    1.Compile  Javac JavaのCompileにはjavacを利用します。javacコマンドの実行により.javaなどのjavaソースファイルを.classファイルにコンパイルします。javacの使用についてはhelpが参考になるので内容を見てみます。
$ java -version java version &#34;1.7.0_01&#34; Java(TM) SE Runtime Environment (build 1.</description>
    </item>
    
    <item>
      <title>C&#43;&#43;最速マスター その3</title>
      <link>https://yutakikuchi.github.io/post/201204231128/</link>
      <pubDate>Mon, 23 Apr 2012 11:28:47 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201204231128/</guid>
      <description>[C++] : C++最速マスター その3 C++プログラミング入門
作者: グレゴリーサティア,ダウグブラウン,Gregory Satir,Doug Brown,望月康司,谷口功出版社/メーカー: オライリー・ジャパン発売日: 2001/11メディア: 単行本購入: 9人 クリック: 147回この商品を含むブログ (29件) を見る
index   Char Pointer String Function  wide character / wstring atof/atoi/atol join/split character encoding conversion zen2han / han2zen kata2hira / hira2kata / latin2kata / latin2kata / kata2latin/ latin2hira / hira2latin  Regex  regcomp / regexec / regfree / regerror     Char Pointer  C++の場合string型を使ってしまいがちですが、標準関数がcharにしか対応していない(stringがNG)ことも多くcharの性質を理解しておきます。charは1文字を表す型で配列にすると文字列が表現できます。ポインタにはデータのアドレスが格納、ポインタ変数には配列の先頭アドレスが格納されるのでインクリメントしてアドレスの中身であるデータを取得します。char配列はint配列と性質が異なります。例えばint *pp = {1,2,3,4,5}; はコンパイルエラーになりますが、char *stp = &#34;</description>
    </item>
    
    <item>
      <title>C&#43;&#43;最速マスター その2</title>
      <link>https://yutakikuchi.github.io/post/201204200839/</link>
      <pubDate>Fri, 20 Apr 2012 08:39:08 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201204200839/</guid>
      <description>[C++] : C++最速マスター その2 C++プログラミング入門
作者: グレゴリーサティア,ダウグブラウン,Gregory Satir,Doug Brown,望月康司,谷口功出版社/メーカー: オライリー・ジャパン発売日: 2001/11メディア: 単行本購入: 9人 クリック: 147回この商品を含むブログ (29件) を見る
index  最速C++マスター その2です。今回内容としてまとめることは以下の通りです。
 Const Static Template  Template関数 Templateクラス  Struct SmartPointer STL  Container List Vecotr Map  curl    Const  Constは変数を後から書き換えができないようにする制約修飾子です。Const変数に対して書き換えを行うプログラミングを書くとコンパイル時にread-onlyエラーがでます。Constを付けると誤ったプログラミングを防ぐだけでなく、コンパイラが最適化しやすくなるので出来る限りConstを付けても良い箇所は付けるようにすると良いと思います。Constは通常の変数だけでなく、ポインタ変数、関数の引数、メンバ関数にも適用可能です。
#include  #include  class Student { private : std::string name; int number; public : void setName( const std::string&amp; name ); void setNumber( const int&amp; number ); std::string getName() const; int getNumber() const; }; //Constructor Student::Student() { } //Destructor Student::~Student() { delete this; } // メンバ関数 引数をconstで定義 void Student::setName( const std::string&amp; name ) { this-name = name; } // メンバ関数 引数をconstで定義 void Student::setNumber( const int&amp; number ) { this-number = number; } // メンバ関数 メンバ関数もconstを指定できる  std::string Student::getName() const { return this-name; } // メンバ関数 メンバ関数もconstを指定できる  int Student::getNumber() const { return this-number; } int main(){ const int a = 1; // errorになる // a = 2; Student *student = new Student(); student-setName( &#34;</description>
    </item>
    
    <item>
      <title>C&#43;&#43;最速マスター その1</title>
      <link>https://yutakikuchi.github.io/post/201204160815/</link>
      <pubDate>Mon, 16 Apr 2012 08:15:54 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201204160815/</guid>
      <description>[C++] : C++最速マスター その1 C++プログラミング入門
作者: グレゴリーサティア,ダウグブラウン,Gregory Satir,Doug Brown,望月康司,谷口功出版社/メーカー: オライリー・ジャパン発売日: 2001/11メディア: 単行本購入: 9人 クリック: 147回この商品を含むブログ (29件) を見る
Index  業務でC++を使う事になったので少し勉強した内容をまとめておきます。内容をまとめるのに時間がかかりそうなので3回ぐらいの記事に分けてまとめたいと思います。今回の記事では以下を紹介します。
 Compiler  man options usage  Hello World Address / Pointer  Address / Pointer 値渡し / 参照渡し  extern C Class  Basic Class Constructor / Destructor Object Pointer Inheritance Singleton virtual function  Link    Compiler  C言語のコンパイラはgccを利用しましたが、C++ではg++というコンパイラを利用します。基本的な使い方はgcc,g++ともに違いは無いようです。
man $ man g++ G++(1) GNU Tools G++(1) 名称 g++ - GNU プロジェクト C++ コンパイラ (v2.</description>
    </item>
    
    <item>
      <title>Hadoop HDFS SHELL TIPS</title>
      <link>https://yutakikuchi.github.io/post/201204091021/</link>
      <pubDate>Mon, 09 Apr 2012 10:21:16 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201204091021/</guid>
      <description>[Hadoop] : Hadoop HDFS SHELL TIPS Hadoop Articles  今までHadoop関連で紹介した記事は以下のものです。それぞれMapReduceについては紹介したのですが、HDFSの操作については記述していなかったので今回まとめてみました。
 CentOSでHadoopを使ってみる - Yuta.Kikuchiの日記  Hadoopをより便利に使う！HiveでのMapReduceまとめ - Yuta.Kikuchiの日記  「魔法少女まどか☆マギカ」の台詞をJavaScriptでMapReduceしてGoogle Chart APIでグラフ出力したよ！ - Yuta.Kikuchiの日記     HDFS SHELL  HDFS File System Shell Guide 
 cat  chgrp chmod chown copyFromLocal copyToLocal count cp du dus  expunge get  getmerge ls lsr  mkdir moveFromLocal moveToLocal mv put  rm rmr  setrep stat tail test text touchz    OverViews   The FileSystem (FS) shell is invoked by bin/hadoop fs .</description>
    </item>
    
    <item>
      <title>「魔法少女まどか☆マギカ」の台詞をJavaScriptでMapReduceしてGoogle Chart APIでグラフ出力したよ！</title>
      <link>https://yutakikuchi.github.io/post/201204030828/</link>
      <pubDate>Tue, 03 Apr 2012 08:28:04 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201204030828/</guid>
      <description>[Hadoop] : 「魔法少女まどか☆マギカ」の台詞をJavaScriptでMapReduceしてGoogle Chart APIでグラフ出力したよ！ Hadoop 第2版
作者: Tom White,玉川竜司,兼田聖士出版社/メーカー: オライリージャパン発売日: 2011/07/23メディア: 大型本購入: 9人 クリック: 182回この商品を含むブログ (24件) を見る
概要   「魔法少女まどか☆マギカ」の台詞をNLTK(Natural Language Toolkit)で解析する - Yuta.Kikuchiの日記  SpiderMonkeyでのコマンドラインJavascript - Yuta.Kikuchiの日記  CentOSでHadoopを使ってみる - Yuta.Kikuchiの日記  以前に「魔法少女まどか☆マギカ」の台詞をNLTK(Natural Language Toolkit)で解析することに挑戦しましたが、解析結果の集計グラフが奇麗に表示されませんでした。今回はそれを改善すべく手法を変えて挑戦します。グラフ化はNLTKではなくGoogle Chart APIを利用します。Google Chart Tools - Google Code  またHadoopのMapReduceにJavaScriptを用い、Google Chart APIとのデータ連携をしやすいようにします。以下に大まかな処理の流れを記述します。
 SpiderMonkeyをCentOSに設定 魔法少女まどか☆マギカの台詞をPythonでスクレイピング NLTKによる分かち書き JavaScriptによるMapReduce HadoopでMapReduce Google Chart APIでMapReduce結果をグラフ化    SpiderMonkeyをCentOSに設定  参考 HadoopのMapReduceをJavaScriptで行うためにCentOSにSpiderMonkeyを設定します。本家サイトのinstall手順を参考にしました。https://developer.mozilla.org/en/Building_only_SpiderMonkey
 ソースコード取得/解凍 $ wget http://ftp.</description>
    </item>
    
    <item>
      <title>超絶簡単！JavaScriptの性質を10分で理解するための重要なポイントのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201203120837/</link>
      <pubDate>Mon, 12 Mar 2012 08:37:33 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201203120837/</guid>
      <description>[javascript] : 超絶簡単！JavaScriptの性質を10分で理解するための重要なポイントのまとめ JavaScript: The Good Parts ―「良いパーツ」によるベストプラクティス
作者: Douglas Crockford,水野貴明出版社/メーカー: オライリージャパン発売日: 2008/12/22メディア: 大型本購入: 94人 クリック: 1,643回この商品を含むブログ (187件) を見る
JavaScriptのニーズ  NodeJSやTitaniumMobileの普及によりサーバサイド/スマフォアプリの作成をJavaScriptで書こうとする動きが盛んです。それだけ注目を集めているせいかブログの記事でもJavaScriptのネタを書くとはてぶ登録されやすい傾向が現れます。一応今までJavaScript系の記事をいくつか書いてきたのでリンクを紹介します。
 Javascriptによる正規表現まとめ - Yuta.Kikuchiの日記  WebSocket対応状況のまとめ - Yuta.Kikuchiの日記  Node.jsでWebSocketを試してみる - Yuta.Kikuchiの日記  jQuery Proven Performance Tips And Tricks (翻訳) - Yuta.Kikuchiの日記  SpiderMonkeyでのコマンドラインJavascript - Yuta.Kikuchiの日記  jQueryの参考にすべきSiteのまとめ - Yuta.Kikuchiの日記  javascriptのクラスまとめ - Yuta.Kikuchiの日記  JavaScriptは記述の制限が緩い言語で記述者が自由に定義できてしまいます。自由度が高いという言葉は良いように聞こえますが、各人それぞれの志向に左右されることが多く、特に他人のコードを読む時に苦労します。今日はJavaScriptの性質を理解するためにdebugを仕込みながら勉強を進めたいと思います。
  JavaScriptを理解する上で大切な事  個人的にJavaScriptの性質を理解する上で重要な事はObjectと関数を理解することだと認識しています。JavaScriptのほとんどのデータ定義がObjectで表現されます(関数、文字列、数値以外)。JavaScriptにはClassといった概念が無く、関数定義を他言語でのClassのConstructorのように扱い、Classメソッドのようなものをprototypeといった暗黙参照のObjectで定義します。左の言葉に少し違和感を感じられるかもしれません。後にコード例を示すのでそちらも参照してください。
  配列/連想配列の定義  配列と連想配列は当然ながら別物です。配列はList型のデータに対して連想配列はKeyからValueを得るHashになります。JavaScriptの表記としても別々で扱いますが、両方ともObjectとしての性質を持ちます。
配列の定義 配列の宣言はArrayもしくは[]を利用します。私はArrayで宣言か添字で入れる書き方をします。newを使っても使わなくても同じということであれば使用しない事にしています。また添字を文字列で入れるまたhashで一度宣言という方法は使った事が無いです。</description>
    </item>
    
    <item>
      <title>html5のcanvasを使ってブラウザ上でのお絵描きやニコニコ動画風テロップを実装する</title>
      <link>https://yutakikuchi.github.io/post/201203040039/</link>
      <pubDate>Sun, 04 Mar 2012 00:39:34 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201203040039/</guid>
      <description>[javascript] : html5のcanvasを使ってブラウザ上でのお絵描きやニコニコ動画風テロップを実装する 概要  html5のcanvasで遊んでみます。canvasの2dは学生の頃から使ってGoogleMap上にお絵描きできるシステムを作りました。またcanvasを巧く使えばニコニコ動画風のテロップも作れると思って今回実装してみました。次回は3dに挑戦したいです。
  ブラウザ上でのお絵描き  
仕組み 作り方は非常に簡単でhtmlにcanvasタグを埋め込み、それをJavascriptでdocument.getElementById( &#39;canvas&#39; ).getContext( &#39;2d&#39; );とするだけでcanvasの2dオブジェクトが操作できます。mousedown,mousemove,mouseupのイベントを追加して、downしたら描画開始/moveしたら軌跡を残す/upしたら描画終了という処理の流れになります。当然の事ながらマウスのポイントを取得する必要がありevent.clientX/event.clientYで取得します。canvas上に軌跡を残すにはマウスポイントの一つ前の座標と新しい移動座標をからmoveTo、lineTo、strokeといったcanvasの2dオブジェクトが持っているメソッドを利用するだけです。jsも全部で50行ぐらいです。必要なことを箇条書きでも記します。
   項目   簡易コード     canvas2dの取得   document.getElementById( &#39;canvas&#39; ).getContext( &#39;2d&#39; );     マウスイベント追加   addEventListener( &#39;mousedown&#39;, Canvas.mousedown, false); addEventListener( &#39;mousemove&#39;, Canvas.mousemove, false);
addEventListener( &#39;mouseup&#39; , Canvas.mouseup , false);     マウスポイント取得   { x:e.clientX, y:e.clientY };     canvasに軌跡を書く   var can = document.</description>
    </item>
    
    <item>
      <title>JavaScript Ajax : XmlHttpRequestのLevel2でSameOriginPolicyを回避する</title>
      <link>https://yutakikuchi.github.io/post/201202251904/</link>
      <pubDate>Sat, 25 Feb 2012 19:04:34 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201202251904/</guid>
      <description>[javascript] : JavaScript Ajax : XmlHttpRequestのLevel2でSameOriginPolicyを回避する クロスドメイン制限の回避について   今まではXHR(XmlHttpRequest)の仕様によりJSを読み込んでいるHTMLファイルがあるサーバから異なるドメインサーバへのAjaxリクエストが制限されていました。Same Origin Policyと呼ばれているものです。Same Origin Policyの役割としては悪意のあるscriptが個人情報等を他のサイトに転送する事を防ぐためです。このセキリティ制限を回避するために多くの人が代表的なJSONP(JSON with Padding)を利用してサーバサイドでクライアントのコールバック関数をechoしてクライアント側で実行されることにより、クロスドメイン間のAjax通信をそれっぽく動くように対応していたと思います。 JSONPについては以前記事を書いたので宜しければ参照してください。20秒で理解するJSONP - Yuta.Kikuchiの日記   JSONPを利用するのは少し面倒でコールバック関数をリクエストされたサーバサイドから返却しないといけなく、またセキュリティ面でも不安で十分に気をつける必要があります。JSONP以外にもリバースProxyを用意する、Flashを経由するとクロスドメインリクエスト可能となりますがこの面倒な事に頭を悩ませるJavascripterも多かったと思うのですが、色々と調べてみるとXHRのLevel2の仕様によりJSONPを使わずとも異なるドメイン間でのAjaxリクエストが可能となりそうです。    制限を回避するために  Access-Control-Allow-Origin  HTTP access control - MDN にあるようにAccess-Control-Allow-OriginというHttpResponseHeaderを仕込むとクロスドメインを回避する事ができるようです。このHttpResponseHeaderをどこに仕込むのか？ということが疑問になりそうですが、Ajaxリクエスト先のAPIサーバに設置します。(リクエスト元に設定する方式だとXSRFなどが好き勝手出来てしまうのでそれは無いですね) Access-Control-Allow-Originにリクエストを受け付けるURLを指定、もしくは*(ワイルドカード)を指定すると全てのURLを受け付ける設定になります。   関連Header一覧  色々なブラウザ仕様があるのでAccess-Control-Allow-Originだけでなく、念のため以下のResponseHeaderも設定しておくと動作が確認できるようです。  Access-Control-Allow-Origin : 上で説明した通りでアクセス元のURLを指定します。*(ワイルドカード)指定可能です。 Access-Control-Allow-Methods : GET,POST,PUT,DELETE,OPTIONSなどの受け付けるRequestMethodを指定します。 Access-Control-Allow-Headers : RequestHeaderに仕込んである値を見て許可する内容を指定します。*(ワイルドカード)指定可能です。 Access-Control-Max-Age : 各種OptionHeaderの有効時間を設定します。      Demo  今回のAPI/ClientともにサンプルコードをGitHubに挙げました。https://github.com/yutakikuchi/JS/tree/master/crossdomain
API Server  アクセスするAPIサーバをGoogle App Engineに置きます。上で説明したHeaderを仕込み、とりえあずは簡単な文字列(CrossDoaminRequest)だけをprintします。 #!/usr/bin/env python # -*- coding: utf-8 -*- from google.</description>
    </item>
    
    <item>
      <title>継続的インテグレーション(CI)ツールJenkinsを導入するためのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201202220843/</link>
      <pubDate>Wed, 22 Feb 2012 08:43:14 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201202220843/</guid>
      <description>[Web] : 継続的インテグレーション(CI)ツールJenkinsを導入するためのまとめ 
概要  誰もがSorceビルドや開発debugは面倒と思う作業。これらの作業を少しでも手間を掛からないようにするために継続的インテグレーションツールのjenkinsというものがあります。jenkinsはテストおよびビルドの自動化やバグの早期発見に使われます。主にJava開発者向けのツールのようですが、他言語でも利用が出来るようです。今回の記事では主に導入周りの情報を整理したいと思います。
  導入のメリット  @ITにJenkinsの特徴が書いてあります。「Hudson」改め「Jenkins」で始めるCI（継続的インテグレーション）入門 (1/4) - ＠IT 
 ソースコードの自動ビルド バグの早期発見レポート ソースコード解析、統計 テストの自動化 様々なプラグインによる機能拡張 日本語版のJenkins wikiにも色々と書いてあります。Meet Jenkins - 日本語 - Jenkins Wiki 
 差分サポート 永続リンク RSS/Eメール/IM との連携 ビルド後のタグ JUnit/TestNGによるテスト結果のレポート 分散ビルド ファイル指紋 infoQのJenkins開発者のコラムに流行った理由が書いてありました。InfoQ: Jenkinsによる継続的インテグレーションのススメ(1) 
 インストールや設定がWebUIから行える 拡張性の高いプラグイン    対応OS  本家サイトにもあるように各種OSに対応したパッケージがあるようです。Welcome to Jenkins CI! | Jenkins CI 
 Windows Ubuntu/Debian Red Hat/Fedora/CentOS Mac OS X openSUSE FreeBSD OpenBSD Solaris/OpenIndiana Gentoo    導入  環境 私が設定する環境はMacOSX Version10.</description>
    </item>
    
    <item>
      <title>PHPのHash/暗号化関数の使用方法まとめ</title>
      <link>https://yutakikuchi.github.io/post/201202050219/</link>
      <pubDate>Sun, 05 Feb 2012 02:19:45 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201202050219/</guid>
      <description>[PHP] : PHPのHash/暗号化関数の使用方法まとめ 概要  「そもそもHashと暗号化って何が違うの？」この記事はそういった疑問を持っている私自身がまとめた記事でとても初歩的な内容になります。記事の紹介の中ではPHPをメインに話を進めます。PHPのHash関数、暗号化関数の種類が豊富で中々覚えづらい内容が多いです。今回の記事ではPHPによるHash/暗号化関数の使用とJAVAとの共通鍵暗号方式を意識した内容になります。また以前共通鍵暗号のAESについての内容をまとめたのでそちらも参照してください。AES暗号のまとめ - Yuta.Kikuchiの日記 
  Hash、暗号化の違いについて  個人的な見識を書きますので間違っている可能性が高いです。
 HashとはMD5やSHAなどのHashアルゴリズムに基づいて生成されるチェックサムです。特定の値に対してアルゴリズム(関数)をかけることによって1つの値を取得します。導き出されたHash値から元の値を出す事は難しいとされ、一般的には不可逆性を持つと言えます。Hashの目的としてはデータの送信、受け取り側の両方で共通のアルゴリズムでHashを確認するようなデータ改竄の防止などに利用されています。 暗号化とはデータの秘匿性を守るための方法です。公開鍵暗号/共通鍵暗号などの方式がありますが、平文というデータを第三者に知られないように鍵を用いて暗号文を作成することが目的です。Hashと大きく異なるのが暗号文から鍵を利用して平文を取り出せるので、可逆性であると言えます。 注意:Hashに対しても鍵を使用することがあります。たとえばSHA256のhmac方式等です。SHA256というアルゴリズムに対して鍵を指定してHashを作成します。    PHPでHash化  似たような関数が存在し分かりにくいので実際に使用した感じをまとめていきます。
登録されているHashアルゴリズム一覧  print_r(hash_algos()); Array ( [0] = md2 [1] = md4 [2] = md5 [3] = sha1 [4] = sha224 [5] = sha256 [6] = sha384 [7] = sha512 [8] = ripemd128 [9] = ripemd160 [10] = ripemd256 [11] = ripemd320 [12] = whirlpool [13] = tiger128,3 [14] = tiger160,3 [15] = tiger192,3 [16] = tiger128,4 [17] = tiger160,4 [18] = tiger192,4 [19] = snefru [20] = snefru256 [21] = gost [22] = adler32 [23] = crc32 [24] = crc32b [25] = salsa10 [26] = salsa20 [27] = haval128,3 [28] = haval160,3 [29] = haval192,3 [30] = haval224,3 [31] = haval256,3 [32] = haval128,4 [33] = haval160,4 [34] = haval192,4 [35] = haval224,4 [36] = haval256,4 [37] = haval128,5 [38] = haval160,5 [39] = haval192,5 [40] = haval224,5 [41] = haval256,5 )   16ByteのMD5を得る方法 16進数表記であるため32文字でデータ量は16Byteになります。hash、hash_init/hash_update/hash_final、md5のどれでも同じ値を返します。</description>
    </item>
    
    <item>
      <title>InnoDBの設計とインデックスを意識したサロゲートキーと複合プライマリキーの比較</title>
      <link>https://yutakikuchi.github.io/post/201201291307/</link>
      <pubDate>Sun, 29 Jan 2012 13:07:32 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201201291307/</guid>
      <description>[Mysql] : InnoDBの設計とインデックスを意識したサロゲートキーと複合プライマリキーの比較 実践ハイパフォーマンスMySQL 第2版
作者: Baron Schwartz,Peter Zaitsev,Vadim Tkachenko,Jeremy D. Zawodny,Arjen Lentz,Derek J. Balling,伊藤直也(監訳),田中慎司(監訳),吉川英興(監訳),株式会社クイープ出版社/メーカー: オライリージャパン発売日: 2009/12/14メディア: 大型本購入: 17人 クリック: 373回この商品を含むブログ (46件) を見る
概要  この記事ではInnoDBを使用する上でのクラスタインデックスを意識したサロゲートキーと複合主キーの比較をパフォーマンスの観点から行います。サロゲートキーか複合キーのどちらを使うべきかは様々な議論がなされているようにケースバイケースといったところで自分は納得しています。ここでは自分の実現したいテーブル設計と利用目的を明確にしてそのケースに合うのはどちらかという検証をしたいと思います。ちなみに今までの業務ではサロゲートを使う事がほとんどでした。理由としては参加したPJのほとんどで元々のテーブルが全部サロゲートであること、また自分で設計する時も主キー管理に面倒なことを考えなくて済むからという理由でサロゲートを使用していました。(使用しているフレームワークの制限でサロゲートを使用しないといけないということはありませんでした。)今までパフォーマンスを求められる箇所のDB設計を担当しておらず勉強不足が否めないので、今回検証してみることにしました。間違い等あればどんどんご指摘ください。宜しくお願いいたします。以下初めにこの記事で利用するキーの名前の定義をしておきます。
キーの定義    名前   意味     ナチュラルキー(自然キー)   システムの外部から入力される値に対するキー。      サロゲートキー(代替キー)   自動生成される連番キーで主にプライマリーに利用されるが、値としては意味を持たない。    プライマリキー(主キー)   テーブル内の行を一意とするための識別子。    複合プライマリキー(複合主キー)   複数のナチュラルキーにて構成されるプライマリキー       ユニークキー(一意キー)   NULL以外をユニークとするキー。    複合ユニークキー   複数のカラムでユニークを意味するキー。      実現したい内容  商品販売を行う携帯サービスを展開する時に 「どのユーザ」が「どの商品」を「どの携帯電話」で利用しているかという状況の履歴を管理するテーブルを作成します。データとしてはユーザ、商品、携帯電話の値それぞれがデータの意味としては一意となりますが履歴のテーブルであるため3つ揃って一つのユニークな履歴となります。（3つの値を持つ同じ履歴レコードは他には存在しない状況）テーブルとして必要な情報を以下にまとめます。</description>
    </item>
    
    <item>
      <title>PHPにおける時間表記のISO-8061、DATE_ATOM、DATE_RFC3309、DATE_W3Cの違いは何か</title>
      <link>https://yutakikuchi.github.io/post/201201240753/</link>
      <pubDate>Tue, 24 Jan 2012 07:53:43 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201201240753/</guid>
      <description>[PHP] : PHPにおける時間表記のISO-8061、DATE_ATOM、DATE_RFC3309、DATE_W3Cの違いは何か ISO-8601時刻表記について  ISO-8601はあまり使わない時刻を表記する形式ではあるがAtomフィードの時刻表記でその互換規格として利用されます。Atomについては最近記事を書いたのでそちらを参照してください。RSSより便利なAtomデータの詳細と利用方法について簡単にまとめてみた - Yuta.Kikuchiの日記 ISO-8601とは簡単に説明すると時刻の表記を以下のような形式で表現します。
 年、月、日の関係はYYYY-MM-DDと表記する。(2012-01-24、20120124)。 年、年内の日番号はYYYY-DDDと表記する。(2012024) 2012年の24日目。 年、週、曜日をYYYY-Www-Dと表記する。(2012-W04-2) 2012年の4週目の火曜日。すなわち2012/01/24。 時刻の表記にはhh:mm:ssの形式が利用される。  10時30分20.5秒 ( 10:30:20,5 ) 10:30,5(1130,5)は10:30:30と同じ。 10.5は10:30と同じ。  timezone指定  UTCでは時刻の後ろにZを添える。2012-01-24T12:00Z UTC以外では時刻の後ろに+-hh:mm、+-hhmm、+-hhのいずれかを添える。2012-01-24T12:00+09:00(日本時間での2012/01/24の12時)   日付と時刻の間にはTを入れる。2012-01-24T12:00+09:00 期間を表記する場合は開始日時/終了日で表記する。2012-01-01T00:00+09:00/2012-01-24T12:00+09:00 (2012/01/01の0時〜2012/01/24の12時) 年/月/日の表記が省略表記等自由に表現できることや期間の表現が可能であるというメリットはありますが、定義が曖昧なのと人の目には分かりづらい表記のように思います。(timestampよりはましですが)。しかしほとんどのプログラミング言語、ミドルウェアはこの表記をサポートしているということで覚えた方が良いという事です。
  AtomのDateコンストラクタ  RFC 4287 The Atom Syndication Format 日本語訳  ここのDateコンストラクタ定義によるとRFC3339に準拠している必要があるようですが、この規格自体がISO-8601やW3C.NOTE-datetime-19980827、W3C.REC-xmlschema-2-20041028と互換性があります。以下はDateコンストラクタの簡単な例になります。
2012-01-24T00:00:00Z 2012-01-24T00:00:00.00Z 2012-01-24T00:00:00+09:00 2012-01-24T00:00:00.00+09:00   PHPでの各規格の表記の違い  PHP: DateTime - Manual 
ここに載っているDateTime Manualの表記の違いについてコメントしますが、結論からするとISO8601だけ少し表記が違うぐらいの差でしかありません。なぜなら上のISO-8601の定義から同じ時刻を示すからです。規格自体が変わらないのだとすれば定数を増やすのは無意味と感じてしまいます。
   定数   実データ     DATE_ISO8601   2012-01-23T23:14:15+0900       DATE_RFC3339   2012-01-23T23:14:15+09:00     DATE_ATOM   2012-01-23T23:14:15+09:00     DATE_W3C   2012-01-23T23:14:15+09:00   PHP: date - Manual  こちらのdate関数の説明も更に混乱を招きます。date関数の第一引数に&#39;c&#39;を指定するとISO8601形式と書いてありますが、実際には他の3パターンの時刻表記で表示されます。以下はPHPのサンプルコードとその実行結果です。どれも同じ値を示しますが混乱しないように注意しましょう。</description>
    </item>
    
    <item>
      <title>実践 ハイパフォーマンスMySQL(第2版)を斜め読みして前半の重要なポイントだけをまとめてみた</title>
      <link>https://yutakikuchi.github.io/post/201201221535/</link>
      <pubDate>Sun, 22 Jan 2012 15:35:27 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201201221535/</guid>
      <description>[Mysql] : 実践 ハイパフォーマンスMySQL(第2版)を斜め読みして前半の重要なポイントだけをまとめてみた 実践ハイパフォーマンスMySQL 第2版
作者: Baron Schwartz,Peter Zaitsev,Vadim Tkachenko,Jeremy D. Zawodny,Arjen Lentz,Derek J. Balling,伊藤直也(監訳),田中慎司(監訳),吉川英興(監訳),株式会社クイープ出版社/メーカー: オライリージャパン発売日: 2009/12/14メディア: 大型本購入: 17人 クリック: 373回この商品を含むブログ (46件) を見る
 実践ハイパフォーマンスMySQLですがMySQL開発者にとっては誰もが目を通しておいた方が良いと推薦される本なので重要なポイントだけをまとめてみたいと思います。まだ前半しか読んでいませんがMySQLの細かいテクニックが載っていて感動です。全部で700ページ近くある本なので斜め読みをした結果を載せて行きます。内容としては実践的なものをカットし、理論的なものだけを抽出しています。今回の記事だけでなく複数回に分けて内容を紹介したいと思います。この記事とは関係なく過去にIndexについての記事を書いたのでそちらも参照してもらえればと思います。MYSQL INDEXのまとめ - Yuta.Kikuchiの日記 
 目次   1章 MySQLのアーキテクチャ  MySQLの論理アーキテクチャ 平行性の制御 トランザクション マルチバージョンの平行性制御(MVCC) MySQLのストレージエンジン  2章 ボトルネックの検出:ベンチマークとプロファイリング  ベンチマークを実行する理由 ベンチマーク戦略 ベンチマーク戦術 ベンチマークツール  3章 スキーマの最適化とインデックス  最適なデータ型の戦略 インデックスの基礎 高いパフォーマンスを実現するためのインデックス戦略 インデックスのケーススタディ インデックスとテーブルの管理 正規化と非正規化 ALTER TABLEの高速化 ストレージエンジンに関する注意点     MySQLのアーキテクチャ  MySQLの論理アーキテクチャ  MySQLの論理的なアーキテクチャは以下の3つのレイヤから構成される。  接続管理とセキュリティ 最適化と実行 ストレージエンジン  最適化と実行例レイヤがMySQLの中枢であり、Query解析、分析、最適化、キャッシュ、組み込み関数、ストアドプロシージャ、トリガ、ビューなどが含まれる。 ストレージエンジン間の違いを吸収するためサーバとエンジンはAPIを経由してデータのやり取りを行う。 クライアントから接続されるときは1つのスレッドを作成し、スレッドはコアやCPUと関連付けられる。スレッドは常にキャッシュされる。 接続はSSLでも可能である。 最適化のレイヤではQueryを解析して内部構造を作成し、Queryの書き換え、テーブルを読み取る順序、使用するインデックスの選択などが行われる。 オプティマイザが最適化を試みる。オプティマイザについては後で詳しく説明。 発行したQueryに対する結果がキャッシュされていれば(クエリキャッシュ)ばその結果を返すだけ。   平行性の制御  データベースの世界ではロック問題が絶え間なく発生する。ロックシステムの実装は大きく分けて次の２点  共有ロック(読み取りロック) 排他ロック(書き込みロック)  ロックの粒度でロック対象のデータ容量を最小に抑える事が望ましい。 ロックのオーバヘッド(ロックの取得、チェック、解除)にリソースが取られパフォーマンスが低下する可能性がある。 テーブルロックは最もオーバヘッドが低い。テーブル全体をロックするから1クライアントが書き込み中は他のクライアントからの読み取りや書き込みは全て拒否される。 書き込みロックは読み取りロックよりも常に優先される。 行ロックは平行性が最も高いがオーバーヘッドも同様に高い。行ロックはストレージエンジンで実装されている。   トランザクション  トランザクションとは1つの作業単位として不可分に扱われるSQLQueryの集まり。処理を一つにまとめて失敗した時は処理全体をロールバックする事を可能にする。 トランザクションはACID特性を持つ必要がある。Atomicity(不可分性)、Consistency(一貫性)、Isolaction(分離性)、Durability(永続性) トランザクジョンもパフォーマンスを下げるので、不必要であればトランザクションに対応しないストレージエンジンを選ぶ事によって改善できる可能性がある。 トランザクションは4つの分離レベルが定義されている。分離レベルが低いほど平行性が高くなる。READ UNCOMMITTED、READ COMMITTED、REPEEATABLE READ、SERIALIZABLE。 デッドロックとは複数のトランザクションが互いに拘束し合うこと。トランザクジョンが異なる順序でリソースをロックしようとした時に発生する。ロックの解放を永遠に待ち続けてしまう。 デッドロックを回避するために検知システムとタイムアウトを実装している。InnoDBなどは循環依存に気づいてすぐにエラーを返す。InnoDBのデッドロックへの対処法は排他ロック行がもっとも少ないトランザクションをロールバックすることである。 トランザクションログをはトランザクションの効率化に役立つ。ストレージエンジンは変更が発生する度にディスク上のデータを更新するのではなく、データのメモリ上のコピーを変更する。その後にトランザクションログに変更を記録する。 MySQLデフォルトではAUTOCOMMITモードで動作している。トランザクジョンを明示的に開始しない限り各Queryを別々のトランザクションで実行する。 InnoDBとMyISAMの併用をしたときに、トランザクションのロールバックが発生した場合は注意しないといけない。理由は非トランザクションテーブルへの変更を取り消す事ができないからデータの矛盾が発生してしまう可能性がある。   マルチバージョンの平行性制御(MVCC)  トランザクションをサポートしているエンジンは行ロックとMVCCを組み合わせている。MVCCはバージョンを基にしたスナップショットを作成する仕組み。InnoDBのMVCCは各イベントが発生した時のバージョンを記録している。   MySQLのストレージエンジン  各MySQLストレージエンジンのまとめ    ストレージエンジン  MySQLバージョン  トランザクション  ロックの粒度  用途   使用すべきでない状況     MyISAM   全て   不可   同時挿入が可能なテーブル   SELECT、INSERT、一括読み込み   読み取りと書き込みの混在     MyISAM Merge   全て   不可   同時挿入が可能なテーブル   セグメント化されたアーカイブ、データウェアハウジング   多くのグローバルルックアップ     Memory(ヒープ)   全て   不可   テーブル   中間計算、静的なルックアップデータ   大きなデータセット、永続ストレージ     InnoDB   全て   可   MVCCを使用する行レベル   トランザクション処理   無し     Falcon   6.</description>
    </item>
    
    <item>
      <title>RSSより便利なAtomデータの詳細と利用方法について簡単にまとめてみた</title>
      <link>https://yutakikuchi.github.io/post/201201200831/</link>
      <pubDate>Fri, 20 Jan 2012 08:31:57 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201201200831/</guid>
      <description>[WebAPI] : RSSより便利なAtomデータの詳細と利用方法について簡単にまとめてみた 
概要  Google Data APIがAtom形式のプロトコルを採用しているように、RSS/Atomというサイト情報を配信する仕組みからWebAPIでのリクエストやレスポンスにまでAtomが広く利用されつつあります。Atomって単純なXML文書の拡張でRSSとさほど変わりないと思っていたのですが、最近仕事でAtomに触れる機会が増えてきているので簡単にまとめた記事を書きます。
  RSSとの違い  Atomが生まれた経緯はRSSが古い仕様かつRSS1.0/2.0の仕様策定で色々と揉めていたときにAtomを作ろうという動きになったようです。RSSのバージョンごとに非互換性であったり、仕様が明確になっていなかった事が利用者からの不満が多くありました。今後のRSS仕様の策定は2.0から改変が加えられる様子は無いです。RSSの不満は仕様が曖昧な故に配信者に委ねられるという点です。例えばHTMLのマークアップがコンテンツに含めて良いのか？などの規定がありません。AtomはRSSでの反省を踏まえて、特定のベンダに依存しない、全ての人が自由に実装できる、誰でも自由に拡張可能である、明確にかつ詳細に定義する理念を掲げています。
  Atomについて  Atomには二つの仕様があります。一つ目はコンテンツ配信フィード用で｢Atom配信フォーマット｣（Atom Syndication Format）と呼ばれています。もう一つはコンテンツを編集するための｢Atom出版プロトコル｣（Atom Publishing Protocol）でAtomAPIやAtomPPと呼ばれる事もあるようです。Atom Syndication FormatはRFC4287で、Atom Publishing ProtocolはRFC5023で策定がされています。Atomのメリットをいくつか挙げるとすると独自の名前空間が使えるのでデータを自由にカスタマイズできる、配信内容がSummary(要約)とContent(内容)に明確に分離されているのでデータ構造が分かりやすい、contentフィールドにはテキストは勿論のことながらHTMLや画像/動画も含めて配信する事が可能になります。またIETFという機関が仕様を管理しているのでRSSのように配信者の意向に左右されることはありません。以下はAtomのサンプルです。
xml version=&#34;1.0&#34;encoding=&#34;utf-8&#34;? xmlns=&#34;http://www.w3.org/2005/Atom&#34; type=&#34;text&#34;dive into mark type=&#34;html&#34; A &amp;lt;em&amp;gt;lot&amp;lt;/em&amp;gt; of effort went into making this effortless  2005-07-31T12:29:29Z tag:example.org,2003:3 rel=&#34;alternate&#34;type=&#34;text/html&#34; hreflang=&#34;en&#34;href=&#34;http://example.org/&#34;/ rel=&#34;self&#34;type=&#34;application/atom+xml&#34; href=&#34;http://example.org/feed.atom&#34;/ Copyright (c) 2003, Mark Pilgrim uri=&#34;http://www.example.com/&#34;version=&#34;1.0&#34; Example Toolkit   Atom draft-07 snapshot rel=&#34;alternate&#34;type=&#34;text/html&#34; href=&#34;http://example.org/2005/04/02/atom&#34;/ rel=&#34;enclosure&#34;type=&#34;audio/mpeg&#34;length=&#34;1337&#34; href=&#34;http://example.org/audio/ph34r_my_podcast.mp3&#34;/ tag:example.org,2003:3.2397 2005-07-31T12:29:29Z 2003-12-13T08:29:29-04:00  Mark Pilgrim http://example.</description>
    </item>
    
    <item>
      <title>「魔法少女まどか☆マギカ」の台詞をNLTK(Natural Language Toolkit)で解析する</title>
      <link>https://yutakikuchi.github.io/post/201112311440/</link>
      <pubDate>Sat, 31 Dec 2011 14:40:04 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201112311440/</guid>
      <description>[Python] : 「魔法少女まどか☆マギカ」の台詞をNLTK(Natural Language Toolkit)で解析する 目次   魔法少女まどか☆マギカ NLTK NLTKコーパス まど☆マギ台詞単語解析 まど☆マギ台詞形態素解析    魔法少女まどか☆マギカ  
NLTK練習の題材として2011年の大ヒットアニメ？「魔法少女まどか☆マギカ」の台詞を用いる。通称まど☆マギで知られる本作品であるが、第15回文化庁メディア芸術祭アニメーション部門大賞、既に映画かも決まっておりテレビシリーズの総集編前後編と完全新作の全3作品の製作が予定されている。いわゆるダークファンタジーの世界観で台詞の中にも絶望を彷彿させるマイナス思考な台詞が多いように思う。事前の予想では「いやだ」とか「助けて」などの台詞が頻繁に使われていると考えたが、それらをNLTKを用いて検証してみる。この記事の前半はNLTKの設定、後半がまど☆マギの台詞を単語/形態素の両面で解析している。尚使用するサンプルコードは全てgithubに挙げているので、そちらを参照して欲しい。https://github.com/yutakikuchi/NLTK/tree/master/madmagi
  NLTK  NLTKとは  NLTKとはNatural Language Toolkitの略。 Pythonで書かれた自然言語処理ToolKit。テキストマイニング等が可能。 Windows/MacOS/Linux上で動作させる事ができる。 Linuxの場合Python2.4x,2.5x,2.6xを必要とする。 インストールする際にはPyAMLが必要で、解析結果をグラフ化するようにNumPyやmatplotlibを一緒に使うと良い。   NLTKのインストール  http://www.nltk.org/download 公式ドキュメントに書いてある内容を実行する。尚今回のインストール実行環境はCentOS release 5.7 (Final)とする。 始めに入れておくと後で嵌らないpkgは次のもの $ sudo yum install gcc-c++ kernel-devel ncurses-devel glibc glibc-devel glib-devel glib2-devel gtk2-devel tk tk-devel tcl tcl-devel tkinter freetype freetype-devel libpng libpng-devel  Pythonのversion確認  pythonのversionが2.4x python pythonのインストールを行う必要がある。 $ python -V Python 2.</description>
    </item>
    
    <item>
      <title>Introduction to Computational Advertising - Stanford大学資料から学ぶオンライン広告知識</title>
      <link>https://yutakikuchi.github.io/post/201112231435/</link>
      <pubDate>Fri, 23 Dec 2011 14:35:43 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201112231435/</guid>
      <description>[AD] : Introduction to Computational Advertising - Stanford大学資料から学ぶオンライン広告知識 Stanford大学オンライン広告授業  ★★Stanford大学オンライン広告授業資料★★
機械学習/DB/人工知能の内容での授業公開で有名なStanford大学ですが、オンライン広告の授業資料も公開しています。一応私も広告配信、集計、解析に業務で携わっており、ようやく最近海外のオンライン広告のトレンドや論文の内容に注目し始めました。Standord大学オンライン広告の授業一覧と内容は次のようになっています。
 Overview and Introduction Marketplace and Economics Textual Advertising 1: Sponsored Search Textual Advertising 2: Contextual Advertising Display Advertising 1 Display Advertising 2 Targeting Recommender Systems Mobile, Video and other Emerging Formats Project Presentations 今日のブログでは第1回のOverview and Introductionについて日本語に訳しながら個人的に重要だと思う箇所だけ紹介します。みなさんからの反応が良ければ2回目以降の内容についても記述するかもです(笑) ちなみに授業のInstructorはYahoo!Incで研究者をやっているVanja Josifovski、Andrei_BroderAndrei Broderさんの二人です。
  授業内容  (注) 以下ではcomputational” advertisingをオンライン広告と訳しています。本来の直訳はコンピュータによる広告です。
オンライン広告とは  Find the &#34;best match&#34; between a given user in a given context and a suitable advertisement.</description>
    </item>
    
    <item>
      <title>Hadoopをより便利に使う！HiveでのMapReduceまとめ</title>
      <link>https://yutakikuchi.github.io/post/201112190830/</link>
      <pubDate>Mon, 19 Dec 2011 08:30:34 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201112190830/</guid>
      <description>[Hadoop] : Hadoopをより便利に使う！HiveでのMapReduceまとめ 
目次   Hiveとは Hiveの設定 HiveQL構文(DDL)  DataBase/SCHEMAの作成 Database/SCHEMAの削除 Tableの作成 Tableの削除 Table名変更 Partition作成 Partitionの削除 Columnの変更 Columnの追加/置き換え TableのProperty変更 SerDe Propertyの追加  HiveQL構文(SQL)  テーブル一覧表示 テーブルの内容を表示 基本的なSELECT文 WHERE句(条件指定) DISTINCT(重複削除) ORDER BY / SORT BY句(ソート) GROUP BY句(グループ化) HAVING句(グループ化後の条件) LIMIT句 JOIN(テーブル結合) 抽出カラムを正規表現で指定 UNION(結果の結合) SUBQUERY LOAD(データの読み込み)  Hiveを使う  事前準備 起動と終了 テーブルの定義 データの読み込み Tableのパーティション定義 検索/MapReduce SELECT結果をINSERT ローカルファイルとして保存  まとめ リンク    Hiveとは   2008年にFaceBookで開発さえてHadoopプロジェクトに寄贈される。 Yahoo!で開発しているPigのライバルプロジェクト？ 一言で表すとHadoop上で動作するデータウエアハウス。 HiveQLというSQLのような言語でHDFSなどの分散ファイルシステム上のデータを操作できる。 HiveQLの実行でMap/Reduce処理が完了する。 私見だが複雑なデータのMapReduceから特定のデータを抽出したい場合には便利かも。    Hiveの設定   CentOSでのHiveの設定はCentOSでHadoopを使ってみる - Yuta.</description>
    </item>
    
    <item>
      <title>NLP(自然言語処理)用語まとめ</title>
      <link>https://yutakikuchi.github.io/post/201112121517/</link>
      <pubDate>Mon, 12 Dec 2011 15:17:42 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201112121517/</guid>
      <description>[自然言語処理] : NLP(自然言語処理)用語まとめ 一般用語  NLP  自然言語処理のこと。Natural language processing : NLP。 コンピュータが人間の言葉を処理する事。自然言語の反対は形式言語。   テキストマイニング  自然言語処理とデータマイニングの技術を合わせてテキストから知識発見を行う技術。  知らない知識をテキストから発見するのをテキストマイニング。 既知の情報の位置を特定するのを情報検索。     素性  属性/属性値   自立語  他の単語が無くても意味をなす単語。助詞/助動詞以外。   bag of words  与えられたテキストの集合で並び順が無視されること。   N-Gram  連続するn個の要素。要素が何を表すかによって表現が異なる。  &#34;文字N-gram&#34; &#34;単語N-gram&#34;  2-gram(bigram)、3-gram(trigram)をよく使う。 形態素解析では文字n-gramを利用する。   TF・IDF  索引後の重み付け方法。 TF(Term Frequency)は文書に置ける単語の頻度 IDF(Inverted Document Frequency)は索引語が現れる相対文書頻度の逆数の対数 1文書に同一の索引語が多く出現すればTF・IDFの値は大きくなる。多くの文書に索引後が出現すれば値は小さくなる。   コサイン類似度  二つのデータが似ているかどうかをデータベクトルの距離により算出すること。   コーパス  自然言語の大量のテキスト集合。   シソーラス  単語の関係(上位/下位、部分/全体、同義、類義)によって分類し体系づけた辞書。     機会学習   人間が自然と行っているパターン認識や経験則に基づく判断をコンピュータを用いて行う技術や理論。  ナイーブベイズ(単純分類器)  ベイズの定理を適用することに基づいた単純な確率分類器。 テキスト分類に頻繁に用いられている。 パラメータ推定には最尤法が使われる。   決定木  葉と根を利用した予測モデル。 データマイニングでよく利用され、葉が分類、枝がその分類に至るまでの特徴の集合。   K-平均  距離ベースのクラスタアルゴリズムで事前に決められた数のクラスタにデータを割り振る。   SVM  Support Vector Machine。２値分類器。 座標上にサンプル値をプロットし、正値/負例の集合からもっとも距離が大きくなる識別面 を決定。</description>
    </item>
    
    <item>
      <title>HerokuでNodeJSを動かしてみる</title>
      <link>https://yutakikuchi.github.io/post/201112091947/</link>
      <pubDate>Fri, 09 Dec 2011 19:47:36 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201112091947/</guid>
      <description>[javascript] : HerokuでNodeJSを動かしてみる 
前提   Herokuとは？  rubyやnodejsのPaas。ファイルのDeployでアプリケーションを作成することができる。 Googleが提供しているGAEやWindowsのAzureみたいなもの。 少し前にSalesforce.comに買収されている。 Herokuのアカウント登録は次のURLから https://api.heroku.com/signup GitHubでソースを開発/管理して、動作させるのはHerokuサーバ。これが素晴らしい。 アプリケーションサーバを1プロセス(dyno)を利用できる。dynoとはHeroku独自の単位。 PostgreSQLを5MBまで利用可能。  この記事の紹介はHerokuのアカウントを保持していること、自分のPCにgem(Rubyのパッケージインストールコマンド),node,npmが入っている事を前提とする。(node,npmあたりは以前記事を書いた Node.jsでWebSocketを試してみる - Yuta.Kikuchiの日記  を参照して欲しい) GitHubに対する設定と知識も必要。以前Macでの設定を書いた。MacユーザのためのGitHub登録手順 - Yuta.Kikuchiの日記  私が試した環境はMacOSX。バージョン10.6.8    準備  
nodejs/npmのバージョン確認 $ node -v v0.4.8 $ npm -v 1.0.6    Herokuパッケージ設定手順  Herokuのinstall $ sudo gem update --system $ sudo install heroku  アプリ一覧を表示 まだ何も登録していないのでYou have no apps.と表示される。
$ heroku list Enter your Heroku credentials Email: hoge@gmail.</description>
    </item>
    
    <item>
      <title>CentOSでHadoopを使ってみる</title>
      <link>https://yutakikuchi.github.io/post/201112050830/</link>
      <pubDate>Mon, 05 Dec 2011 08:30:24 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201112050830/</guid>
      <description>[Hadoop] : CentOSでHadoopを使ってみる __ __ __ / / / /___ __/ / ____ ____ / /_/ / __ ‘/ __ / __ \/ __ \/ __ / __ / // / // / // / // / // / // //_,/_,_/_/_/ .__/ // インストール
  hadoopをcentosに入れてみる。最新バージョンは2011/11/25日の段階では0.23.0 各Linuxディストリビューションに対応済みのcdh3(Cloudera Distribution including Apache Hadoop v3)を入れる。cdh3の最新バージョンは0.20.0 hadoopの他に愉快な仲間達のhive,pig,hbaseも入れる。  jdkのインストール  hadoopはjavaで動くので当然必要となる。既にインストール済みの場合は不要。 jdkのダウンロード  jdkのインストール  $ wget http://download.oracle.com/otn-pub/java/jdk/7u1-b08/jdk-7u1-linux-x64.rpm $ sudo rpm -ivh jdk-7u1-linux-x64.</description>
    </item>
    
    <item>
      <title>快適なUnixLifeのために覚えておきたい10のTips</title>
      <link>https://yutakikuchi.github.io/post/201111250834/</link>
      <pubDate>Fri, 25 Nov 2011 08:34:19 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201111250834/</guid>
      <description>[Linux] : 快適なUnixLifeのために覚えておきたい10のTips 
 この記事では快適なUnixLifeを送るための便利な開発環境設定とコマンドの基本的なTipsについて紹介する。 vim.emacsなどのeditorについて紹介するとキリがないので、また別で紹介したいと思う。  目次
  screen zsh find  grep  mkdir diff nkf perl cut,sort,uniq その他    screen
  poderosaなどのタブ機能付きターミナルを使わなくてもscreenコマンドで複数のターミナルウィンドウを管理することが可能。 サーバ内部のプロセスとしてscreenを管理しているので、restartなどをしない限りscreenのデタッチで一時的終了、アタッチで元々の作業画面を呼び起こす事が可能。よって会社での作業もscreenを使っていれば前日の作業の途中から開始する事ができる。  設定  必要な設定ファイルは$HOME/.screenrcとして次のような内容を書き込む。 escape ^Z^z #control + zでscreenを操作する hardstatus alwayslastline &#34;%`%-w%{=b bw}%n %t%{-}%+w&#34;#接続しているhost名を表示する bind s startup_message off # おまけ: 起動画面OFF  autodetach on # おまけ: 回線切断時に自動でdetach defscrollback 1024 # おまけ: スクロールバッファ行数を増やす bind s split #画面分割 bind i focus #画面移動 bind 1 only #一画面に戻す bind 0 remove #画面削除 bind r eval ‘echo “Resize window”‘‘command -c resize’#Ctrl+trで分割した画面サイズ を変更｡ bind -c resize ^] command  bind -c resize j eval ‘resize +1′‘command -c resize’#jで1行大きくきく bind -c resize k eval ‘resize -1′‘command -c resize’#kで1行小さくさく   新規でsshした時に新しいscreen windowを開くように$HOME/.</description>
    </item>
    
    <item>
      <title>美女サイトのデータハッキング方法をまとめてみた</title>
      <link>https://yutakikuchi.github.io/post/201111240842/</link>
      <pubDate>Thu, 24 Nov 2011 08:42:46 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201111240842/</guid>
      <description>[Python] : 美女サイトのデータハッキング方法をまとめてみた 

美女サイト  美人時計　http://www.bijint.com/ 美女暦　http://www.bijogoyomi.com/ 今更ではあるが2つのサイトのデータをハッキングしてみた。 以下の内容を試す場合は自己責任でお願いします。また内容はすぐに消す可能性があります。   美人時計の画像  数分置きに美人が入れ替わる仕組みで、画像のURLは日付に関係なく時間で固定。例えば http://www.bijint.com/jp/tokei_images/1200.jpg URLが時間で固定なので好みの美人の画像に巡り会えない可能性がある。 アクセスする際にrefererを仕込んでおこないとエラーになるので、refererをhttp://www.bijint.com/jp/とする。 プロフィールが掲載されているhtmlパーツもほぼ同じ仕組みで取得可能。   画像、htmlパーツ取得のheader情報  実際にブラウザ上でどのようにデータが取得されているのかを見てみる。ツールはwebインスペクタやfirebugでheader情報を参照する。以下の内容はsafari = webインスペクタ = ネットワーク = ヘッダを参照したもの。 画像 URL を要求:http://www.bijint.com/jp/tokei_images/0000.jpg リクエストメソッド:GET ステータスコード:200 OK リクエストヘッダソースの表示 Referer:http://www.bijint.com/jp/ User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_6_8) AppleWebKit/534.51.22 (KHTML, like Gecko) Version/5.1.1 Safari/534.51.22 レスポンスヘッダソースの表示 Accept-Ranges:bytes Connection:close Content-Length:156987 Content-Type:image/jpeg Date:Wed, 23 Nov 2011 16:00:00 GMT Last-Modified:Mon, 03 Oct 2011 05:04:15 GMT Server:Apache  htmlパーツ URL を要求:http://www.</description>
    </item>
    
    <item>
      <title>yumコマンド</title>
      <link>https://yutakikuchi.github.io/post/201111131340/</link>
      <pubDate>Sun, 13 Nov 2011 13:40:16 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201111131340/</guid>
      <description>[CentOs] : yumコマンド 概要  BSDのportコマンドのようにLinuxのRPMパッケージを管理するプログラム。 パッケージインストーラとも言える。 パッケージの依存関係解消をある程度自動的に解消?   yum usage List of Commands: check-update Check for available package updates clean Remove cached data deplist List a package&#39;s dependencies downgrade downgrade a package erase Remove a package or packages from your system groupinfo Display details about a package group groupinstall Install the packages in a group on your system grouplist List available package groups groupremove Remove the packages in a group from your system help Display a helpful usage message info Display details about a package or group of packages install Install a package or packages on your system list List a package or groups of packages localinstall Install a local RPM makecache Generate the metadata cache provides Find what package provides the given value reinstall reinstall a package repolist Display the configured software repositories resolvedep Determine which package provides the given dependency search Search package details for the given string shell Run an interactive yum shell update Update a package or packages on your system upgrade Update packages taking obsoletes into account options: -h, --help show this help message and exit -t, --tolerant be tolerant of errors -C run entirely from cache, don&#39;t update cache -c [config file] config file location -R [minutes] maximum command wait time -d [debug level] debugging output level --showduplicates show duplicates, in repos, in list/search commands -e [error level] error output level -q, --quiet quiet operation -v, --verbose verbose operation -y answer yes for all questions --version show Yum version and exit --installroot=[path] set install root --enablerepo=[repo] enable one or more repositories (wildcards allowed) --disablerepo=[repo] disable one or more repositories (wildcards allowed) -x [package], --exclude=[package] exclude package(s) by name or glob --disableexcludes=[repo] disable exclude from main, for a repo or for everything --obsoletes enable obsoletes processing during updates --noplugins disable Yum plugins --nogpgcheck disable gpg signature checking --disableplugin=[plugin] disable plugins by name --enableplugin=[plugin] enable plugins by name --skip-broken skip packages with depsolving problems --color=COLOR control whether color is used Plugin Options: アップデート可能なパッケージ一覧取得 $ yum check-update $ yum list updates  yumに関するデータを消去 $ yum clean  /headers / metadata / cache / dbcache / all  install済みパッケージの依存関係表示 $ yum deplist   ダウングレード $ yum downgrade   パッケージを削除 $ yum remove  $ yum erase   パッケージgroup一覧の表示 $ yum grouplist  パッケージgroup情報表示 $ yum groupinfo   パッケージgroupのインストール $ yum groupinstall   パッケージgroupのアップデート $ yum groupupdate   パッケージgroupの削除 $ yum groupremove   ヘルプ表示 $ yum help  パッケージ情報の表示 $ yum info   パッケージのinstall $ yum install   RPMパッケージ一覧を表示 $ yum list  install済みパッケージ一覧を表示 $ yum list installed  ローカルにあるパッケージをinstall $ yum localinstall   メタデータを生成 $ yum makecache  パッケージに含まれるファイル一覧表示 $ yum provides   再度install $ yum reinstall   repositoryの情報を表示 $ yum repolist  パッケージの依存関係を解消 $ yum resolvedep   指定文字列を含むパッケージの検索 $ yum search   パッケージのupdate $ yum update   install済みパッケージのupgradeを行う $ yum upgrade  コマンドalias設定 ※タイピングが面倒になりそうなコマンドはalias張って短くすると便利。</description>
    </item>
    
    <item>
      <title>AES暗号のまとめ</title>
      <link>https://yutakikuchi.github.io/post/201111100831/</link>
      <pubDate>Thu, 10 Nov 2011 08:31:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201111100831/</guid>
      <description>[PHP] : AES暗号のまとめ AESとは  共通鍵暗号方式の一つ。http://ja.wikipedia.org/wiki/AES暗号 共通鍵暗号方式にはブロック暗号とストリーム暗号が存在し、AESはブロック暗号方式。 ブロック暗号方式は固定長のブロックデータを単位として処理をする。 一般的にブロック暗号方式は公開鍵暗号方式に比べて高速と言われている。 ブロック暗号方式のモードはいくつか存在し、以下のページに詳しく載っている。http://www.triplefalcon.com/Lexicon/Encryption-Block-Mode-1.htm http://ja.wikipedia.org/wiki/暗号利用モード ブロックモードはCBC(暗号文ブロック連鎖モード)等が一般的で、CBCは前のブロックの暗号化された値を次のブロックの暗号化に利用する。最初のブロックの暗号を行う場合、最後のブロックかIV(Initial Vector:初期ベクトル)を使って暗号化する。 ブロック長は128bit。鍵長は128ビット、192ビット、256ビットの3つが利用できる。 脆弱性? http://www.computerworld.jp/topics/563/セキュリティ・マネジメント/200562/マイクロソフトの研究   PHPでAES(RIJNDAEL)  phpで利用するにはconfigureで--with-mcryptを指定する必要がある。→ centosではyum install php-mcryptを実行すればOK。 phpを--with-mcryptで指定するにはlibmcryptとmhashが必要。※なぜかmacでsudo port install libmcryptで設定するとうまくコンパイルできない。原因は調査中。 PHP: mcrypt_get_iv_size - Manual PHP: mcrypt_create_iv - Manual PHP: mcrypt_encrypt - Manual PHP: mcrypt_decrypt - Manual mcrypt-get-iv-size,mcrypt-create-iv,mcrypt-encrypt,mcrypt-decrypt関数を利用。 暗号化/複合化のサンプルプログラム  //暗号化方式をRIJNDAELの256bitブロック長、暗号化方式をCBC $iv_size = mcrypt_get_iv_size(MCRYPT_RIJNDAEL_256, MCRYPT_MODE_CBC); //Initial Vectorの大きさ echo &#34;iv size:&#34; . $iv_size . &#34;\n&#34;; //Initial Vector生成 $iv = mcrypt_create_iv($iv_size, MCRYPT_RAND); echo &#34;iv value:&#34;</description>
    </item>
    
    <item>
      <title>Apacheのチューニング</title>
      <link>https://yutakikuchi.github.io/post/201110180830/</link>
      <pubDate>Tue, 18 Oct 2011 08:30:58 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201110180830/</guid>
      <description>[Apache] : Apacheのチューニング Apacheをより有効的に利用するためのノウハウメモです。設定に用いたマシンはMacOSX、Apacheのバージョンは2.2としています。
http headers http://www.yahoo.co.jp/ GET / HTTP/1.1 Host: www.yahoo.co.jp User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows CE; IEMobile 8.12; MSIEMobile 6.5) KDDI-TS01 Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; KDDI-TS01; Windows Phone 6.5.3.5) Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: ja,en-us;q=0.7,en;q=0.3 Accept-Encoding: gzip, deflate Accept-Charset: Shift_JIS,utf-8;q=0.7,*;q=0.7 Connection: keep-alive Cookie: B=XXXXXXXXXXX HTTP/1.1 200 OK Date: Sun, 09 Oct 2011 03:16:08 GMT P3P: policyref=&#34;http://privacy.yahoo.co.jp/w3c/p3p.xml&#34;, CP=&#34;CAO DSP COR CUR ADM DEV TAI PSA PSD IVAi IVDi CONi TELo OTPi OUR DELi SAMi OTRi UNRi PUBi IND PHY ONL UNI PUR FIN COM NAV INT DEM CNT STA POL HEA PRE GOV&#34;</description>
    </item>
    
    <item>
      <title>Date/Timestamp変換のまとめ</title>
      <link>https://yutakikuchi.github.io/post/201110150812/</link>
      <pubDate>Sat, 15 Oct 2011 08:12:17 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201110150812/</guid>
      <description>[PHP] : Date/Timestamp変換のまとめ 2013/07/17追記 ※本記事の内容より詳しいものを書きました。
誰もが一度は陥る日付処理。各種プログラミング言語におけるDateTime型/TimeStamp型の変換方法のまとめ - Yuta.Kikuchiの日記 

 概要  Mysqlからアプリケーションで日付情報を引くときDate型Timestamp型の変換を行う事がしばしばある。 今日はPHP/Python/JavascriptのDate/Timestampの型変換についてまとめる。 Javascriptの例ではSpiderMonkeyを利用している。   Date型の日付を出力する php  //現在時刻をDate型で出力 //date関数を使う場合 echo date(&#34;Y-m-d H:i:s&#34;) . &#34;\n&#34;; //結果　2011-10-14 00:00:00 //strftime関数を使う場合 echo strftime(&#34;%Y-%m-%d %H:%M:%S&#34;) . &#34;\n&#34;; //結果　2011-10-14 00:00:00   python #! /usr/bin/env python # -*- coding:utf-8 -*- from datetime import * #現在時刻を表示 print datetime.now().strftime( &#39;%Y-%m-%d %H:%M:%S&#39; ) #結果 2011-10-14 00:00:00   Javascript //Dateオブジェクトを利用 var d = new Date(); var year = d.</description>
    </item>
    
    <item>
      <title>sudoでリダイレクトをしたいとき</title>
      <link>https://yutakikuchi.github.io/post/201110130127/</link>
      <pubDate>Thu, 13 Oct 2011 01:27:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201110130127/</guid>
      <description>[Shell] : sudoでリダイレクトをしたいとき sudoでリダイレクトに失敗  UNIX/Linuxでコマンドラインで実行した結果をファイルにリダイレクトしたい時があります。(例えばPHPのバッチを実行した時に処理のログとエラーログをファイルに書き込んでおきたい。) ユーザに実行権限がないディレクトリのファイルに対してリダイレクトでデータを書き込みたいとき、sudo をつけても失敗します。 次はリダイレクトが失敗する例です。zsh: permission deniedと怒られます。 $ sudo echo &#39;redirect&#39;  /var/test/hoge.txt zsh: permission denied: /var/test/hoge.txt  うまくいく方法 以下に方法を書きます。私が知る限りでは２つ方法があります。
回避策1 ： shのオプションに-cを付ける man sudoを見てみます。そうすると以下のように実行せよという記述がありました。-cとして実行したいコマンドを&#34;&#34;でくくるみたいです。
To make a usage listing of the directories in the /home partition. Note that this runs the commands in a sub-shell to make the cd and file redirection work. $ sudo sh -c &#34;cd /home ; du -s * | sort -rn  USAGE&#34;</description>
    </item>
    
    <item>
      <title>SpiderMonkeyでのコマンドラインJavascript</title>
      <link>https://yutakikuchi.github.io/post/201109041203/</link>
      <pubDate>Sun, 04 Sep 2011 12:03:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201109041203/</guid>
      <description>[javascript] : SpiderMonkeyでのコマンドラインJavascript Javascriptの実行はWebブラウザ上に限定されたものではない。ターミナル上でもスクリプト言語として実行可能だし、構文チェックも可能。今日はあまり知られていない非ブラウザでのJavascriptの話。SpiderMonkeyをターミナル上で動かすことを中心にまとめる。
SpiderMonkeyとは   世界初のJavascriptエンジンのコード名。 Mozilla Foundationによって保守運用。 C言語で開発され、インタプリタ、コンパイラ/逆コンパイラ、ガーベージコレクション、標準クラス群を提供。 LinuxやMacにinstallするとコマンドラインからJavascriptが操作可能。構文チェックも可能。    install  Macにinstallする方法を挙げる。以下2つのどちらかの方法を採用すれば良い。
※sourceからのbuildはmacの場合色々と設定が大変だったのでport installした。
1 portからinstallする これが一番簡単。
$ sudo port install spidermonkey  2 sourceをdownloadしてbuild ※以下は実行していないが念のためダウンロードとインストールについて記述する。
SpiderMonkey Build Documentation ここを参考に設定する。
Mac OS X Build Prerequisites 設定が結構大変。
autoconf213が必要なのでまずはそれを入れておく。
sudo port install autoconf213ソースの取得方法は次の通り。※murcurialからの取得が時間がかかる。
 tarballでの取得 $ wget http://ftp.mozilla.org/pub/mozilla.org/js/js185-1.0.0.tar.gz $ tar xzf js185-1.0.0.tar.gz  murcurialからの取得 $ hg clone http://hg.mozilla.org/mozilla-central/※ murcurialを操作するhgコマンドが無いと取得できないので設定されていなければportからinstallする。
sudo port install mercurial  CVSからの取得 $ cvs -d :pserver:anonymous@cvs-mirror.</description>
    </item>
    
    <item>
      <title>モバイルサイトを構築するための文字コード知識</title>
      <link>https://yutakikuchi.github.io/post/201108280833/</link>
      <pubDate>Sun, 28 Aug 2011 08:33:30 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201108280833/</guid>
      <description>[Web] : モバイルサイトを構築するための文字コード知識 はじめに 携帯キャリア向けのサイトを作っている時に必ず発生する文字コード問題(主に文字化けやByte数問題)。この記事では文字コード(文字集合、符号化)に対する内容についても触れつつ、モバイルサイト作りで注意すべき点について詳しく記述する。
 そもそも文字コードとは 以下では簡単に説明する。
 一つずつの文字を表すためのByte列表現であり、Byte列表現と文字の対応関係でもある。 別の言い方をすると文字コードとは各種文字についての符号の番号を独自の順序と計算式で示したByte列表現。 言語の文化と密接な関係を持っており、例えば英語圏、アジア圏でそれぞれ固有の文字コードが制定されるが、コンピュータ上で正常に表現できない言語も多い。 日本語を表現する有名な文字コードとしてはISO-2022-JP(JIS)、EUC-JP、Shift_JIS、UTF-8などである。 1バイト系文字コード(シングルバイト、半角文字)と2バイト系文字コード(マルチバイト、全角文字) 色々な文字コードが存在するため、システムで変換するときには様々な問題が起こる。変換には処理コストも掛かるし、対応が正確でないこともある。 Unicodeは上の互換性の問題に対応するために作られ、全ての言語を表現する。 文字コードは文字集合(coded character set)と符号化方式(character encoding scheme)という区別がされる。  文字集合JISX0208に対してISO-2022-JP、EUC-JP、Shift_JISなどの符号化方式が存在。 文字集合Unicodeに対してUTF-7、UTF-8、UTF-16などが存在する。    各キャリアサイトの文字コード 各キャリアの技術サイトを見てみる。
 DoCoMo imode browser html DoCoMo imode browser xhtml au SoftBank willcom emobile マトリックスでまとめると以下のようになる。
   キャリア   Shift_JIS   ISO-2022-JP   EUC-JP   UTF-8   備考     DoCoMo   ○   ×   ×   ○   htmlだとUTF-8が使用不可     Au   ○   ×   ×   ×   formのデータ送信がページ文字コードによらずShift_JISになる     SoftBank   ○   ○   ○   △   UTF-8のサポートは機種による     Willcom   ○   ○   ○   △   JRCの端末は一部UTF-8未対応     emobile   ○   ○   ○   ○       スマートフォン   ○   ○   ○   ○   ほとんどPCと同じだが、タグの指定とOSのバージョンで一部文字化けすることも     この結果からキャリアのサイトを作る場合はShift_JISで作る事が望ましい。UTF-8は完全に対応できてはいない。DoCoMo,Auの制約が厳しい。 スマートフォンの文字コード制約は特にないので適切なサーバサイドから渡す文字コードとクライアントのContent-Typeの設定を一致させる(UTF-8が一般的か) xhtmlのContent-Type指定のサンプルは次のように設定。 xml version=&#34;</description>
    </item>
    
    <item>
      <title>Pythonでbase64エンコード・デコードする方法</title>
      <link>https://yutakikuchi.github.io/post/201108250824/</link>
      <pubDate>Thu, 25 Aug 2011 08:24:18 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201108250824/</guid>
      <description>[Python] : Pythonでbase64エンコード・デコードする方法 import base64でできる バイナリデータをASCII文字列化する手段の一つであるbase64encode・decodeをpythonでやる方法。import base64によりすごく簡単にできる。エンコード:base64.b64encod、decode:base64.b64decode。Paddingには&#39;=&#39;が使用される様子。以下はサンプルサンプルコードでバイナリ形式のimageファイルを読み込み、base64encodeを行い、encodeした文字列からdecode関数で元ファイルのバイナリと同じであるかどうかを確認している。
#! /usr/bin/env python # -*- coding:utf-8 -*- import base64 #file読み込み file = open(&#39;./image.jpg&#39;, &#39;rt&#39;).read() #base64でencode enc_file = base64.b64encode( file ) #encodeしたascii文字列を出力 print enc_file #decodeしてもとデータに変換 dec_file = base64.b64decode( enc_file ) #decodeしたデータと元データを比較 if file == dec_file : print &#39;SAME&#39; else : print &#39;NOT SAME&#39; encode結果(一部)
VQOeSpPIrtIP2jv2f59HklHxv+Eu2QCVI5/F1jGy55IwZMgg9iM1/K9iis4YSKd7jeLk1sf1QTftEfAWeymB+N/wfWRk5P8AwmNhkEjt+9/OstPj/wDAUps/4Xb8I41+YqD4xsQME5wcS9e+PWv5cqKp4WL6k/WZH9REH7QPwGtlljl+NXwmuELlkceMLDPfr++57dajHx8+BlxbO5+NXweiLJgA+M7EHqDn/W8EZNfy90VDwUL7l/W5H9NU/wAefgoJ0D  urlsafeをサポート urlにbase64encodeした文字列を含めたい時に有用。なぜならば&#39;/&#39;,&#39;+&#39;はurlのパスやquery文字列に使用されるので変換が必要となる。pythonの場合は&#39;/&#39;を&#39;_&#39;、&#39;+&#39;を&#39;-&#39;に変換するurlsafe形式を組み込み関数でサポートしている。※他の言語だとdecode/encodeの前後で文字列を自前で変換する。encode:base64.urlsafe_b64encode、decode:base64.urlsafe_b64decodeを利用する。以下はサンプルコード。
#! /usr/bin/env python # -*- coding:utf-8 -*- import base64 #file読み込み file = open(&#39;./image.jpg&#39;, &#39;rt&#39;).read() #base64でencode enc_file = base64.</description>
    </item>
    
    <item>
      <title>Android端末のOpenSSL経由でファイルダウンロードができない問題</title>
      <link>https://yutakikuchi.github.io/post/201108200857/</link>
      <pubDate>Sat, 20 Aug 2011 08:57:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201108200857/</guid>
      <description>[Android] : Android端末のOpenSSL経由でファイルダウンロードができない問題 問題詳細 OpenSSLを使って立ちあげたサーバに設置したファイルに対してAndroidのブラウザからアクセスしても全くダウンロードが始まらない現象が起きていた。サーバのレスポンスとしてのContent-Typeやその他headerについても確認しても特に問題なさそうだったので、試しにhttpでアクセスしてみたらうまくダウンロードできた。httpsの何が原因なのかが分からないが、androidはオレオレ認証を認めてくれないようだ。別にOpenSSLぐらいのアクセスぐらい認めてくれてもいいのに、iPhoneは例外承認すればダウンロードさせてくれますよ。時間があればAndroidブラウザソースを追いたいと思ったが、既にIssueTicketが発行されていたので、そのリンクと内容を張っておく。
Issue 3492
※原文の一部を下に引用
 I tried to download a file from a password protected page (AuthType Basic),
which is accessed over https. Accessing the page itself with the browser
works without problems. But when I try to download files from the page, the
Download Manager shortly displays &#34;Starting download...&#34; and the switches
to and keeps saying &#34;Waiting for data connection...&#34;. The download is never</description>
    </item>
    
    <item>
      <title>MongoDBをpythonから利用する</title>
      <link>https://yutakikuchi.github.io/post/201108190844/</link>
      <pubDate>Fri, 19 Aug 2011 08:44:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201108190844/</guid>
      <description>[Python] : MongoDBをpythonから利用する MongoDB 特徴  アメリカの10gen社によってC++で書かれたドキュメント指向データベース。 DBへのアクセスはJavascriptによって行う。 ドキュメント指向データベースとは、自由なデータ構造のドキュメントを登録する事が可能。 MongoDBのデータベースはそれぞれ独立している。 テーブルの構造を事前に決めないのでスキーマレス。 １つのデータベースには1つ以上のコレクションとコレクションにはドキュメント(オブジェクト)から成り立つ。 コレクションとはドキュメントのグループ。RDBMSのテーブルに相当。 ドキュメントとは登録データ(オブジェクト)。 RDBMSにオブジェクトを登録することはプログラムが複雑化するが、ドキュメント指向データベースならオブジェクトをそのまま保存。 ドキュメント指向データベースに他にCouchDBがある。 RDMBSと比較して簡単にスケールアウトできる仕組み。 BSONというバイナリ形式のデータを内部で保存。BSONとはJSONを元にしたKey-Value型のデータ構造。 BSONのStringCodeはUTF-8。 indexの設定が可能。 データを複数のデータベースに分割するShardingが可能。 JOIN機能が無いのでテーブルの結合などの必要がある場合はRDBMSを利用する。   BSONのデータ構造  MongoDBの各ドキュメントは_idというuniq keyを保持。 idはBSONのobject idでといった12Byteの値。 idはコレクション内でユニークであり、コレクションがindexを持つ場合はユニークが強制される。 ユーザ自身で_idを指定する事も可能だが、指定しなかった場合は自動で付与される。 idは12バイトの値でほとんどのケースでユニークになるようにデザイン。 MongoDBのシェルとして提供されているObjectId()はidを生成するために使う。Object( &#39;e4d1a14651d0f018e0000004&#39; )のように使い16進数文字列から生成。 idは4バイトのタイムスタンプ(epoch秒)、3バイトのマシーンID、2バイトのプロセスID、3バイトのカウンターで構成され、計12Byte。 タイムスタンプカウンターフィールドはbig endianで格納。バイト毎に比較した時に必ず昇順になることを保証。   install macOSXに対しての手順を記述する。
1. port install port installは時間が掛かりすぎてやめた。
 $ sudo port install mongodb
 1. download and install Downloads - MongoDB
 $ tar xzf mongodb-osx-x86_64-1.8.2.tar
$ sudo cp -R mongodb-osx-x86_64-1.</description>
    </item>
    
    <item>
      <title>各キャリアのメール添付のまとめ</title>
      <link>https://yutakikuchi.github.io/post/201108141230/</link>
      <pubDate>Sun, 14 Aug 2011 12:30:49 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201108141230/</guid>
      <description>[Web] : 各キャリアのメール添付のまとめ 送信    サービス   可能容量   最大ファイル数   ドキュメント     DoCoMo SPモード   10M   -   http://www.nttdocomo.co.jp/binary/pdf/service/provider/spmode/guide/sp.pdf     DoCoMo iモード   100K - 2M   10   http://www.nttdocomo.co.jp/service/communication/imode_mail/function/ishot/datasize/index.html     EzWeb Eメール   100K - 2M   5  http://www.au.kddi.com/service/email/tsukaikata/kino/index.html     SoftBank SMS   2M   5  http://mb.</description>
    </item>
    
    <item>
      <title> jQuery Proven Performance Tips And Tricks (翻訳)</title>
      <link>https://yutakikuchi.github.io/post/201107311431/</link>
      <pubDate>Sun, 31 Jul 2011 14:31:18 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201107311431/</guid>
      <description>[javascript] : jQuery Proven Performance Tips And Tricks (翻訳) http://addyosmani.com/jqprovenperformance/
このサイトに書いてある内容をまとめます。英語は苦手なんで適当な意訳が多いです。
Performancetip1 バージョンのお話   可能な限り常に最新バージョンを利用する。 アップデートする前にはかならずregression testを行う事。 最新のバージョンは1.6.2や1.7で、この秋にリリースされる予定。 twitterなどもjquery1.3.0を利用している。 1.6はパフォーマンス改善や 新しい特徴が組み込まれている。 1.6.2はバグのパッチやregressionが修正される。 古いバージョンではこれらお手軽なパフォーマンスを受けることができない。 47%の有名なサイトがJqueryを使っているが、バージョン変更にテストが大変。 アップデートは通常、大変ではないプロセスである。 1.4.2、1.4.4と1.6.2のSelectorを比較してもパフォーマンスの違いは明らか。 .attr( &#39;value&#39; ), .val()(get)なども1.6でパフォーマンスが改善されている。 しかし特定のselectorは1.4.xより遅くなっている。 使っているselectorのパフォーマンスに注意すれば、大丈夫。    Performancetip2 Selector   全てのSelectorは平等に作られない。 Selectorの生成方法は沢山あるし、それぞれのSelectorは同一のパフォーマンスを出さない。 最速と最遅いSelectorがどれか知っていますか？ IDとElementのSelectorは最速。$( &#39;#Element, form, input&#39; )  IDとElementでの指定は裏側でnativeDOMの操作を利用している。(getElementById())  Class Selectorは遅い。#( &#39;.element&#39; )  getElementsByClassNameはIE5-8ではサポートしていない Firefox3、Safari4、Chrome4、Opera10.10以上はこれは速い。  最も遅いのは正しくない指定とattributeでのselector。$( &#39;:visible, :hidden&#39; ); $( &#39;[attribute=value]&#39; );  上のselectorはnative関数を呼び出す事ができない。 最新のブラウザーにはquerySelector()やquerySelectorAll()などサポートされている。  querySelectorAll()はCSSベースでのSelectorとなる要素をDOMとして検索できる。  JQueryはquerySelectorAll()の利用を試みている。 querySelectorAllを使うSelectorの最適化と:first,:last,:eq等ではないSelectorの最適化との比較。 有効なSelectorこれをより利用する機会がある。  曖昧なselectorは有効ではあるが、使う時には注意が必要。 :hiddenを指定した要素が100個あったとき、JQueryは100個のアイテムをコールする。 :hiddenは有効であるが全ての曖昧な指定は全ての要素に対して実行を試みる。 できれば上の事は避けたい。 jQuery1.</description>
    </item>
    
    <item>
      <title>Vimの置換方法まとめ</title>
      <link>https://yutakikuchi.github.io/post/201107232055/</link>
      <pubDate>Sat, 23 Jul 2011 20:55:39 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201107232055/</guid>
      <description>[vim] : Vimの置換方法まとめ Vimの標準機能が素晴らしすぎて泣けてくる。編集ファイル内部の置換はまだしもファイル外の置換までできるとは。忘れないようにメモをしておこう。
ファイル内置換 Vimで編集中のファイルにたいしての置換は次の通り。
&#34;ESC&#34;でコマンドモードに切り替えた状態からの説明。
指定ルール    説明   ルール     カーソル行の最初の文字だけ置換   :s///     カールル行の全ての文字を置換   :s///g     全ての行の最初の文字だけを置換   :%s///     全ての行の全ての文字を置換   :%s///g     置換確認モード   :%s///gc     指定行数内の全ての文字を置換   :,s///g     正規表現でマッチした文字を\1で後方参照置換   :s/\(正規表現\)/\1/g   ※上のスラッシュ(/)をセミコロンで置き換えてもOK。例えばファイルパス置換の時はバックスラッシュでエスケープしないといけないがセミコロン指定では不要になる。</description>
    </item>
    
    <item>
      <title>5分で分かるCakePHPの基礎</title>
      <link>https://yutakikuchi.github.io/post/201107031259/</link>
      <pubDate>Sun, 03 Jul 2011 12:59:35 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201107031259/</guid>
      <description>[PHP] : 5分で分かるCakePHPの基礎  PHPのFWを比較することを試みようと思っている。PHPの4大FWと言われる、CakePHP,Symphoney,Zend,Codeigniterは少しずつ勉強を進めよう。まずは軽量かつ覚えやすいというCakePHPについて記述する。※コードの追跡を行ったのはCakePHP version1.3.9。現時点での最新Version。
 特徴や規則   RoRの影響を受けて作られている。2006年頃から存在し、PHP４でも使える。 MVC(Model View Controller)。Viewを純粋なPHPでかける。 習得が比較的楽。 ViewHelperが利用可能。ViewHelperで自動的に出力をsanitize。 Action/FilterChainをサポート。 Validatorサポート。 関数の短縮表記。 簡単な処理の流れを説明すると、entrypointからDispatcherが呼び出すControllerを決定し、ControllerはModelとデータのやり取りをして取得結果を整形してViewに渡す　という構成。 ファイル名の設定にはアンダースコア、クラス名の設定にはキャメル記法を用いる。(例)my_sample_class.phpというファイル名にMySampleClassというクラスを定義する事ができる。 ファイルやDBの命名規則が厳格。 Controllerクラスの中でメソッド名の接頭辞としてアンダースコアを付けるとURLからはアクセス不可能なメソッドとして扱う事ができる。 Modelクラスの定義もキャメル記法。呼び出されるdbのテーブル名はアンダースコア表記。名前は複数形とする。 呼び出し方は http://Cakeのパス/コントローラ/メソッド(アクション)/パラメータ。内部のrewrite処理によりapp/webroot/index.phpが起動されコントローラ、メソッド、パラメータを解釈。 キャッシュ機構も備えている。(apc,memcache,xcache)。Viewのキャッシュも可能。    Directory Tree  主要なフォルダ構成は次の通り
 app : 作成したアプリケーションを設置する場所  config: DB接続やbootstrap、coreの設定ファイルを入れる controllers : controllerとcomponentを入れる libs : ファーストパーティ用のライブラリ locale : 国際化のための文字ファイル models : models,behaviors,datasourcesを入れる plugins : pluginパッケージを入れる tests : testケースのコードを入れる tmp : ログ、セッション情報、データの一時保存場所 vendors : サードパーティ用のライブラリ views : 表示用のファイルを設置 webroot : アプリケーションのエントリポイントなどのドキュメントルート  cake : coreライブラリ。中身の変更は禁止。 vendors : 作成したライブラリを設置できる。    処理の流れ詳細   URLとしてのエントリポイント呼び出し(app/webroot/index.</description>
    </item>
    
    <item>
      <title>Mysqlのユーザ/権限管理</title>
      <link>https://yutakikuchi.github.io/post/201106091011/</link>
      <pubDate>Thu, 09 Jun 2011 10:11:05 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201106091011/</guid>
      <description>[Mysql] : Mysqlのユーザ/権限管理 GRANT文でユーザ設定をする!  GRANT構文はユーザの追加とユーザに対して与える権限の指定や接続可能元のhost名やパスワードの設定が可能。 GRANT文実行後はFLUSH PRIVILEGES;の実行により権限テーブルの再読み込みを行い、権限データを反映させる。 権限の範囲は全てのDB、特定DB内部の全てのテーブル、特定テーブルの全てのカラム、指定テーブルの特定テーブルの計4種類。  構文 GRANT 付与する権限の種類[(カラムリスト)] [,付与する権限の種類[(カラムリスト) …] ON データベース名.テーブル名 TO ユーザー名@ホスト名 [,ユーザー名@ホスト名 …] [IDENTIFIED BY [パスワード]] [WITH [GRANT_OPTION | MAX_QUERIES_PER_HOUR # | MAX_UPDATES_PER_HOUR # | MAX_CONNECTIONS_PER_HOUR #]]  全データベースに何でもできる権限を与える。 sample
 googleユーザを作成。 googleユーザに対して全ての権限(ALL PRIVILEGES)を付与。 権限の範囲は全てのDB(*.*)。 localhostからの接続を許可する。 パスワードは&#39;all&#39;として設定。 mysql GRANT ALL PRIVILEGES ON *.* TO google@localhost IDENTIFIED BY &#39;all&#39;; mysql FLUSH PRIVILEGES;   特定のデータベースにのみ特定の権限を与える。 sample
 yahooユーザを作成。 yahooユーザに対して特定の権限を付与(SELECT,INSERT)。 権限の範囲はtestDB内の全てのテーブル(test.*)。 127.0.0.1というIPからの接続を許可する。 パスワードは&#39;db&#39;として設定。 mysql GRANT SELECT,INSERT ON test.</description>
    </item>
    
    <item>
      <title>注目各社のTechBlogまとめ</title>
      <link>https://yutakikuchi.github.io/post/201106051924/</link>
      <pubDate>Sun, 05 Jun 2011 19:24:10 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201106051924/</guid>
      <description>[blog] : 注目各社のTechBlogまとめ 主力ビジネスごとに各社の技術ブログをカテゴリ化しました。RSSでまとめて購読しておくといいかもしれません。個人的にはソーシャル系のDeNA、Gree、mixi、klabのブログが熱いかと思っています。
 検索/ポータル 10 ソーシャル系 9 その他 9  検索/ポータル
Google Japan http://googlejapan.blogspot.com/
 Yahoo! Japan http://techblog.yahoo.co.jp/
 Naver Japan http://naverland.naver.jp/
 Baidu Japan http://staffblog.baidu.jp/
 Bing Japan http://mssearchjp.wordpress.com/
 livedoor http://blog.livedoor.jp/techblog/
 Hatena http://d.hatena.ne.jp/hatenatech/
 CookPad http://techlife.cookpad.com/
 Recruit http://mtl.recruit.co.jp/
 ECNavi http://tech.ecnavi.co.jp/

  ソーシャル系 Gree http://labs.gree.jp/blog/
 mixi http://alpha.mixi.co.jp/blog/
 DeNA http://engineer.dena.jp/
 twitter http://blog.jp.twitter.com/
 Zynga Japan http://labs.unoh.net/
 ドワンゴ http://info.dwango.co.jp/rd/</description>
    </item>
    
    <item>
      <title>WebSocket対応状況のまとめ</title>
      <link>https://yutakikuchi.github.io/post/201106020845/</link>
      <pubDate>Thu, 02 Jun 2011 08:45:10 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201106020845/</guid>
      <description>[programming] : WebSocket対応状況のまとめ 概要  Node.jsでWebSocketを試してみる - Yuta.Kikuchiの日記 
以前Node.jsを使ったWebSocketの導入についてまとめたので、今回はブラウザ、サーバ言語の対応状況についてまとめてみる。
  WebSocketプロトコルの種類   draft-hixie-thewebsocketprotocol-75 draft-hixie-thewebsocketprotocol-76 draft-ietf-hybi-thewebsocketprotocol-00 draft-ietf-hybi-thewebsocketprotocol-06 等
元々はdraft75,76がメインだったようで二つが混在していた。draft-ietf-hybi-thewebsocketprotocol-03、それ以前のプロトコルにセキュリティホールが発見される。
http://gihyo.jp/dev/feature/01/websocket/0003
  仕様策定状況   HTML5の仕様からは分離。 各種ブラウザもセキュリティの状況を見ての対応を進めている様子。 websocketの仕様は2011/5月中に策定完了を目標に動いていた。    対応ブラウザ   プロトコル   ブラウザ       draft-hixie-thewebsocketprotocol-75   Google Chrome4 / Safari5.0.0     draft-hixie-thewebsocketprotocol-76 draft-ietf-hybi-thewebsocketprotocol-00   Google Chrome6 / Safari5.0.1     draft-ietf-hybi-thewebsocketprotocol-06   IE HTML5Labs   基本的にはGoogle ChromeとSafariしか対応していない。FF/Operaはデフォルト仕様から外している。 (2011/12/24追記)</description>
    </item>
    
    <item>
      <title>Node.jsでWebSocketを試してみる</title>
      <link>https://yutakikuchi.github.io/post/201105310830/</link>
      <pubDate>Tue, 31 May 2011 08:30:31 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201105310830/</guid>
      <description>[javascript] : Node.jsでWebSocketを試してみる Nodejs   サーバサイドJavascript。 V8 Javascriptを利用。 シングルスレッドの非同期処理環境。 処理を待たずにcallbackを実行するイベントループ、ノンブロッキングI/Oを実装。 nodejsの設定は簡単。パッケージ化されているし、buildしてもそれほど時間がかからない。    設定  macでinstallしてみる。以下のどちらか一方を行えば設定は可能だがportのversionは0.2.0、buildの最新は0.4.8。post installは少し時間がかかる。
port install $ sudo port install nodejs $ node -v v0.2.0  make install $ fetch http://nodejs.org/dist/node-v0.4.8.tar.gz $ tar -xzf node-v0.4.8.tar.gz $ cd node-v0.4.8 $ ./configure $ sudo make install $ ./node -v v0.4.8    実行  日本語ドキュメントにあるサンプルをそのまま実行してみる。サンプルはHttp,Socketサーバ。以下のそれぞれのJavascriptっぽいコードをsample.jsなどのファイルとして保存してnodeコマンドにより実行する。
Http スクリプト
var http = require(&#39;http&#39;); http.createServer(function (req, res) { res.</description>
    </item>
    
    <item>
      <title>ackコマンド</title>
      <link>https://yutakikuchi.github.io/post/201105230950/</link>
      <pubDate>Mon, 23 May 2011 09:50:55 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201105230950/</guid>
      <description>[Linux] : ackコマンド 目次   概要 設定  port install コマンドを作る cpan install 設定ファイル  コマンドの実例  通常検索 検索対象から外す 全部検索 Pager指定 ファイルタイプ指定 大小文字区別なし ファイル検索 ファイルパス名の正規表現指定 ファイルパス+ファイル名の正規表現指定 完全一致検索 指定単語の前後行出力 一致したファイル名を出力 再起的に検索しない group指定を外す 検索対象から特定ディレクトリを外す 猫キャラ表示 デフォルト指定?     概要   ackコマンド  ソースコード検索はgrepが有名だが、より強力なツールと言われるackを試してみる。 grepより処理が早い。(設定ファイルなど無視) grepより文法が短くて済む。 grepと文法が似ている。 grepの機能がほとんど使えて、findの要素も持つ。 Perlで書かれていてwindowsでも動く。 Perlの正規表現が使える。     設定  macで試してみる。以下3つのうちどちらかの手順で設定が行える。
1. port install 1行コマンドを実行するだけ。
sudo port install p5-app-ack  2.コマンド作る 自分のローカルフォルダに設置。PATHの設定も必要。</description>
    </item>
    
    <item>
      <title>超簡単! ボタン一つで登録可能なソーシャルメディアShareBookmarklet</title>
      <link>https://yutakikuchi.github.io/post/201105081014/</link>
      <pubDate>Sun, 08 May 2011 10:14:35 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201105081014/</guid>
      <description>[javascript] : 超簡単! ボタン一つで登録可能なソーシャルメディアShareBookmarklet 
概要   上記アイコンのWebサービスに対してクリックや画面タッチで簡単にシェアができるBookmarkletを作成しました。  Webページには各ソーシャルサービスに対するシェアボタン/bookmarkボタンが無い場合が多い。シェアボタン/bookmarkボタンが無いとユーザは自分で各サービスのページに遷移し、内容を登録しなければならないが、それはとても面倒である。 上記アイコンのサービスはそれぞれでシェア用のbookmarkletを作成している場合もあるが、サービスごとのbookmarklet管理はユーザの負担。 addthisというマルチシェアサービスには多サービスへの展開可能なbookmarkletが存在するが、アイコンが小さく使いづらいのとsmartphone非対応。     設定   以下のいずれかの方法でbookmark登録してください。※対応しているブラウザはページ下部で確認してください。  次のリンク先に遷移し、ShareBookmarkletというものを登録します。bookmarklet一覧 次のbookmarkletスクリプトを直接登録します。  javascript:void((function(){var%20s=document.createElement(&#39;script&#39;);s.setAttribute(&#39;src&#39;,&#39;http://mobiles-proxy.appspot.com/statics/js/multishare.js&#39;);s.setAttribute(&#39;id&#39;,&#39;multishare&#39;);document.body.appendChild(s)})())     スマートフォンで利用したい人向けにbookmarkletスクリプトのQRCodeを張っておきます。スマートフォンで読み取ったデータをbookmarkとして保存してください。スマートフォンでの登録方法は次のページが参考になると思います。http://ascii.jp/elem/000/000/150/150144/  
  動作イメージ   閲覧しているWebページ上で登録したbookmarkletをクリックするとアイコンメニューがページ左に表示されます。 
 各サービスアイコンのどれかをクリックします。下の画像はtwitterの例です。クリックするとtwitterの画面に遷移し、タイトルと短縮URLがつぶやきのエリアに入ります。その他のソーシャルサービスでも同じ様にbookmarkやshareが簡単にできます。 
 iPhoneでも同様の事が可能です。twitterのボタンをクリックするとmobileのtwitterページに遷移します。後は上の例と同じです。 
  シェア対象サービス  以下のサービスにシェアすることを可能としています。
 hatena bookmark twitter facebook livedoor clip delicious google bookmark yahoo bookmark    動作確認  以下のブラウザで動作確認をしています。※android端末は現在動作確認中です。</description>
    </item>
    
    <item>
      <title>jQueryの参考にすべきSiteのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201105051042/</link>
      <pubDate>Thu, 05 May 2011 10:42:44 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201105051042/</guid>
      <description>[javascript] : jQueryの参考にすべきSiteのまとめ 基礎   jQuery日本語リファレンス http://semooh.jp/jquery/  文法チェックに誰もが見るサイト。  jQuery速習講座 http://ascii.jp/elem/000/000/498/498710/  selector、命令、イベント、アニメーション、UIについての解説。初心者にとっては分かりやすい。  jQuery入門 http://www.openspc2.org/JavaScript/Ajax/jQuery_study/ver1.3.1/index.html  jQueryの基礎的な内容についてのまとめ。  一日で学ぶjQuery http://blog.spicebox.jp/labs/2009/04/_jquery.html  jQueryについてサンプルをまとめたサイト。     UI   jQueryサンプル集 http://www.designwalker.com/2008/04/jquery.html  画像の処理のサンプル集  jQuery UI 実践サンプル http://jsajax.com/jQueryUI.aspx  Interactions、Widgets、Effectsを紹介。シンプルな例が多い。  150 best jQuery effects for web designers and developers http://www.webdesignshock.com/showcase/best-jquery-effects/  アニメーションからインタフェースまで充実  jQuery+CSSで実装するナビゲーションメニュー総集編 http://kachibito.net/web-design/css-menu-with-jquery-collection.html  menu中心の内容     Plugin   jQueryList http://jquerylist.</description>
    </item>
    
    <item>
      <title>MYSQL INDEXのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201104180831/</link>
      <pubDate>Mon, 18 Apr 2011 08:31:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201104180831/</guid>
      <description>[Mysql] : MYSQL INDEXのまとめ 概要   大規模なデータを管理するためのMYSQL-INDEXについて必要な情報をまとめてみます    PRIMARYKEY / UNIQKEY / INDEXについて   PRIMARYKEYとはそのテーブル内において重複が許されないもので、自動的にINDEXが張られる。 UNIQKEYとはそのテーブル内に置いて重複を許さない。ただし、NOT NULLにしなければNULLの重複は認める。 INDEXとは特定の値を持つレコードを高速に検索するための木構造データ。INDEXを張らないとテーブル全体のデータを検索してしまう。最適化されたINDEXを利用するとテーブルデータを全く参照せずにデータを返却できる。 まとめるとPRIMARYKEY = UNIQKEY + INDEX    複合INDEXについて   複数のカラムに対してのINDEXを作成する事。単一のINDEXより高速な検索ができる。 複合INDEXを利用する場合はカラム順番が重要。例えば複合INDEXの第一KEYがカラムA,第二KEYがカラムBとなっている時、検索条件にAが含まれない場合は当然ながらINDEXは使用されない。これは複合INDEXの数が3つ以上の場合にも同じ事が言え、A,B,Cの順番の複合INDEXの場合、A,Cしか検索条件に含まれない場合はAを指定した時のみのパフォーマンスと同等になる。    部分INDEX   INDEXのデータ容量が気になる場合は部分INDEXを用いることができる。たとえば文字列の前から数ByteまでをINDEXとして利用するなど。 部分INDEXの場合、部分一致した複数のレコードが一致してしまう可能性がある。    INDEXのデータ構造  INDEXの木構造にもいくつか種類がある。InnoDBではBtree,MyISAMでもデフォルトではBtreeを採用。
 BtreeINDEX : 一般的なINDEX構造。データのバランスが取れている木構造。 HashINDEX : Hashを利用したINDEX構造 RtreeINDEX : 幾何データ型専用のINDEX構造    INDEXが使われるケース  参考:http://slashdot.jp/journal.pl?op=display&amp;uid=4&amp;id=26710
 フィールド値を定数と比較するとき (where name = &#39;hogehoge&#39;) フィールド値でJOINするとき (where a.</description>
    </item>
    
    <item>
      <title>Rewrite/Redirect</title>
      <link>https://yutakikuchi.github.io/post/201104050813/</link>
      <pubDate>Tue, 05 Apr 2011 08:13:20 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201104050813/</guid>
      <description>[Apache] : Rewrite/Redirect RewriteとRedirectの違い   Rewrite:URLの書き換えが発生しない Redirect:URLの書き換えが発生する    設定ファイル  mod_rewriteの仕組みを利用します。
Linux(CentOS)の場合設定ファイルは以下の次のパスになる。/etc/httpd/conf/httpd.conf
今回は上のhttpd.confを少し書き換えて、rewrite/redirectの設定ファイルをincludeさせることを試みる。
httpd.confの最後の行に以下を追記する。Include conf/rewrite.conf
rewrite.confに色々とリダイレクトの記述をして、試してみる。
  RewriteEngine,RewriteCond,RewriteRule  apacheの設定ファイル(httpd.conf、今回はrewrite.conf)にRewriteEngine On,RewriteCond,RewriteRuleといった記述をすることでリダイレクト/リライトを実現する。
 RewriteEngine On : Rewriteを適用しますという宣言。 RewriteCond : Rewriteを行う条件を指定する。たとえばUserAgentがFireFoxだったら以下のRewriteRuleを適用せよといった指定など。 RewriteRule : Rewriteの適用ルールを記述する。たとえば特定のURIへのアクセスは全部一律で別のURIに置き換えるといった指定など。    Rewrite変数  以下はRewriteCond,RewriteRuleで適用できる変数一覧です。
HTTP headers:
HTTP_USER_AGENT
HTTP_REFERER
HTTP_COOKIE
HTTP_FORWARDED
HTTP_HOST
HTTP_PROXY_CONNECTION
HTTP_ACCEPT
connection &amp; request:
REMOTE_ADDR
REMOTE_HOST
REMOTE_USER
REMOTE_IDENT
REQUEST_METHOD
SCRIPT_FILENAME
PATH_INFO
QUERY_STRING
AUTH_TYPE
server internals:
DOCUMENT_ROOT
SERVER_ADMIN
SERVER_NAME
SERVER_PORT</description>
    </item>
    
    <item>
      <title>Mobile-UserAgent検索システム</title>
      <link>https://yutakikuchi.github.io/post/201103311334/</link>
      <pubDate>Thu, 31 Mar 2011 13:34:01 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201103311334/</guid>
      <description>[Mobile] : Mobile-UserAgent検索システム 概要   Mobileサイト開発者向けのUserAgentの検索、一覧表示、useragentswitcher用のxml-dumpが可能です。 モバイル端末(SmartPhone含む)をサポートしています。 キャリアの公式サイトなどを自動的にクローリングしてデータを生成しています。 一覧取得のWebAPIを作成中です。 Google App Engine(Python)で作成しています。    URL  index http://mobile-ua.appspot.com/
 search http://mobile-ua.appspot.com/search
 all list http://mobile-ua.appspot.com/list
 dump for useragentswitcher http://mobile-ua.appspot.com/download
   データ更新   基本的にレスポンスデータの更新は１日１回です。 キャリアの公式/UserAgentのまとめサイトをスクレイプしています。 WillcomはUserAgentの掲載がPDFなので手動で更新します。 SmartPhoneのUserAgentが各キャリアサイトに載っていないので、随時メーカーサイトをwatchして開発者が手動で更新をかけます。 このブログのコメント投稿に皆さんから追加してほしいUserAgentを載せてもらい、それを開発者が随時確認して手動で更新という運用フローにしたいと思います。    開発者めも  DBモデル設計    項目   型   データ内容     Carrier   String   DoCoMo/EzWeb/SoftBank/WillCom/emobile     Model   String   端末のモデル名 001SHなど     Type   String   Browser/JAR/Appli(DoCoMo) Browser/Appli/Widget/Flash/(SoftBank)     UserAgent   String   SoftBank/2.</description>
    </item>
    
    <item>
      <title>ShellScriptのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201103241438/</link>
      <pubDate>Thu, 24 Mar 2011 14:38:53 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201103241438/</guid>
      <description>[Shell] : ShellScriptのまとめ 概要  ShellScriptについて簡単にまとめます。
私はzsh使いですが、この記事ではshを中心に書いています。
  Shellの種類と特徴     Shell名   特徴      sh 最初のシェル（Bourneシェル）     csh C言語の文法に似たスクリプト用いることができる     bash Bourneシェルを拡張したもの（Bourne Again Shell）     tcsh cshを拡張したもの     ksh 商用UNIX(SystemV)の標準シェル     zsh 最強のシェル(bashやtcshの機能を併せ持つ)       基本コマンド     コマンド   説明      Ctrl+H カーソルの左の文字を一文字削除（BSキー）     Ctrl+B カーソルを左に移動     Ctrl+F カーソルを右に移動     Ctrl+D カーソル上の文字を一文字削除（Dellキー）     Ctrl+K カーソルから右側の文字をすべて削除     Ctrl+S 画面への出力を一時停止     Ctrl+Q 画面への出力を再開     Ctrl+L 画面をクリアー     Ctrl+A 行の先頭にカーソルを移動     Ctrl+E 行の最後にカーソルを移動     Ctrl+C プログラムの処理を中止     Ctrl+Z プログラムの処理を一時停止     Ctrl+P  履歴をひとつ前に遡る      Ctrl+N 履歴を逆順の遡る     !</description>
    </item>
    
    <item>
      <title>Javascriptによる正規表現まとめ</title>
      <link>https://yutakikuchi.github.io/post/201103051845/</link>
      <pubDate>Sat, 05 Mar 2011 18:45:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201103051845/</guid>
      <description>[javascript] : Javascriptによる正規表現まとめ Index  JavaScriptで利用できる正規表現についてまとめました。
最後の後読み、否定後読み以外は利用可能かと思います。
 モード修飾子(i,m,g) \n,\r,\u2028,\u2029以外の任意の1文字 空白文字以外( \S ) 数字( \d ) 数字以外( \D ) 先頭( ^ )  末尾( $ )  単語区切り( \b ) 単語区切り以外( \B ) 0以上の繰り返し( * )  1以上の繰り返し( + )  0または1回( ? )  最短マッチ( .*? ) nの繰り返し( {n} ) n以上の繰り返し( {n,} ) n以上,m以下の繰り返し( {n,m} ) いずれかの文字( [▲◎■] ) いずれかのパターン( (▲|◎|■) ) 後方参照( \n ) エスケープ( \ )  先読み( ?</description>
    </item>
    
    <item>
      <title>Hashの用語まとめ</title>
      <link>https://yutakikuchi.github.io/post/201103040151/</link>
      <pubDate>Fri, 04 Mar 2011 01:51:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201103040151/</guid>
      <description> [Math] : Hashの用語まとめ 強衝突耐性  f(x) = f(y) という条件のx!=yという(x,y)というペアを発見するのは難しいという内容。
md5/shaなどで入力の非ユニークな値、すなわちx!=yとなる(x,y)のペアが発見されている様子。
  弱衝突耐性  xがNに含まれ、f(x) = f(y)となるx!=yとなることを発見するのが難しいという内容。
例えば、f(x) = A という状態が分かっていて、Aとなるf(y)を見つけることが難しい。
  非可逆性  f(x)というHash値からxという入力値を求めることが難しいという内容。
  完全Hash  コリジョンが発生しないHash関数。
しかしコリジョンが発生しないのはあらかじめデータの個数(N)が分かっているとき。
  最小完全Hash  N個のKeyに対する完全Hashが最小の状態となる事。
つまりHashの大きさがNとなり、無駄の無い最小な状態である事。
  参考  wikipedia
http://ja.wikipedia.org/wiki/MD5
  </description>
    </item>
    
    <item>
      <title>ギャル文字変換Bookmarklet</title>
      <link>https://yutakikuchi.github.io/post/201103020038/</link>
      <pubDate>Wed, 02 Mar 2011 00:38:44 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201103020038/</guid>
      <description>[javascript] : ギャル文字変換Bookmarklet 概要  ギャル文字も新しい日本語表記の一つだと勝手に思っています。
「ギャル文字の方が断然読みやすいです！」ギャルの中でも少数派だとは思っていますが、
そんな方のためにWebページのコンテンツをギャル文字に変換するスクリプトを書いてみました。
変換機能はbookmarkletとして提供します。方法は閲覧中のWebページでbookmarkletを実行するだけです。
※利用は自己責任でお願いします。また何かしら問題が発生したら提供を即中止します。
  Bookmarklet  ブックマーク ギャル文字変換bookmarklet
※上のリンク先に遷移し、表示されているリンクをbookmarkしてください。
 bookmarkソース javascript:void((function(){var%20s=document.createElement(&#39;script&#39;);s.setAttribute(&#39;src&#39;,&#39;http://mobiles-proxy.appspot.com/statics/js/v1/gmbookmarklet.js&#39;);s.setAttribute(&#39;id&#39;,&#39;gmbookmark&#39;);document.body.appendChild(s)})()); 上のソースをそのままbookmarkに貼付けてもらっても大丈夫です。
   変換後イメージ 某サイトのTopics(iPhone-Safari)
twitterのstatus(safari)
 GitHub  https://github.com/yutakikuchi/JS/tree/master/gmconv
JSに詳しい方、修正点などありましたらご指摘いただけると助かります。
マルチバイトkeyのhashテーブルを基に変換処理をしているだけです。
ソースも一部blogに張っておきます。
bookmarkletから呼び出されるscript if( !GM ) var GM = {}; if( !IE ) var IE = ( /*@cc_on!@*/0 ) ? true : false; if( !JS ) var JS = {}; (function(){ JS.util = {}; JS.util.getTagName = function( tag ) { return document.</description>
    </item>
    
    <item>
      <title>iPhoneのwindow.getSelection()について</title>
      <link>https://yutakikuchi.github.io/post/201102160833/</link>
      <pubDate>Wed, 16 Feb 2011 08:33:48 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201102160833/</guid>
      <description>[javascript] : iPhoneのwindow.getSelection()について 値の取得  window.getSelection(document.getSelection)は画面のドラッグにより選択した文字列を読み取るjavascriptの関数です。
以前はsafariなどで利用できなかったようですが、現在はほとんどのブラウザで読み取ることが可能です。
(safari/firefox/chrome/operaでは動作を確認しています。2011/2/15日現在)
iphoneでwindow.getSelectionを利用するときに少しハマったので、メモしておきます。
  browser-javascript  HTMLに仕込んだjavascriptのwindow.getSelection()はiphoneでも動作します。
HTMLを読み込んだ時点でjavascriptが正しく設定されていると値が取得できるようです。
以下は選択肢た文字列をgoogle検索結果に自動的に飛ばす処理を書いています。
 htmlhead metahttp-equiv=&#34;content-type&#34;content=&#34;text/html; charset=UTF-8&#34;/ metahttp-equiv=&#34;Pragma&#34;content=&#34;no-cache&#34; metahttp-equiv=&#34;Cache-Control&#34;content=&#34;no-cache&#34; titleget Selection Testtitle script vardrag = false; vargetText = function(){ if(drag === true){ window.open(&#39;http://www.google.com?q=&#39;+ encodeURI(window.getSelection())); } drag = false; } varsetDragStatus = function(){ drag = true; } document.addEventListener(&#39;touchend&#39;, getText, false); document.addEventListener(&#39;touchstart&#39;, setDragStatus, false); script head body 選択文字列 iPhone テストbr 選択文字列 iPhone テストbr 選択文字列 iPhone テストbr brbr 選択文字列 iPhone テストbr body html    bookmarklet-javascript  bookmarkletの場合は注意が必要です。※PCのbrowserでは以下のコードが動作しても、iphone上では動作しません。</description>
    </item>
    
    <item>
      <title>WordPressをGoogle App Engine上で動かす</title>
      <link>https://yutakikuchi.github.io/post/201102111710/</link>
      <pubDate>Fri, 11 Feb 2011 17:10:44 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201102111710/</guid>
      <description>[Python] : WordPressをGoogle App Engine上で動かす WordPress on Google App Engine  WordPressを手軽にGoogle App Engine上で動かすためのOpen Sourceがあります。
PyPress For GAE
http://github.com/mdmcginn/pp4gae
手順は以下に細かく書いてあります。
https://github.com/zrenx/pp4gae#readme
※設定が簡単だったのでローカルで導入してみたところ以下の印象を受けました。
 挙動が遅い。web2pyが遅いのか,pp4gaeが遅いのかが今のところ不明。 カスタマイズの方法に関するドキュメントが無い。 特定のブログ形式しか導入できない？ WordPress本体自体がPHPで書かれているから、WordPressをそのままGAE for PHPとして動かせないか。← 少なくともGAEはRDBをサポートしていないので、WordPressのMysql部分はGQLなどへの書き換えが必要になりそう。 buddypressのようなプラグインが設定できない？ http://ja.wordpress.org/2009/05/01/make-friends-with-buddypress/    画面キャプチャ  シンプルなblogです。通常のWordPressには管理画面などが存在するようなのですが、PyPressには存在しないようです。

  導入手順  1. pp4gaeをgithubよりclone
git clone http://github.com/mdmcginn/pp4gae.git2. web2pyのダウンロード/解凍
fetch http://www.web2py.com/examples/static/web2py_src.zip tar -xzf web2py_src.zip3. pp4gaeをweb2py側にコピー
cp -r pp4gae web2py/applications4. app.yaml,routes.pyをコピー
cp web2py/applications/pp4gae/app.yaml web2py/ cp web2py/applications/pp4gae/routes.py web2py/5. app.yamlを編集
先頭の１行目を自分のapplication名に変更します。 application: hogehoge6.</description>
    </item>
    
    <item>
      <title>Mysqlの起動に関するメモ</title>
      <link>https://yutakikuchi.github.io/post/201102102219/</link>
      <pubDate>Thu, 10 Feb 2011 22:19:02 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201102102219/</guid>
      <description>[Mysql] : Mysqlの起動に関するメモ Mac OS Xでの設定  今回はMacOS10.6.5での設定を行いました。
UnixやLinuxとの違いはあると思いますが、多少は参考になるかもしれません。
  自動起動設定  起動 /Library/StartupItems/MySQLCOM/MySQLCOM start  停止 /Library/StartupItems/MySQLCOM/MySQLCOM stop    起動/停止  起動 sudo mysqld_safe  停止 mysqladmin -u root -p shutdown  起動確認 mysqladmin ping mysqld is alive    rootのpassword設定  これはまず最初にやるべき事ですね。
空passwordはセキュリティ上よろしく無いので、rootのパスワードを変更します。
mysqladmin -u root password 新しいパスワード   my.cnf  初期設定時だと/etc/my.cnfが存在しないので、特定のフォルダからコピーをします。
sudo cp /usr/local/mysql-5.1.39-osx10.5-x86/support-files/my-medium.cnf /etc/my.cnf必要に応じて上のファイルをカスタマイズする必要があります。
my.cnfを変更した場合はmysqlを再起動する必要があります。
  my.cnfのチューニング  log-slow-queries,long-query-timeの設定を行います。</description>
    </item>
    
    <item>
      <title>Pythonでの暗号化/復号化(AEC-DES,RSA)</title>
      <link>https://yutakikuchi.github.io/post/201102071829/</link>
      <pubDate>Mon, 07 Feb 2011 18:29:26 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201102071829/</guid>
      <description>[Python] : Pythonでの暗号化/復号化(AEC-DES,RSA) 概要  Pythonで文字列の暗号化を行う場合は、Python Cryptography Toolkitを利用すると簡単に実装ができます。
Python Cryptography ToolkitはGoogle App Engineでもサポートされています。
http://www.amk.ca/python/code/crypto.html
http://code.google.com/intl/ja/appengine/docs/python/tools/libraries.html#PyCrypto
この記事ではAES-DES,RSAの実装について説明したいと思います。
  始めに注意  Google App Engineのサーバに下記のプログラムを配布すれば利用ができますが、
localの場合は最初に
http://www.amk.ca/python/code/crypto.html
からファイルをダウンロード、installする必要があります。
Macでやりましたが、以下の手順でinstallが可能です。
fetch http://www.amk.ca/files/python/crypto/pycrypto-2.0.1.tar.gz tar -xzf pycrypto-2.0.1.tar.gz cd pycrypto-2.0.1 python setup.py build sudo python setup.py install   AES-DES  メッセージを特定のkEYで暗号化します。しかしこの方法には以下のようなByte数の制限があります。
KEY : 16,24,32Byteのいずれか 
MESSAGE : 16Byteの倍数 
以下のサンプルコードでは16ByteのKEYを利用し、unixtimeと文字を結合した文字列を暗号化/復号化しているサンプルになります。
サンプルコード #! /usr/bin/env python # -*- coding: utf-8 -*- import Crypto.Cipher.AES as AES import datetime,time #16,24,32byteの文字列が暗号化KEY CRYPTO_KEY = &#34;</description>
    </item>
    
    <item>
      <title>超絶簡単 Pythonでの正規表現</title>
      <link>https://yutakikuchi.github.io/post/201102040134/</link>
      <pubDate>Fri, 04 Feb 2011 01:34:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201102040134/</guid>
      <description>[Python] : 超絶簡単 Pythonでの正規表現 概要  Pythonでの正規表現の使い方についてまとめます。
  r&#39;a&#39;表記  raw string表記を示します。余分な\を省くことが可能です。
例えば改行はr&#39;\n&#39;とすれば正規表現として扱うことが出来ます。   メソッド     メソッド   役割     match   正規表現が文字列の先頭と一致するかチェック     search   正規表現による文字列を走査     split   正規表現に一致するもので配列に分離     sub   正規表現による置換     subn   subと同じだが、個数を制限可能      matchとsearch  ※matchとsearchでは役割が異なるので注意が必要です。</description>
    </item>
    
    <item>
      <title>system callのtraceについて</title>
      <link>https://yutakikuchi.github.io/post/201102020154/</link>
      <pubDate>Wed, 02 Feb 2011 01:54:27 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201102020154/</guid>
      <description>[CentOs] : system callのtraceについて system call trace  起動プログラムからOSのシステムコールを呼び出される箇所をトレースするコマンドがあります。
strace / itrace / ktrace / kdumpなど。
単純なPHPのプログラムでは特にトレースする意味はないと思いますが、PHPのエクステンション作成やミドルウェア構築の時にはトレースして状態を確認してみるのが良いかと考えます。 今回はstraceの使い方についてまとめます。
  使いそうなオプション  strace    オプション   意味     -c   カウント数やコール、エラーなどを記録し、サマリーも表示     -e   特定のシステムコールのみ拾う     -f   forkされた子プロセスも拾う     -F   vforkを拾う     -T   処理にかかった時間を表示     -o   ファイルに記録する     -p   プロセスIDを指定する      通常の起動  strace httpd</description>
    </item>
    
    <item>
      <title>MyISAM,InnoDBについて</title>
      <link>https://yutakikuchi.github.io/post/201101312328/</link>
      <pubDate>Mon, 31 Jan 2011 23:28:46 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201101312328/</guid>
      <description> [Mysql] : MyISAM,InnoDBについて イメージ  MyISAM:検索が速い。InnoDB : 障害に強い。こんなイメージしかありませんでしたが、勉強もかねて性能の違いをまとめていきたいと思います。
  メリデメ  MyISAMのメリット   参照系処理が高速。 全文検索可能。 参照テーブルの圧縮。    MyISAMのデメリット   更新系処理が重い? テーブルロックをかけるので、複数のクライアントからの同時更新でロックが発生する。 外部KEY制約を使うことが出来ない。 リカバリが面倒。    InnoDBのメリット   トランザクションが使えるので、リカバリが楽。 行ロックを行うので更新/参照の競合が限定的になる。 外部KEY制約を設けることが可能。 オンラインバックアップ可能。 インデックスの更新/再構築が効率化。 主KEYがクラスタインデックスなので、主KEYでの検索が高速。    InnoDBのデメリット   データサイズがMyISAMに比べて大きくなるのでその分ディスク容量が必要。 参照処理がMyISAMに比べて遅い。 余程のことが無い限り全文検索や参照テーブルの圧縮といったことは行わないはずなので、MyISAMのみの性能を重視するメリットは正直薄いのかなと思います。またInnoDBのメリット/デメリットの項目を見ても利用する価値はInnoDBの方が高いと言えそうです。今後はInnoDBを使いましょう。参照する場合のポイントはクラスタインデックスの利用などでしょうか。自分は思いつかなかったのですが、マスターをInnoDBで、スレーブをMyISAMで構築したらいいという話があるようなのですが、http://opendatabaselife.blogspot.com/2009/08/innodbmyisam.html　こちらのサイトでロックの観点/スレーブからマスターへ移すときの難点が書かれています。
  リンク   show table status like &#39;table_name&#39; Rowsの違い MyISAMからInnoDBへ切り替えるときの注意点      </description>
    </item>
    
    <item>
      <title>超絶簡単Pythonクラスのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201101202333/</link>
      <pubDate>Thu, 20 Jan 2011 23:33:14 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201101202333/</guid>
      <description>[Python] : 超絶簡単Pythonクラスのまとめ 概要  このページではPythonのクラス定義および使い方について簡単にまとめます。
紹介する項目は次のものです。
 クラス定義 継承 階層定義    1.クラスの定義  クラスファイル  #! /usr/bin/env python  # -*- coding: utf-8 -*-  class SuperClass(object): #classの宣言 name = &#39;&#39; #public変数 __callcount = 0 #private変数 def __init__( self ): #コンストラクタ self.name = &#39;SuperClass&#39; def call( self ): #メソッドのself記述の省略はできません。ちょっと面倒です。  self.__callcount = self.__callcount + 1 return self.name def getCallCount( self ): return self.__callcount def setName( self, name ): #第一引数はself,第二引数から通常引数  self.</description>
    </item>
    
    <item>
      <title>innerHTMLのeventに関する罠</title>
      <link>https://yutakikuchi.github.io/post/201101190020/</link>
      <pubDate>Wed, 19 Jan 2011 00:20:54 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201101190020/</guid>
      <description>[javascript] : innerHTMLのeventに関する罠 罠  innerHTMLの罠を紹介します。
addEventListenerにてtagにeventハンドラを追加している場合、
そのタグの中身をinnerHTMLを使って書き換えを行うと、eventが利用できなくなります。
  sampleコード  html script varinit = function(){ varsample = document.getElementById(&#39;child_id&#39;); sample.addEventListener(&#39;mousedown&#39;, down, false); } vardown = function(){ alert(&#39;mouse down&#39;); } varreplaceHTML = function(){ varparent= document.getElementById(&#39;parent_id&#39;); vartmp = parent.innerHTML; parent.innerHTML = tmp; } script bodyonload=&#34;init();&#34; divid=&#34;parent_id&#34; divid=&#34;child_id&#34;div samplediv div inputtype=&#34;button&#34;value=&#34;push&#34;onclick=&#34;replaceHTML();&#34; body html    sampleコードの中ではparent_id,child_idが入れ子の状態になっています。
parent_idのinnerHTMLに元々のhtmlをそのまま代入しているだけなのですが、
child_idのeventが失われてしまいます。innerHTMLの値を書き換える場合は注意が必要です。
  回避策  必要な要素のみ書き換えを行うようにします。
上の場合ですと、child_idのinnerHTMLだけを書き換えます。
※Safariでは正常動作確認済みです。
  html script varinit = function(){ varchild = document.</description>
    </item>
    
    <item>
      <title>超簡単 Perl一問一答学習帳</title>
      <link>https://yutakikuchi.github.io/post/201101082020/</link>
      <pubDate>Sat, 08 Jan 2011 20:20:25 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201101082020/</guid>
      <description>[Perl] : 超簡単 Perl一問一答学習帳 はじめに  Perlの初心者が基本的な内容について学習した内容をQ&amp;A方式でまとめてみました。
間違いや最適化などあればご指導よろしくお願い致します。
  目次   コマンドライン系  コマンドラインでプログラムを実行するには？ ファイルのバックアップを取って、文字列を置換するには？ CPANモジュールをinstallするには？ インストール済みのCPANモジュールを表示するには？   処理関連系  配列を単純に出力するには？ 連想配列を単純に出力するには？ 配列の大きさを知るには？ ファイルの中身を逆から出力するには？ ファイルの中身から特定の文字を含む行を検索するには？ 配列を特定の文字列で連結するには？ 配列を文字列の長さで並びかえるには？ アルファベットで降順に並び替えるには？ 連想配列の値で重複したデータを取得するには？ サブルーチンを使ってfizzbuzを解くには？  クラス/CPAN系  クラスを継承させるには？ APIからXMLを取得してパース、出力するには？    ■コマンドライン系
 1.コマンドラインでプログラムを実行するには？  以下をコマンドラインで実行するお！
perl -e &#39;for( $i=0; $i   2.ファイルのバックアップを取って、Perlという文字列をJavascriptに変換するには？  以下をコマンドラインで実行するお！
perl -pi.bak -e &#39;s/Perl/Javascript/g&#39; test.txt    3.CPANモジュールをinstallするには？  以下をコマンドラインで実行するお！
sudo perl -MCPAN -e shell cpan[1] install JSON    4.</description>
    </item>
    
    <item>
      <title>5分で理解するPython文字コード</title>
      <link>https://yutakikuchi.github.io/post/201101060304/</link>
      <pubDate>Thu, 06 Jan 2011 03:04:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201101060304/</guid>
      <description>[Python] : 5分で理解するPython文字コード 概要  Pythonの文字コード指定による日本語文字化けの問題は誰もがハマるところ。
この記事では文字化けの解決方法を簡単にまとめたものになります。
実行環境はMacです。UnixやLinuxでもPath以外の箇所は同じように動作すると思います。
間違いの指摘などあればよろしくお願いします。
  注意すべきこと  簡単にいうと以下の4点になりそうです。
1. ファイルの文字コード指定
2. ファイル内部の行頭での文字コード指定
3. Pythonの内部コードはUnicode(オブジェクト)として扱っている。
4. printなどの出力関数では端末の設定に合わせた文字コードが利用されている
※1,2の文字コード指定は一致させます。
  1.ファイルの文字コード指定  サンプルとしてUTF-8で指定する例を挙げます。ファイル名はencode.pyです。
nkf --guess encode.py UTF-8 (LF)もしEUC-JPなどになっていれば以下のコマンドで変換します。
nkf -w --overwrite encode.py   2.ファイル内部の行頭での文字コード指定  encode.pyのファイル内部の行頭に以下のよう設定します。シバンの次の行が文字コード指定になります。
#! /usr/bin/env python # -*- coding:utf-8 -*-    3. Pythonの内部コードはUnicode(オブジェクト)として扱っている。  UTF-8とUnicode(オブジェクト)は違うものであり、必要に応じてそれぞれの変換が必要になります。
Python内部ではUnicodeを使用しているので、文字列置換処理を行う場合などは処理前にUnicodeに変換する必要があります。
以下に簡単なサンプルを書きます。
#! /usr/bin/env python # -*- coding:utf-8 -*- data_orig = &#34;日本語&#34; #日本語代入(UTF-8) print type( data_orig ) #typeはstr data_unic = data_orig.</description>
    </item>
    
    <item>
      <title>20秒で理解するJSONP</title>
      <link>https://yutakikuchi.github.io/post/201101040227/</link>
      <pubDate>Tue, 04 Jan 2011 02:27:52 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201101040227/</guid>
      <description>[javascript] : 20秒で理解するJSONP １行理解  JSONP(JSON with Padding)とはjavascriptコールバック関数を利用し外部ドメインサーバとJSONデータをやり取りする方法である。
 ※通常のHTMLでは同一ドメイン以外の通信が行えないという制約があるが、javascriptタグのsrc属性とJSONデータ/コールバック関数を利用して外部ドメインとデータのやり取りを可能にする技術。

 やること ■クライアント  外部ドメインのサーバから受け取ったJSONデータを処理するコールバック関数の定義。
 ※ここで定義するコールバック関数はサーバサイドで定義される関数名と一致させる必要がある。
 ■サーバ側  JSONデータを引数としたクライアント側のコールバック関数の呼び出し
   クライアントサンプル html head scripttype=&#39;text/javascript&#39;src=&#39;http://www.yutakikuchi.com/jsonp/data.dat?callback=callbackFunc&#39;/ varnumber= Object(); varcallbackFunc = function(data ){ //alert( data ); number= eval(data ); //javascriptの配列データが格納される。 }   ※scriptタグのsrc属性でサーバサイドへのJSONデータのリクエストを行います。callback関数名はクライアント側で自由に設定できることを想定しています。

 サーバサイドサンプル  $callbackfunc = $_GET[ &#39;callback&#39; ] ; $test_data = array( &#39;items&#39; = array( 1,2,3 ) ); $json_data = json_encode( $test_data ); echo $callbackfunc .</description>
    </item>
    
    <item>
      <title>MacUserのためのGitHub登録手順のまとめ</title>
      <link>https://yutakikuchi.github.io/post/201012300524/</link>
      <pubDate>Thu, 30 Dec 2010 05:24:07 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201012300524/</guid>
      <description>[programming] : MacUserのためのGitHub登録手順のまとめ 0 手順  GitHub登録方法を載せておきます。
 準備 Gitユーザ登録  公開鍵登録 レポジトリ作成 Commit  git設定ファイル    1 準備  git-coreのinstallをします。
sudo port install git-coregit-hubに登録する公開鍵をローカルマシーンにて作成しておきます。
cd ~/.ssh ssh-keygen less id_rsa.pub ← ファイルの内容を後でGitHub側に登録します。   2登録  ユーザ登録 翻訳機能で日本語表示できるので迷うことなく登録できると思います。
https://github.com/  公開鍵登録 以下のページのSSH公開鍵登録より先ほど作成した公開鍵をコピペします。
https://github.com/account  レポジトリ作成 GitHubのWeb上で新しいレポジトリを作成できます。
https://github.com/repositories/new  Commit Git用のディレクトリの下に各プロジェクト用のディレクトリを作成しておくと良いと思います。※PHPというプロジェクトのレポジトリを作成してみました。
mkdir -p git/PHP cd git git config --global user.name &#34;自分の名前&#34; git config --global user.email メールアドレス cd PHP git init //初期化 touch README //ファイル作成 git add README //ファイルをadd git commit -m &#39;test commit&#39; README //commit git remote add origin git@github.</description>
    </item>
    
    <item>
      <title>javascriptのクラスまとめ</title>
      <link>https://yutakikuchi.github.io/post/201012210034/</link>
      <pubDate>Tue, 21 Dec 2010 00:34:30 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201012210034/</guid>
      <description>[javascript] : javascriptのクラスまとめ クラスの概念   Javascriptにも一応クラスみたいなものといった概念が存在しますが、多言語のクラスに比べると規制が緩いもののようです。 javascriptではprototypeといったものをベースとしたオブジェクト指向として考えられています。（これはあとで説明） prototypeのチェーンといった方法を利用して継承を実装することも可能です。    シンプルなインスタンス化  関数で定義されたクラス的なものをnewすることでインスタンスを生成することが出来ます。下にサンプルコードを記述します。
var Parent = function() { //ある意味コンストラクタみたいなもの this.name = &#39;Parent&#39;; //クラスのプロパティはthisで表現する。 this.echo = function() { alert( this.name ); } } var instance = new Parent(); //インスタンス化 instance.echo(); // Parent    Prototypeの利用  ここがprototypeベースのオブジェクト指向の内容になります。上のインスタンス化の方法にはクラス的なものをnewするときにプロパティやメソッドをまるまるメモリに展開してしまうため、newの数が増えるたびに無駄な領域確保が発生してしまいます。そこでPrototypeといった属性を利用します。Prototypeを利用することにインスタンスからの参照を暗黙的参照に切り替えることが出来ます。コードを例に見てみます。
var Parent = function() { this.name = &#39;Parent&#39;; } // prototypeを利用 Parent.prototype.echo = functio() { alert( this.name ); // Parent }; var instance = new Parent(); instance.</description>
    </item>
    
    <item>
      <title>SimpleXMLElement Objectの参照</title>
      <link>https://yutakikuchi.github.io/post/201012150044/</link>
      <pubDate>Wed, 15 Dec 2010 00:44:01 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201012150044/</guid>
      <description>[PHP] : SimpleXMLElement Objectの参照 内容  久しぶりにxmlをパースする処理を書いていたんだけど、SimpleXMLElement Objectの参照方法を忘れていたのでメモをしておく。(20120416追記) 今まではサンプルプログラム中のforeachで$resultに添字$iでインクリメントして値を代入していましたが、$iを使わずに配列の[]を使って右辺のarray型を代入するように修正しました。
  xmlサンプル  xml version=&#34;1.0&#34;encoding=&#34;Shift-Jis&#34;?   ソフトバンク株式会社 http://www.softbank.co.jp/   楽天株式会社 http://www.rakuten.co.jp/info/      simplexml_load_string関数  PHPの関数によりSimpleXMLElement ObjectとしてParseを行う。SimpleXMLElement Objectを更にPHP配列に整形してゆく。
 $xml = XML   ソフトバンク株式会社 http://www.softbank.co.jp/   楽天株式会社 http://www.rakuten.co.jp/info/   XML; print( $xml . &#34;\n&#34; ); $data = simplexml_load_string( $xml ); print_r( $data );    simplexml_load_string関数の実行結果  xmlがSimpleXMLElement Objectに変換されていることが分かる。
SimpleXMLElement Object ( [company] = Array ( [0] = SimpleXMLElement Object ( [name] = ソフトバンク株式会社 [url] = http://www.</description>
    </item>
    
    <item>
      <title>超簡単なインスタンスコンテナ 動的編</title>
      <link>https://yutakikuchi.github.io/post/201012060127/</link>
      <pubDate>Mon, 06 Dec 2010 01:27:44 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201012060127/</guid>
      <description>[PHP] : 超簡単なインスタンスコンテナ 動的編 概要 以前のエントリで超簡単なインスタンスコンテナというタイトルで記事を書きましたが、
コンテナに格納できるインスタンスが静的に呼び出されるものだったので、
動的に呼び出されるように修正してみました。以前のエントリは以下のものです。
http://d.hatena.ne.jp/yutakikuchi/20100907/1283877423

 改良したインスタンスコンテナ  class Container { static private $instance_ = array(); private function __construct() {} static public function get( $class, $path = null ) { if( isset( self::$instance_[ $path ] ) ) { return self::$instance_[ $path ]; } if( !is_readable( $path ) ) { throw new Exception(); } require_once $path; $instance = new $class; self::add( $path, $instance ); return $instance; } static public function add( $path, $instance ) { if( !</description>
    </item>
    
    <item>
      <title>SQLのQUERY生成でsprintfを使いたくない</title>
      <link>https://yutakikuchi.github.io/post/201011090001/</link>
      <pubDate>Tue, 09 Nov 2010 00:01:51 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201011090001/</guid>
      <description>[PHP] : SQLのQUERY生成でsprintfを使いたくない 内容  sprintfは見た長い文字列を書き足しやすくするための関数で、SQLの生成ではできるだけ使いたくない。 sprintfは型の指定が入るので、特にnullなどの扱いに気をつけないといけない。 sprintf + LIKE文を作ろうとすると %を多用するので、LIKE &#34;%%%s%%&#34;といった指定になり、見づらい。   例  $sql = sprintf( &#34;INERT INTO %s.%s (%s,%s) value(%s,%s) WHERE id = %s &#34;, &#34;mysql&#34;, &#34;test&#34;, &#34;name&#34;, &#34;key&#34;, null, null, 7 ); echo $sql . &#34;\n&#34;; $sql = sprintf( &#34;SELECT * FROM %s.%s WHERE id = %d AND name LIKE &#39;%%%s%%&#39; &#34;, &#34;mysql&#34;, &#34;test&#34;, 1, &#34;hoge&#34; ); echo $sql . &#34;\n&#34;;   出力SQL文 INERT INTO mysql.</description>
    </item>
    
    <item>
      <title>Google app engineでBeautifulsoupを使う</title>
      <link>https://yutakikuchi.github.io/post/201011030135/</link>
      <pubDate>Wed, 03 Nov 2010 01:35:15 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201011030135/</guid>
      <description>[Python] : Google app engineでBeautifulsoupを使う 概要   GAEでHTMLをパースして加工したいのでBeautifulsoupというモジュールをインストールして使う。 デフォルトGAE環境では利用できないのでBeautifulsoupファイルをアップして使う。    環境   System環境 : iMac.local 10.4.0 Darwin Kernel Version 10.4.0: Fri Apr 23 18:28:53 PDT 2010; root:xnu-1504.7.4~1/RELEASE_I386 i386 python : Python 2.5.5    ダウンロード   圧縮ファイルダウンロードおよび解凍 fetch &#39;http://www.crummy.com/software/BeautifulSoup/download/3.x/BeautifulSoup-3.0.8.tar.gz&#39; tar -xzf BeautifulSoup-3.0.8.tar.gz   インストール  解凍したディレクトリに移動し以下のコマンドを実行
$ python setup.py install running install running build running build_py running install_lib running install_egg_info Removing /opt/local/Library/Frameworks/Python.framework/Versions/2.5/lib/python2.5/site-packages/BeautifulSoup-3.0.8-py2.5.egg-info   Beautifulsoup.</description>
    </item>
    
    <item>
      <title>Mysqlのメモ</title>
      <link>https://yutakikuchi.github.io/post/201011010022/</link>
      <pubDate>Mon, 01 Nov 2010 00:22:40 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201011010022/</guid>
      <description>[Mysql] : Mysqlのメモ 概要 学生の課題みたいだけど、学校の成績管理表を作成してみる。
外部KEYの使い方についてのメモです。
 tableの定義 # 生徒管理テーブル(student)
# 科目管理テーブル(subject)
# 成績管理テーブル(score)
 ER図 

 やりたい事 # １つのテーブルでデータの重複をさせない制約をつける。
# 生徒管理テーブルからデータがUPDATE/DELETEされたら、成績管理テーブルからもデータをUPDATE/DELETE。
# 科目管理テーブルからデータがUPDATE/DELETEされたら、成績管理テーブルからもデータをUPDATE/DELETE。
 データの重複をさせない(UNIQUEな値を入れる) カラム定義の最後にUNIQUEを指定します。そうすると一意な値のみ登録が可能になります。例えば科目名とか。
CREATE TABLE IF NOT EXISTS `Subject` ( `id` INT NOT NULL , `subject` VARCHAR(45) NULL UNIQUE ,   外部KEY制約を利用する。 Engine TypeをInnoDBに設定する。必ず外部KEY制約を設けるtableの両方をInnoDBに設定する。
両方InnoDBに設定しないとtableをcreateする際にエラーが出る。Can&#39;t create table &#39;./mysql/table.frm&#39; (errno: 150)
FOREIGN KEY(a) REFERENCE parent(b)を利用する。
※error:150の原因は他にもあるようです。Mysqlのリファレンスに書いてあります。
http://dev.mysql.com/doc/refman/4.1/ja/innodb-foreign-key-constraints.html
SHOW WARNINGS; CREATE TABLE IF NOT EXISTS `score` ( `id` INT NOT NULL AUTO_INCREMENT , `studentid` INT NULL , `subjectid` INT NULL , `score` INT NULL , PRIMARY KEY (`id`), FOREIGN KEY( `studentid` ) REFERENCE student( `id` ) ON UPDATE CASCADE ON DELETE CASCADE FOREIGN KEY( `subjectid` ) REFERENCE subject( `id` ) ON UPDATE CASCADE ON DELETE CASCADE   外部KEY制約のオプション ON UPDATE CASCADE ON DELETE CASCADE</description>
    </item>
    
    <item>
      <title>serializeとjson関数の比較</title>
      <link>https://yutakikuchi.github.io/post/201010180037/</link>
      <pubDate>Mon, 18 Oct 2010 00:37:58 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201010180037/</guid>
      <description>[PHP] : serializeとjson関数の比較 概要  serializeとjson_encodeのどちらの利用が望ましいかを調べます。理由はWebAPIを作成するときにどのレスポンス形式が最適なのかを検証する必要がでてきたためです。事前に上がった意見としては”json_encodeの方がデータ容量少なくなるから速いっしょ”、”serializeの方がphpをそのまま扱っているんだから変換処理コストが少ないはず”などの意見がありましたが、どれが正確か分からないので実際に試してみます。
  サンプルするデータ  50回実行して処理時間の平均値をサンプリングします。
 php配列から各エンコード処理を施した時の処理コスト  シングルバイト文字列を含む配列を変換するコスト マルチバイト文字列を含む配列を変換するコスト  エンコードしたデータからphp配列への復元処理コスト  シングルバイト文字列を含む配列エンコードしたデータをデコード マルチバイト文字列を含む配列をエンコードしたデータをデコード     エンコード ソースコード   $data = array(); for( $i=0; $i10000; $i ++ ) { $data[] = &#34;test&#34;; //ここを適宜変えます。 } $start = microtime(); //PHP serialize $string = serialize( $data ); //JSON encode //$string = json_encode( $data ); $end = microtime(); echo $end - $start . &#34;</description>
    </item>
    
    <item>
      <title>findとsedを利用した一行野郎のファイル名変換</title>
      <link>https://yutakikuchi.github.io/post/201010142353/</link>
      <pubDate>Thu, 14 Oct 2010 23:53:25 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201010142353/</guid>
      <description>[Linux] : findとsedを利用した一行野郎のファイル名変換 概要 X系環境で特定文字を含んだ大量ファイルを一気に特定のルールに従ったファイル名変換したいことってよくあります。
ファイル名変換用のスクリプトをperlやphpなんかで組んでそれを特定のディレクトリに流してもいいけど、
シェルスクリプトだけでやる方法を記述しておきます。
(例) :　例えば以下のファイルのように&#34;Test&#34;などの特定文字列を含んだファイルが複数存在し、
PHPTestというファイルだけに変換を加えたい時。&#34;Test&#34;を&#34;Stab&#34;に変更します。
-rw-r--r-- 1 YutaKikuchi staff 0 2010-10-14 23:19 PHPTestAction.php -rw-r--r-- 1 YutaKikuchi staff 0 2010-10-14 23:19 PHPTestCode1.php -rw-r--r-- 1 YutaKikuchi staff 0 2010-10-14 23:19 PHPTestModule.php -rw-r--r-- 1 YutaKikuchi staff 0 2010-10-14 23:20 PerlTestAction.pl -rw-r--r-- 1 YutaKikuchi staff 0 2010-10-14 23:20 PerlTestCode1.pl  実行コマンド 確認 find . -name &#34;*PHPTest*&#34; | sed -e &#39;s/\(\(.*\)Test\(.*\)\)/mv \1 \2Stab\3/g&#39;  実行 find . -name &#34;</description>
    </item>
    
    <item>
      <title>IPアドレスのまとめ</title>
      <link>https://yutakikuchi.github.io/post/201010022241/</link>
      <pubDate>Sat, 02 Oct 2010 22:41:35 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201010022241/</guid>
      <description>[network] : IPアドレスのまとめ 大事なこと ・IPv4では３２ビットの整数値。２進数の8ビットずつを４つの組みに分ける。分かりやすいように１０進数で表現されることが多い。
・IPv6では１２８ビット長になる。
・IPアドレスはネットワーク部とホスト部から成り立つ。
・IPアドレスの分類はクラスAからクラスEの５つに分類可能。クラスEは使われていない。※詳細は以下を参照
・ブロードキャストアドレスとはホスト部のビットを全部１としたもの。種類は２つあって自分が所属するローカルブロードキャストアドレスと異なるIPネットワークのダイレクトブロードキャストアドレスがある。
・クラスAやクラスBをそのまま使うのは非効率なので、サブネットマスクの導入により小さく分割する方法がある。
・例えばクラスBのサブネットマスクを255.255.255.192とすると上位26ビットをネットワークアドレスとして利用出来る。
・CIDRとはクラスA,クラスB,クラスCの8ビット単位のクラス分けをなくしたアドレス付けのこと。
・CIDRを利用して、クラスCの連続するネットワークアドレスを１つのアドレスとして利用する事ができる。
・私的なネットワーク内部で利用できるプライベートIPアドレスというものがあり、自由に利用することができる。※詳細は以下を参照
 クラスA ・クラスAは先頭の１ビットが0で始まり、ネットワークアドレス部は戦闘から8ビットとなる。(0.0.0.0-127.255.255.255)
・残りの２４ビットがホスト部となる。2の24乗−２(全て０や１は予約IP)なので割り当て可能数は16777214個となる。
 クラスB ・クラスBは先頭の２ビットが10で始まり、ネットワークアドレス部は先頭から16ビットとなる。(128.0.0.0-191.255.255.255)
・残りの１６ビットがホスト部となる。2の16乗-2(全て０や１は予約IP)なので割り当て可能数は65534個となる。
 クラスC ・クラスCは先頭の３ビットが110で始まり、ネットワークアドレス部は先頭から24ビットとなる。(192.0.0.0-223.255.255.255)
・残りの8ビットがホスト部となる。2の8乗-2(全て０や１は予約IP)なので割り当て可能数は254個となる。
 サブネットマスク    クラス   サブネットマスク     A   255.0.0.0       B   255.255.0.0     C   255.255.255.0       プライベートIPアドレス    クラス   プライベートアドレス     A   10.</description>
    </item>
    
    <item>
      <title>Strategyパターン</title>
      <link>https://yutakikuchi.github.io/post/201010010144/</link>
      <pubDate>Fri, 01 Oct 2010 01:44:20 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201010010144/</guid>
      <description>[PHP] : Strategyパターン 概要 このパターンはアルゴリズムをクラス化してクライアントからクラスの切り替えで処理を行うパターンになります。
クライアントからのアクセス用の共通メソッド(API)を用意してあげて、別々のクラスのメソッドにアクセスをして
完全に処理クラスに委譲することができます。
またこのパターンを利用するとContext内部でif else分などの条件文をすっきりさせることができます。
 Strategyクラス  //Strategy interface interface Strategy { public function calculate( array $array ); } //Strategy interfaceの実装 class SumCalculator implements Strategy { //計算の実行 public function calculate( array $data ) { $sum = 0; foreach( $data as $value ) { $sum += $value; } return $sum; } } //Strategy interfaceの実装 class MultiplyCalculator implements Strategy { //計算の実行 public function calculate( array $data ) { $sum = array_shift( $data ); foreach( $data as $value ) { $sum *= $value; } return $sum; } } このように共通Interfaceを用いて計算処理を行う個別クラスにアルゴリズムを記述します。</description>
    </item>
    
    <item>
      <title>Observerパターン</title>
      <link>https://yutakikuchi.github.io/post/201009300201/</link>
      <pubDate>Thu, 30 Sep 2010 02:01:37 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201009300201/</guid>
      <description>[design pattern] : Observerパターン 概要 このパターンは監視するクラス/監視されるクラスから成り立ち、監視されるクラスで特定の処理が入ったときに
監視クラスにメッセージを送信することができるというもの。例えば監視されるクラスで処理が終わったときに
監視クラスに通知を行ない、監視側で処理を動かしたい時になど利用出来る。ここで重要なことは監視されるクラスは
監視クラスの状態を知る必要はなく、自身の状態を監視クラスに伝えることのみを行えばよいためクラス関係は疎結合
になる。
 監視クラス  //監視を行うクラスinterface interface Observer { public static function update( $subject, $args ); } //監視クラスの実装 class ObserverClass implements Observer { public static function update( $subject, $args ) { echo( &#34;$subjectis update . args = $args&#34; ); } } 監視するクラスは監視されるクラス側から呼び出される通知用のメソッドのみを用意してあげます。
 監視されるクラス  //監視をされるクラスinterface interface Subject { public static function addObserver( Observer $observer ); } //監視をされるクラスの実装 class TestSubject implements Subject { private static $_observers = array(); //メソッドが呼び出される度に監視クラスに通知 public static function addCustomer( $name ) { foreach( self::$_observers as $obs ) { $obs::update( __CLASS__, $name ); } } //監視クラスの追加 public static function addObserver( Observer $observer ) { self::$_observers[] = $observer; } } 監視されるクラスは監視するObserverクラスを登録するようにします。登録している監視クラス全てに対して</description>
    </item>
    
    <item>
      <title>PHPでFilterChainを実装してみた</title>
      <link>https://yutakikuchi.github.io/post/201009292125/</link>
      <pubDate>Wed, 29 Sep 2010 21:25:24 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201009292125/</guid>
      <description>[PHP] : PHPでFilterChainを実装してみた 説明 前処理と後処理を各FilterClassに実装してFilterをChainさせます。
処理の順番としては
■前処理(prefilter)
filterA → filterB → filterC
■後処理(postfilter)
filterC → filterB → filterA
というように後処理は前処理と逆に行われます。  FilterChainクラス  class FilterManager { private static $_filters = array(); private static $_index = 0; private function __construct(){} public function build( array $filters ) { foreach( $filters as $filter ) { $filter_name = ucfirst( $filter ) . &#39;Filter&#39;; $file_name = $class_name . &#39;.php&#39;; //require_once( &#39;./Filter.php&#39; ); require_once( $file_name ); self::add( $filter_name ); } } static private function add( $filter ) { self::$_filters[] = $filter; } static public function execute() { // prefilter foreach( self::$_filters as $filter ) { $filter::prefilter(); ++self::$_index; } // postfilter foreach( array_reverse( self::$_filters ) as $filter ) { $filter::postfilter(); --self::$_index; } } static public function getCurrentFilter() { return self::$_filters[ self::$_index ]; } } FIlterChainを構築するbuildメソッド</description>
    </item>
    
    <item>
      <title>magic method</title>
      <link>https://yutakikuchi.github.io/post/201009270150/</link>
      <pubDate>Mon, 27 Sep 2010 01:50:08 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201009270150/</guid>
      <description>[PHP] : magic method phpのマジックメソッドについて挙動を確認してみました。
とりあえずは代表的なもののみをピックアップ。
&amp;lt;?php
class MagicMethod {
private $data_ = array();
public function __construct() { echo &amp;ldquo;call construct \n&amp;rdquo;; }
public function __destruct() { echo &amp;ldquo;call destruct \n&amp;rdquo;; }
public function __get( $key ) { echo &amp;ldquo;call _get \n&amp;rdquo;; return $this-&amp;gt;data[ $key ]; }
public function _set( $key, $name ) { $this-&amp;gt;data[ $key ] = $name; echo &amp;ldquo;call __set \n&amp;rdquo;; }
public function toString() { return CLASS__; }</description>
    </item>
    
    <item>
      <title>API Frameworkの設計</title>
      <link>https://yutakikuchi.github.io/post/201009241534/</link>
      <pubDate>Fri, 24 Sep 2010 15:34:16 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201009241534/</guid>
      <description> [design pattern] : API Frameworkの設計 クラス設計図 
 やりたいこと APIのように同一の処理の流れを組み込む場合、TemplateMethodパターンを利用してFrameWork化することができると思います。
PHPのFrameWorkはたくさん世の中に出回っていると思いますが、必要用途のモノだけを揃えた軽量FrameWorkが存在しないので
自作します。
1.APIを作成する人はModelクラスだけに手を入れる。Controllerも修正可能にしますが、極力修正させたくない。
2.Validate,DB接続,View機能はFrameWorkとして揃える。
 シーケンス図 
 処理の流れ 1. ControllerはApplicationからリクエストされたURIを取得します。
2. ApplicationからリクエストされたURIを元に設定ファイルを読み込みます。( 設定ファイルはphpのiniファイル形式とする予定 )
3. ApplicationからリクエストされたURIを元に必要なModelクラスを読み込みます。
4. Modelクラス内部でApplicationからリクエストされたパラメータを取得します。(API作者が実装)
5. Modelクラスは取得したパラメータのvalidateを行ないます。(API作者が実装)
6.DBに接続が必要な場合はDBクラスとConnectionを張って、データを取得します。(API作者が実装)
7.ModelクラスはDBから取得したデータの整形を行ないます。(API作者が実装)
8.ControllerにModelの結果を返却します。
9.ControllerにてView用のデータ整形を行ないます。
10.ApplicationにAPIとしての結果を返却します。
 実装 各パートの実装はちょくちょくやっています。
本気でやったら数時間で終わりそうですね。
 </description>
    </item>
    
    <item>
      <title>Singleton</title>
      <link>https://yutakikuchi.github.io/post/201009232329/</link>
      <pubDate>Thu, 23 Sep 2010 23:29:03 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201009232329/</guid>
      <description>[PHP] : Singleton Singletonパターン 1 外部からインスタンスを生成させない
2 インスタンスをひとつだけ生成を許す
ということを実現するデザインパターン。
 Singletonクラス  class singleton { private static $instance_ = null; private function __construct() { echo &#34;make instance \n&#34;; } public static function getInstance() { if( is_null( self::$instance_ ) ) { self::$instance_ = new singleton(); } return self::$instance_; } }   client 外部のclientからnewをしようとするとerrorになる。
インスタンスを取得する場合はクラスメソッドのgetInstance()を利用する。
 //error $instance = new singleton(); //ここはエラーになる。 //success $instance = singleton::getInstance();   利用方法  http://www.</description>
    </item>
    
    <item>
      <title>vimgrep</title>
      <link>https://yutakikuchi.github.io/post/201009060134/</link>
      <pubDate>Mon, 06 Sep 2010 01:34:47 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/post/201009060134/</guid>
      <description> [vim] : vimgrep 概要 vim7からvimにgrep機能が追加されており、いろいろな文字コードにおけるマルチバイト文字列検索なども可能に可能になっているようです。実際につかってみたところ日本語検索は可能なのですが、処理速度がどうしても気になってしまいます。vim開発者がんばれ。日本語検索をおこないたい場合はおすすめです。
vimを開いてコマンドモードで以下を実行します。
:vimgrep /partern/j **/*.php | cwin   オプション   オプション 説明   vimgrep vimgrepを実行します。   /partnern/ 正規表現を記述します。    j   検索の先頭にかかったファイルを開きません。      **   再帰的に検索をしてくれます。    cwin   別windowにファイル一覧を表示。選択でファイルを開くことが可能       </description>
    </item>
    
    <item>
      <title></title>
      <link>https://yutakikuchi.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://yutakikuchi.github.io/about/</guid>
      <description> About me  氏名 : 菊池佑太 学生時代は知能情報工学を専攻。2007年に新卒でYahoo!入社。インターネットユーザーの興味関心情報を膨大なアクセスログから導き出し、最適な広告配信に還元する仕組みを開発。部門全体として1年間で売上3倍を達成、現在の年間1000億規模の売上を作るCore技術の責任者として従事。その後独立を経てFreakOutに入社。FreakOutではDivisionManager・ProductManager・TechLead・DataScientistを兼務し、機械学習によるダイレクトレスポンス広告のClick・Conversion率の改善施策を担当。CPA指標を劇的に向上させた実績を持つ。 2017年4月からABEJAに参画し、ABEJA Platformを活用した新規ビジネスの開発に従事している。2018年3月よりABEJAの執行役員、同年4月より株式会社CA ABEJAの取締役副社長に就任。 https://lose-control.appspot.com/  </description>
    </item>
    
  </channel>
</rss>