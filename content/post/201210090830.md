
---
title: "線形予測の機械学習ツールliblinearで効果最大化のための最適な定数Cを探る"
date: 2012-10-09T08:30:06+00:00
category : [機械学習]
canonicalurl: http://yut.hatenablog.com/entry/20121009/1349739006
---

## [機械学習] : 線形予測の機械学習ツールliblinearで効果最大化のための最適な定数Cを探る

<p><div class="amazlet-box"><a href="http://www.amazon.co.jp/exec/obidos/ASIN/1449303714/yutakikuchi-22/"><img src="http://ecx.images-amazon.com/images/I/51cwV1i8S6L._SL160_.jpg" class="hatena-asin-detail-image" alt="Machine Learning for Hackers" title="Machine Learning for Hackers"></a><div class="hatena-asin-detail-info"><p class="hatena-asin-detail-title"><a href="http://www.amazon.co.jp/exec/obidos/ASIN/1449303714/yutakikuchi-22/">Machine Learning for Hackers</a></p><ul><li><span class="hatena-asin-detail-label">作者:</span> Drew Conway,John Myles White</li><li><span class="hatena-asin-detail-label">出版社/メーカー:</span> Oreilly & Associates Inc</li><li><span class="hatena-asin-detail-label">発売日:</span> 2012/02/28</li><li><span class="hatena-asin-detail-label">メディア:</span> ペーパーバック</li><li> <span class="hatena-asin-detail-label">クリック</span>: 63回</li><li><a href="http://d.hatena.ne.jp/asin/1449303714/yutakikuchi-22" target="_blank">この商品を含むブログを見る</a></li></ul></div><div class="hatena-asin-detail-foot"></div></div></p>

<div class="section">
<h4>liblinear</h4>

<blockquote>
    
<ul>
<li><a href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR -- A Library for Large Linear Classification</a> <a href="http://b.hatena.ne.jp/entry/www.csie.ntu.edu.tw/~cjlin/liblinear/"><img src="http://b.hatena.ne.jp/entry/image/http://www.csie.ntu.edu.tw/~cjlin/liblinear/" alt="はてなブックマーク - LIBLINEAR -- A Library for Large Linear Classification" border="0" /></a></li>
<li><a href="http://d.hatena.ne.jp/yutakikuchi/20120829/1346197290">10秒で設定可能なlibsvmで機械学習を行う - Yuta.Kikuchiの日記</a> <a href="http://b.hatena.ne.jp/entry/d.hatena.ne.jp/yutakikuchi/20120829/1346197290"><img src="http://b.hatena.ne.jp/entry/image/http://d.hatena.ne.jp/yutakikuchi/20120829/1346197290" alt="はてなブックマーク - 10秒で設定可能なlibsvmで機械学習を行う - Yuta.Kikuchiの日記" border="0" /></a></li>
<li><a href="http://d.hatena.ne.jp/yutakikuchi/20120827/1346024147">R言語でSVM(Support Vector Machine)による分類学習 - Yuta.Kikuchiの日記</a> <a href="http://b.hatena.ne.jp/entry/d.hatena.ne.jp/yutakikuchi/20120827/1346024147"><img src="http://b.hatena.ne.jp/entry/image/http://d.hatena.ne.jp/yutakikuchi/20120827/1346024147" alt="はてなブックマーク - R言語でSVM(Support Vector Machine)による分類学習 - Yuta.Kikuchiの日記" border="0" /></a></li>
</ul><p>今日はliblinearを用いた機会学習の話です。今までは<a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>を利用するときはkernelオプション付きの<a class="keyword" href="http://d.hatena.ne.jp/keyword/R%B8%C0%B8%EC">R言語</a>の<a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>/<a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a>/<a class="keyword" href="http://d.hatena.ne.jp/keyword/svm">svm</a>-lightを利用していましたが、学習データが多い時に計算時間が何時間も掛かる事に不便を感じていました。そこで<a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>の<a class="keyword" href="http://d.hatena.ne.jp/keyword/%A5%C4%A1%BC%A5%EB">ツール</a>について色々と調べてみたところ、線形予測に特化したliblinearの存在を知りました。公式のDocumentにも<a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a>とliblinearでの線形予測での処理時間が桁違いにliblinearの方が優れていることが記述されています。以下にliblinearの特徴を記述します。</p>

<ul>
<li>liblinearはinstanceや特徴が100万桁のデータを線形分離するためのtoolであり以下をサポートしています。
<ul>
<li>L2<a class="keyword" href="http://d.hatena.ne.jp/keyword/%C0%B5%C2%A7%B2%BD">正則化</a>の分類
<ul>
<li>L2-loss linear <a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>, L1-loss linear <a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>, and logistic regression (LR)</li>
</ul></li>
<li>L1<a class="keyword" href="http://d.hatena.ne.jp/keyword/%C0%B5%C2%A7%B2%BD">正則化</a>の分類
<ul>
<li>L2-loss linear <a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a> and logistic regression (LR)</li>
</ul></li>
<li>Support Vechtor RegressionのL2<a class="keyword" href="http://d.hatena.ne.jp/keyword/%C0%B5%C2%A7%B2%BD">正則化</a>
<ul>
<li>L2-loss linear <a class="keyword" href="http://d.hatena.ne.jp/keyword/SVR">SVR</a> and L1-loss linear <a class="keyword" href="http://d.hatena.ne.jp/keyword/SVR">SVR</a></li>
</ul></li>
</ul></li>
</ul><p><a class="keyword" href="http://d.hatena.ne.jp/keyword/%C0%B5%C2%A7%B2%BD">正則化</a>とはOverfittingを回避するために罰則項を与える事です。種類としてはL1,L2,L1L2の3つが良く利用されるもので精度とスパース性によって異なります。L1は精度が低くスパース性が高い、L2は精度が高くスパース性が低い、L1L2は両方を取り入れ精度を高く保ちながらスパース性を高くすることです。</p>

<ul>
<li>以下の特徴も含んでいます。
<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/LIBSVM">LIBSVM</a>のデータformatをサポートし、一般的な<a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a><a class="keyword" href="http://d.hatena.ne.jp/keyword/%A5%C4%A1%BC%A5%EB">ツール</a>と同じ目的で、使い方も似ています。</li>
<li>Multi-Classへの分類が可能。1 ) 2値分類、2 ) Crammer and Singer </li>
<li>モデル選択に対するCross-Validation。</li>
<li>確率推定(logistic regression限定で)が可能。</li>
<li>アンバランスなデータに対しての重み付け。</li>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/Matlab">Matlab</a>/<a class="keyword" href="http://d.hatena.ne.jp/keyword/Octave">Octave</a>、<a class="keyword" href="http://d.hatena.ne.jp/keyword/JAVA">JAVA</a>、<a class="keyword" href="http://d.hatena.ne.jp/keyword/Python">Python</a>、<a class="keyword" href="http://d.hatena.ne.jp/keyword/Ruby">Ruby</a>のinterfaceがある。</li>
</ul></li>
</ul>
</blockquote>

</div>
<div class="section">
<h4>liblinear vs <a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a> CrossValidation</h4>

<blockquote>
    <p>liblinear、<a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a>での線形予測で統一したCrossValidationの精度と実行時間を比較します。デー<a class="keyword" href="http://d.hatena.ne.jp/keyword/%A5%BF%A5%BB%A5%C3%A5%C8">タセット</a>に対して5fold-cross-validationを行います。cross-validationのデータとしてはgisetteのscaleデータを利用しました。<a href="http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html">LIBSVM Data: Classification (Binary Class)</a> <a href="http://b.hatena.ne.jp/entry/www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html"><img src="http://b.hatena.ne.jp/entry/image/http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html" alt="はてなブックマーク - LIBSVM Data: Classification (Binary Class)" border="0" /></a> 結果としてはliblinearがAccurary、処理速度ともに<a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a>を上回る結果となりました。</p>

<table>
<tr>
<th> Tool </th>
<th> Accurary </th>
<th> 処理時間 </th>
</tr>
<tr>
<td> liblinear </td>
<td> 96.2119% </td>
<td> 4.89s </td>
</tr>
<tr>
<td> <a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a> </td>
<td>   96.1417% </td>
<td> 96.22s </td>
<td> </td>
</tr>
</table>
<ul>
<li>liblinear</li>
</ul><pre class="code" data-lang="" data-unlink>$time /home/yuta/work/liblinear/liblinear-1.91/train -v 5 gisette_scale   
....*
optimization finished, #iter = 45
Objective value = -0.281585
nSV = 657
....*
optimization finished, #iter = 44
Objective value = -0.268779
nSV = 677
....*
optimization finished, #iter = 45
Objective value = -0.266992
nSV = 658
....*
optimization finished, #iter = 48
Objective value = -0.270758
nSV = 663
....*
optimization finished, #iter = 48
Objective value = -0.282720
nSV = 677
Cross Validation Accuracy = 96.2119%
/home/yuta/work/liblinear/liblinear-1.91/train -v 5 gisette_scale  4.89s user 1.68s system 48% cpu 13.452 total</pre>
<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a></li>
</ul><pre class="code" data-lang="" data-unlink>/home/yuta/work/libsvm/libsvm-3.12/svm-train -v 5 -t 0 gisette_scale           
.*.*
optimization finished, #iter = 2876
nu = 0.000235
obj = -0.268630, rho = 0.708937
nSV = 638, nBSV = 0
Total nSV = 638
.*.*
optimization finished, #iter = 2921
nu = 0.000246
obj = -0.280181, rho = 0.300795
nSV = 660, nBSV = 0
Total nSV = 660
.*..*
optimization finished, #iter = 3122
nu = 0.000249
obj = -0.283893, rho = 0.352812
nSV = 674, nBSV = 0
Total nSV = 674
.*.*
optimization finished, #iter = 2791
nu = 0.000239
obj = -0.272718, rho = 0.532717
nSV = 623, nBSV = 0
Total nSV = 623
.*.*
optimization finished, #iter = 2837
nu = 0.000236
obj = -0.268811, rho = 0.831119
nSV = 642, nBSV = 0
Total nSV = 642
Cross Validation Accuracy = 96.1417%
Positive (+1) class:
  precision = 0.960545
 recall = 0.960545
   F1 value = 0.960545

Negative (-1) class:
  precision = 0.962251
 recall = 0.962251
   F1 value = 0.962251

/home/yuta/work/libsvm/libsvm-3.12/svm-train -v 5 -t 0 gisette_scale  96.22s user 24.09s system 52% cpu 3:49.32 total</pre>
</blockquote>

</div>
<div class="section">
<h4>liblinearのPrecision、recall、<a class="keyword" href="http://d.hatena.ne.jp/keyword/F%C3%CD">F値</a>を求める</h4>

<blockquote>
    <p><a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a>でもdefaultのソースではCross-Validation時にPrecision、recall、<a class="keyword" href="http://d.hatena.ne.jp/keyword/F%C3%CD">F値</a>を求めるコードは書かれていませんが、Patchにより差分を加える事でそれらの値を求める事が可能でした。<a href="http://sleepyheads.jp/software/svm-train.patch">libsvmのCross-ValidationでPrecision、Recall、F値を求めるPatch</a> <a href="http://b.hatena.ne.jp/entry/sleepyheads.jp/software/svm-train.patch"><img src="http://b.hatena.ne.jp/entry/image/http://sleepyheads.jp/software/svm-train.patch" alt="はてなブックマーク - " border="0" /></a> 同じようにliblinearでもpatchが無いか調べたところ、親切な人が<a class="keyword" href="http://d.hatena.ne.jp/keyword/github">github</a>に上げてくれていました。<a href="https://gist.github.com/3341000">LIBLINEARのcross validationオプションでprecision/recallを出力する ― Gist</a> <a href="http://b.hatena.ne.jp/entry/s/gist.github.com/3341000"><img src="http://b.hatena.ne.jp/entry/image/https://gist.github.com/3341000" alt="はてなブックマーク - LIBLINEARのcross validationオプションでprecision/recallを出力する ― Gist" border="0" /></a>このpatchをdownloadして元のソースに当ててみます。</p>
<pre class="code" data-lang="" data-unlink>$ wget https://raw.github.com/gist/3341000/723cad7b81d28164bb7def833767b93a591cdca0/train.patch
$ patch < train.patch
$ gmake 
$ home/yuta/work/liblinear/liblinear-1.91/train -v 5 gisette_scale
....*
optimization finished, #iter = 45
Objective value = -0.281585
nSV = 657
....*
optimization finished, #iter = 44
Objective value = -0.268779
nSV = 677
....*
optimization finished, #iter = 45
Objective value = -0.266992
nSV = 658
....*
optimization finished, #iter = 48
Objective value = -0.270758
nSV = 663
....*
optimization finished, #iter = 48
Objective value = -0.282720
nSV = 677
Cross Validation Accuracy = 96.2119%
Positive (+1) class:
  precision = 0.962697
 recall = 0.959943
   F1 value = 0.961318

Negative (-1) class:
  precision = 0.961565
 recall = 0.964212
   F1 value = 0.962887</pre>
</blockquote>

</div>
<div class="section">
<h4>効果最大なCを探る</h4>

<blockquote>
    <p><a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>は多次元にPlotされた座標のマージン最大化を行う分離線を求める手法です。データが奇麗に分けられるような境界線を常に求められれば問題はありませんが、ノイズやデータのオーバーラップなどの影響を受けて正確に分離が行えない事もあります。データの誤りを許さないような方法はハードマージン、ある程度データの誤りを許すがペナルティを与えるのがソフトマージンと呼ばれるものです。ソフトマージンで利用するパラメータにはスラック変数のζ(ゼータ)と外部変数のCがあります。Cパラメータは<a class="keyword" href="http://d.hatena.ne.jp/keyword/%C0%B5%C2%A7%B2%BD">正則化</a>と同じように誤りに対するペナルティ項です。<a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>ではスラック変数のゼータと外部パラメータのCの関係はCを大きくする = ゼータを小さくする、Cを小さくする = ゼータを大きくするという調整になっています。<br />
目的関数 : <img src="http://chart.apis.google.com/chart?cht=tx&chl=%20%5Cfrac%7B1%7D%7B2%7D%20%7C%7CW%5E2%7C%7C%20%2B%20C%5CSigma_%7Bi%3D1%7D%5En%20%2B%20%5Czeta_%7Bi%7D%20" alt=" \frac{1}{2} ||W^2|| + C\Sigma_{i=1}^n + \zeta_{i} "/><br />
制約条件 : <img src="http://chart.apis.google.com/chart?cht=tx&chl=%20y_%7Bi%7D%28W%5ETx_%7Bi%7D%20%2B%20b%20%29%20%5Cgeq%201%20-%20%5Czeta_%7Bi%7D%2C%20%5Czeta_%7Bi%7D%20%5Cgeq%200%20" alt=" y_{i}(W^Tx_{i} + b ) \geq 1 - \zeta_{i}, \zeta_{i} \geq 0 "/><br />
Cはペナルティ項なのでCを大きくするのは制約を大きくすることを意味するのでCを無限に近づければハードマージン化させることを意味し、逆にCを小さくするとソフトマージン化させることになります。それで最適なCを求めるにはどうしたら良いか？これは総当たりの実験で求めていくしか手段が無いようです。以下の<a class="keyword" href="http://d.hatena.ne.jp/keyword/Python">Python</a>コードでC=0.1〜1、1〜10の5fold-cross-validationを実行してみます。結果としてはC=0.4,0.6,0.9,3,4の時にValidation Accuracy = 96.282%という最大数を得る事が分かりました。Cを掛けた時のAccuraryが線形に変化すると予測していたのですが、どうもバラバラの結果となってしまいました。</p>
<pre class="hljs python" data-lang="python" data-unlink><span class="synComment">#!/usr/bin/env python</span>
<span class="synComment"># -*- coding: utf-8 -*-</span>

<span class="synPreProc">import</span> os,sys
filename = sys.argv[<span class="synConstant">1</span>]

<span class="synStatement">for</span> i <span class="synStatement">in</span> <span class="synIdentifier">range</span>( <span class="synConstant">1</span>, <span class="synConstant">11</span> ) :
c = i
<span class="synIdentifier">print</span> <span class="synConstant">"5fold-cross-validation C="</span> + <span class="synIdentifier">str</span>( c )
os.system( <span class="synConstant">'/home/yuta/work/liblinear/liblinear-1.91/train -v 5 -c '</span> + <span class="synIdentifier">str</span>( c ) + <span class="synConstant">' '</span> + filename )
<span class="synIdentifier">print</span> <span class="synConstant">"</span><span class="synSpecial">\n</span><span class="synConstant">"</span>
</pre><pre class="code" data-lang="" data-unlink>$ python cross-validation.py /home/yuta/work/data/gisette_scale
5fold-cross-validation C=0.1 Cross Validation Accuracy = 96.2469%
5fold-cross-validation C=0.2 Cross Validation Accuracy = 96.1417%
5fold-cross-validation C=0.3 Cross Validation Accuracy = 96.1768%
5fold-cross-validation C=0.4 Cross Validation Accuracy = 96.282%
5fold-cross-validation C=0.5 Cross Validation Accuracy = 96.2469%
5fold-cross-validation C=0.6 Cross Validation Accuracy = 96.282%
5fold-cross-validation C=0.7 Cross Validation Accuracy = 96.1768%
5fold-cross-validation C=0.8 Cross Validation Accuracy = 96.2469%
5fold-cross-validation C=0.9 Cross Validation Accuracy = 96.282%
5fold-cross-validation C=1 Cross Validation Accuracy = 96.2119%
5fold-cross-validation C=2 Cross Validation Accuracy = 96.2119%
5fold-cross-validation C=3 Cross Validation Accuracy = 96.282%
5fold-cross-validation C=4 Cross Validation Accuracy = 96.282%
5fold-cross-validation C=5 Cross Validation Accuracy = 96.1768%
5fold-cross-validation C=6 Cross Validation Accuracy = 96.2119%
5fold-cross-validation C=7 Cross Validation Accuracy = 96.2119%
5fold-cross-validation C=8 Cross Validation Accuracy = 96.2119%
5fold-cross-validation C=9 Cross Validation Accuracy = 96.2119%
5fold-cross-validation C=10 Cross Validation Accuracy = 96.2119%</pre>
<div class="section">
<h4>まとめ</h4>

<blockquote>
    
<ul>
<li>liblinearは様々な<a class="keyword" href="http://d.hatena.ne.jp/keyword/%C0%B5%C2%A7%B2%BD">正則化</a>に対応している。</li>
<li>liblinearは<a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>の線形予測を行う<a class="keyword" href="http://d.hatena.ne.jp/keyword/%A5%C4%A1%BC%A5%EB">ツール</a>だが<a class="keyword" href="http://d.hatena.ne.jp/keyword/libsvm">libsvm</a>より処理速度が速い。</li>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>の線形データの分離の制限はC定数にて行う。C定数を小さくすれば条件は緩く、大きくすれば条件を厳しくすることになる。</li>
</ul>
</blockquote>

</div>
<div class="section">
<h4>Links</h4>

<blockquote>
    
<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/SVM">SVM</a>実践ガイド</li>
</ul><p><iframe src="http://www.slideshare.net/slideshow/embed_code/13435949" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC;border-width:1px 1px 0;margin-bottom:5px" allowfullscreen> </iframe> <div style="margin-bottom:5px"> <strong> <a href="http://www.slideshare.net/sleepy_yoshi/svm-13435949" title="SVM実践ガイド (A Practical Guide to Support Vector Classification)" target="_blank">SVM実践ガイド (A Practical Guide to Support Vector Classification)</a> </strong> from <strong><a href="http://www.slideshare.net/sleepy_yoshi" target="_blank">sleepy_yoshi</a></strong> </div></p>

<ul>
<li><a href="http://www.r.dl.itc.u-tokyo.ac.jp/node/48/">統計的機械学習入門 | 中川研究室</a> <a href="http://b.hatena.ne.jp/entry/www.r.dl.itc.u-tokyo.ac.jp/node/48/"><img src="http://b.hatena.ne.jp/entry/image/http://www.r.dl.itc.u-tokyo.ac.jp/node/48/" alt="はてなブックマーク - 統計的機械学習入門 | 中川研究室" border="0" /></a></li>
</ul>
</blockquote>

</div>
</blockquote>

</div>

