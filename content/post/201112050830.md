
---
title: "CentOSでHadoopを使ってみる"
date: 2011-12-05T08:30:24+00:00
category : [Hadoop]
canonicalurl: http://yut.hatenablog.com/entry/20111205/1323041424
---

## [Hadoop] : CentOSでHadoopを使ってみる

<pre class="code" data-lang="" data-unlink>            __  __          __
       / / / /___ _____/ /___  ____  ____
      / /_/ / __ ‘/ __  / __ \/ __ \/ __ \
     / __  / /_/ / /_/ / /_/ / /_/ / /_/ /
    /_/ /_/\__,_/\__,_/\____/\____/ .___/
                                 /_/</pre>
<div class="section">
<h4><span class="deco" style="font-size:large;">インストール</span></h4>

<blockquote>
    
<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>を<a class="keyword" href="http://d.hatena.ne.jp/keyword/centos">centos</a>に入れてみる。<a href="http://hadoop.apache.org/common/releases.html">最新バージョン</a>は2011/11/25日の段階では0.23.0</li>
<li>各<a class="keyword" href="http://d.hatena.ne.jp/keyword/Linux%A5%C7%A5%A3%A5%B9%A5%C8%A5%EA%A5%D3%A5%E5%A1%BC%A5%B7%A5%E7%A5%F3">Linuxディストリビューション</a>に対応済みのcdh3(Cloudera Distribution including <a class="keyword" href="http://d.hatena.ne.jp/keyword/Apache">Apache</a> <a class="keyword" href="http://d.hatena.ne.jp/keyword/Hadoop">Hadoop</a> v3)を入れる。cdh3の最新バージョンは0.20.0</li>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>の他に愉快な仲間達のhive,pig,hbaseも入れる。</li>
</ul>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/jdk">jdk</a>のインストール</h5>

<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>は<a class="keyword" href="http://d.hatena.ne.jp/keyword/java">java</a>で動くので当然必要となる。既にインストール済みの場合は不要。</li>
<li><a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html">jdkのダウンロード </a></li>
<li><a href="http://docs.oracle.com/javase/7/docs/webnotes/install/linux/linux-jdk.html">jdkのインストール </a></li>
</ul><pre class="code" data-lang="" data-unlink>$ wget http://download.oracle.com/otn-pub/java/jdk/7u1-b08/jdk-7u1-linux-x64.rpm
$ sudo rpm -ivh jdk-7u1-linux-x64.rpm
準備中...                ########################################### [100%]
   1:jdk                    ########################################### [100%]
Unpacking JAR files...
rt.jar...
jsse.jar...
charsets.jar...
tools.jar...
localedata.jar...</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/%B4%C4%B6%AD%CA%D1%BF%F4">環境変数</a>の設定</h5>

<ul>
<li>全てのユーザの<a class="keyword" href="http://d.hatena.ne.jp/keyword/%B4%C4%B6%AD%CA%D1%BF%F4">環境変数</a>として設定したいため、/etc/profileに<a class="keyword" href="http://d.hatena.ne.jp/keyword/JAVA">JAVA</a>_HOMEの設定を追加する。</li>
</ul><pre class="code" data-lang="" data-unlink>export JAVA_HOME=/usr/java/default</pre>
<ul>
<li>設定変更を反映と確認をする。</li>
</ul><pre class="code" data-lang="" data-unlink>$ source /etc/profile
$ echo $JAVA_HOME
/usr/java/default</pre>
</div>
<div class="section">
<h5>reposの登録</h5>

<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/yum">yum</a>でダウンロード/インストールするための例の奴。cloudera社の<a class="keyword" href="http://d.hatena.ne.jp/keyword/%A5%C9%A5%E1%A5%A4%A5%F3">ドメイン</a>から取得する。</li>
</ul><pre class="code" data-lang="" data-unlink>$ wget http://archive.cloudera.com/redhat/cdh/cloudera-cdh3.repo
$ sudo mv cloudera-cdh3.repo /etc/yum.repos.d/
$ sudo yum update yum
$ yum search hadoop
Loaded plugins: downloadonly, fastestmirror
cloudera-cdh3                                                                                                         81/81
===================================================== Matched: hadoop ======================================================
flume.noarch : Flume is a reliable, scalable, and manageable distributed log collection application for collecting data such
         : as logs and delivering it to data stores such as Hadoop's HDFS.
flume-master.noarch : The flume master daemon is the central administration and data path control point for flume nodes.
flume-node.noarch : The flume node daemon is a core element of flume's data path and is responsible for generating,
              : processing, and delivering data.
hadoop-0.20.noarch : Hadoop is a software platform for processing vast amounts of data
hadoop-0.20-conf-pseudo.noarch : Hadoop installation in pseudo-distributed mode
hadoop-0.20-datanode.noarch : Hadoop Data Node
hadoop-0.20-debuginfo.i386 : Debug information for package hadoop-0.20
hadoop-0.20-debuginfo.x86_64 : Debug information for package hadoop-0.20
hadoop-0.20-doc.noarch : Hadoop Documentation
hadoop-0.20-fuse.i386 : Mountable HDFS
hadoop-0.20-fuse.x86_64 : Mountable HDFS
hadoop-0.20-jobtracker.noarch : Hadoop Job Tracker
hadoop-0.20-libhdfs.i386 : Hadoop Filesystem Library
hadoop-0.20-libhdfs.x86_64 : Hadoop Filesystem Library
hadoop-0.20-namenode.noarch : The Hadoop namenode manages the block locations of HDFS files
hadoop-0.20-native.i386 : Native libraries for Hadoop Compression
hadoop-0.20-native.x86_64 : Native libraries for Hadoop Compression
hadoop-0.20-pipes.i386 : Hadoop Pipes Library
hadoop-0.20-pipes.x86_64 : Hadoop Pipes Library
hadoop-0.20-sbin.i386 : Binaries for secured Hadoop clusters
hadoop-0.20-sbin.x86_64 : Binaries for secured Hadoop clusters
hadoop-0.20-secondarynamenode.noarch : Hadoop Secondary namenode
hadoop-0.20-source.noarch : Source code for Hadoop
hadoop-0.20-tasktracker.noarch : Hadoop Task Tracker
hadoop-hbase.noarch : HBase is the Hadoop database. Use it when you need random, realtime read/write access to your Big
                : Data. This project's goal is the hosting of very large tables -- billions of rows X millions of
                : columns -- atop clusters of commodity hardware.
hadoop-hbase-doc.noarch : Hbase Documentation
hadoop-hbase-master.noarch : The Hadoop HBase master Server.
hadoop-hbase-regionserver.noarch : The Hadoop HBase RegionServer server.
hadoop-hbase-thrift.noarch : The Hadoop HBase Thrift Interface
hadoop-hive.noarch : Hive is a data warehouse infrastructure built on top of Hadoop
hadoop-hive-metastore.noarch : Shared metadata repository for Hive.
hadoop-hive-server.noarch : Provides a Hive Thrift service.
hadoop-pig.noarch : Pig is a platform for analyzing large data sets
hadoop-zookeeper.noarch : A high-performance coordination service for distributed applications.
hadoop-zookeeper-server.noarch : The Hadoop Zookeeper server
hue.noarch : The hue metapackage
hue-common.i386 : A browser-based desktop interface for Hadoop
hue-common.x86_64 : A browser-based desktop interface for Hadoop
hue-filebrowser.noarch : A UI for the Hadoop Distributed File System (HDFS)
hue-jobbrowser.noarch : A UI for viewing Hadoop map-reduce jobs
hue-jobsub.noarch : A UI for designing and submitting map-reduce jobs to Hadoop
hue-plugins.noarch : Hadoop plugins for Hue
hue-shell.i386 : A shell for console based Hadoop applications
hue-shell.x86_64 : A shell for console based Hadoop applications
mahout.noarch : A set of Java libraries for scalable machine learning.
oozie.noarch : Oozie is a system that runs workflows of Hadoop jobs.
sqoop.noarch : Sqoop allows easy imports and exports of data sets between databases and the Hadoop Distributed File System
         : (HDFS).</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/yum">yum</a> install</h5>

<ul>
<li>とりえあず本体だけ入れる。</li>
</ul><pre class="code" data-lang="" data-unlink>$ su
$ yum install hadoop-0.20 -y</pre>
<ul>
<li>1台で動かす設定を入れる。</li>
</ul><pre class="code" data-lang="" data-unlink>$ yum install hadoop-0.20-conf-pseudo -y</pre>
<ul>
<li>上のコマンドを実行すると依存パッケージが入る。これらはデーモンプロセスを起動するために必要。
<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>-0.20-datanode</li>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>-0.20-jobtracker</li>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>-0.20-namenode</li>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>-0.20-secondarynamenode</li>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>-0.20-tasktracker</li>
</ul></li>
<li>hive,pig,hbaseを入れる。</li>
</ul><pre class="code" data-lang="" data-unlink>$ yum install hadoop-hive -y
$ yum install hadoop-pig -y
$ yum install hadoop-hbase -y</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>の起動</h5>

<ul>
<li>依存して入ったパッケージによるデーモンプロセスを起動。</li>
</ul><pre class="code" data-lang="" data-unlink>$ /etc/init.d/hadoop-0.20-datanode start
$ /etc/init.d/hadoop-0.20-namenode start
$ /etc/init.d/hadoop-0.20-tasktracker start
$ /etc/init.d/hadoop-0.20-jobtracker start
$ /etc/init.d/hadoop-0.20-secondarynamenode start #これは最初は不要</pre>
</div>
</blockquote>

</div>
<div class="section">
<h4><span class="deco" style="font-size:large;">HDF(<a class="keyword" href="http://d.hatena.ne.jp/keyword/Hadoop">Hadoop</a> Distributed File System)</span></h4>

<blockquote>
    
<ul>
<li><span class="deco" style="color:#FF0000;"><a class="keyword" href="http://d.hatena.ne.jp/keyword/HDFS">HDFS</a>を一言で表すなら<a class="keyword" href="http://d.hatena.ne.jp/keyword/Hadoop">Hadoop</a>上のファイルを効率よく安全に管理するための<a class="keyword" href="http://d.hatena.ne.jp/keyword/%A5%D5%A5%A1%A5%A4%A5%EB%A5%B7%A5%B9%A5%C6%A5%E0">ファイルシステム</a></span>。</li>
<li>Map/Reduceの仕組みを利用し、データの計算を分散サーバ上で行い、結果をネットワークを介して取得する。</li>
<li>NameNodeとDataNodeの２週類サーバで構成。
<ul>
<li>NameNodeサーバにファイルのメタ情報(どのサーバのどこのディレクトリに何のファイルのアクセス権で設定されているかなど)を格納。</li>
<li>DataNodeサーバにデータファイルを格納。(ファイルはブロックという特定サイズで分割し格納。)</li>
</ul></li>
<li>設定ファイルのディレクトリ:/etc/<a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>-0.20/conf.pseudo/</li>
</ul>
<div class="section">
<h5>alias設定</h5>

<ul>
<li><a class="keyword" href="http://d.hatena.ne.jp/keyword/hdfs">hdfs</a>のコマンドは長くなってしまうので、以下の1行を$HOME/.bashrc等にaliasとして張っておくと便利。</li>
</ul><pre class="code" data-lang="" data-unlink>alias hdfs='/usr/bin/hadoop dfs'</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/hdfs">hdfs</a>上にフォルダを作成</h5>
<pre class="code" data-lang="" data-unlink>$ hdfs -mkdir HDFS_TEST01
$ hdfs -ls
hdfs -ls
Found 1 items
drwxr-xr-x   - yuta supergroup          0 2011-12-04 22:36 /user/yuta/HDFS_TEST01</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/hdfs">hdfs</a>にファイルをコピー</h5>

<ul>
<li>ローカルで作成したファイルを<a class="keyword" href="http://d.hatena.ne.jp/keyword/hdfs">hdfs</a>上に配る。</li>
</ul><pre class="code" data-lang="" data-unlink>$ cat hdfs_input/input 
haddop1
haddop2
haddop3
haddop4
haddop5
$ hdfs -put hdfs_input/input HDFS_TEST01</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/hdfs">hdfs</a>上のファイルを確認</h5>
<pre class="code" data-lang="" data-unlink>$ hdfs -cat HDFS_TEST01/input
hadoop1
hadoop2
hadoop3
hadoop4
hadoop5</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/hdfs">hdfs</a>上のファイルを取得</h5>
<pre class="code" data-lang="" data-unlink>$ hdfs -get HDFS_TEST01/input output 
$ cat output/input
hadoop1
hadoop2
hadoop3
hadoop4
hadoop5</pre>
</div>
</blockquote>

</div>
<div class="section">
<h4><span class="deco" style="font-size:large;">Map/Reduce</span></h4>

<blockquote>
    
<ul>
<li>Map/Reduceは正確に言うとMap/Shuffle/Reduce処理に分けられる。Shuffleは内部的に自動で行われる。
<ul>
<li>MapはHashを作成することをイメージする。例えば英語テキスト中の単語のカウントをする場合、単語を区切り、各単語に1という数値を割り当てる。その際に{key,value}というペアでデータを持つ。</li>
<li>Shuffleはkey順でのsortと重複しないようにユニークなkeyに対してvalueを割り当てる。shuffleの段階ではkeyの重複を削るだけ。まだReduce処理はしない。</li>
<li>Reduceでkeyに対するvalueを整形する。例えば単語カウントの場合、shuffleされた各value値を加算する。</li>
</ul></li>
</ul>
<div class="section">
<h5>円周率を計算してみる</h5>

<ul>
<li>サンプルとして存在するMap/Reduceのpi計算を行う。5というのがMapの数。</li>
</ul><pre class="code" data-lang="" data-unlink>$ hadoop jar /usr/lib/hadoop-0.20/hadoop-examples.jar pi 5 2000
Number of Maps  = 5
Samples per Map = 2000
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Wrote input for Map #4
Starting Job
11/12/04 23:08:40 INFO mapred.FileInputFormat: Total input paths to process : 5
11/12/04 23:08:41 INFO mapred.JobClient: Running job: job_201112042137_0001
11/12/04 23:08:42 INFO mapred.JobClient:  map 0% reduce 0%
11/12/04 23:09:00 INFO mapred.JobClient:  map 20% reduce 0%
11/12/04 23:09:01 INFO mapred.JobClient:  map 40% reduce 0%
11/12/04 23:09:14 INFO mapred.JobClient:  map 60% reduce 0%
11/12/04 23:09:17 INFO mapred.JobClient:  map 80% reduce 0%
11/12/04 23:09:23 INFO mapred.JobClient:  map 100% reduce 0%
11/12/04 23:09:45 INFO mapred.JobClient:  map 100% reduce 100%
11/12/04 23:09:51 INFO mapred.JobClient: Job complete: job_201112042137_0001
11/12/04 23:09:51 INFO mapred.JobClient: Counters: 23
11/12/04 23:09:51 INFO mapred.JobClient:   Job Counters 
11/12/04 23:09:51 INFO mapred.JobClient:     Launched reduce tasks=1
11/12/04 23:09:51 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=67682
11/12/04 23:09:51 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
11/12/04 23:09:51 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
11/12/04 23:09:51 INFO mapred.JobClient:     Launched map tasks=5
11/12/04 23:09:51 INFO mapred.JobClient:     Data-local map tasks=5
11/12/04 23:09:51 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=46591
11/12/04 23:09:51 INFO mapred.JobClient:   FileSystemCounters
11/12/04 23:09:51 INFO mapred.JobClient:     FILE_BYTES_READ=116
11/12/04 23:09:51 INFO mapred.JobClient:     HDFS_BYTES_READ=1170
11/12/04 23:09:51 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=324508
11/12/04 23:09:51 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=215
11/12/04 23:09:51 INFO mapred.JobClient:   Map-Reduce Framework
11/12/04 23:09:51 INFO mapred.JobClient:     Reduce input groups=2
11/12/04 23:09:51 INFO mapred.JobClient:     Combine output records=0
11/12/04 23:09:51 INFO mapred.JobClient:     Map input records=5
11/12/04 23:09:51 INFO mapred.JobClient:     Reduce shuffle bytes=140
11/12/04 23:09:51 INFO mapred.JobClient:     Reduce output records=0
11/12/04 23:09:51 INFO mapred.JobClient:     Spilled Records=20
11/12/04 23:09:51 INFO mapred.JobClient:     Map output bytes=90
11/12/04 23:09:51 INFO mapred.JobClient:     Map input bytes=120
11/12/04 23:09:51 INFO mapred.JobClient:     Combine input records=0
11/12/04 23:09:51 INFO mapred.JobClient:     Map output records=10
11/12/04 23:09:51 INFO mapred.JobClient:     SPLIT_RAW_BYTES=580
11/12/04 23:09:51 INFO mapred.JobClient:     Reduce input records=10
Job Finished in 72.311 seconds
Estimated value of Pi is 3.14080000000000000000</pre><p>72.311sも掛かりながらこの精度ですか....</p>

</div>
</blockquote>

</div>
<div class="section">
<h4><span class="deco" style="font-size:large;">Map/Reduceを自分で書く</span></h4>

<blockquote>
    
<ul>
<li>以下では<a class="keyword" href="http://d.hatena.ne.jp/keyword/Hadoop">Hadoop</a>拡張のHadoopStreamingを用いて標準入出力を介するプログラム処理を記述する。</li>
<li>単語の出現回数を計算する処理を自分で書く。言語は<a class="keyword" href="http://d.hatena.ne.jp/keyword/Python">Python</a>を利用する。</li>
<li>map.py reduce.pyの2種類を用意する</li>
</ul>
<div class="section">
<h5>map.py/reduce.pyの作成</h5>

<ul>
<li>map.py</li>
</ul><pre class="hljs python" data-lang="python" data-unlink><span class="synComment">#!/usr/bin/env python</span>
 
<span class="synPreProc">import</span> sys
 
<span class="synStatement">for</span> line <span class="synStatement">in</span> sys.stdin:
line = line.strip()
words = line.split()
<span class="synStatement">for</span> word <span class="synStatement">in</span> words:
        <span class="synIdentifier">print</span> <span class="synConstant">'%s</span><span class="synSpecial">\t</span><span class="synConstant">%s'</span> % (word, <span class="synConstant">1</span>)
</pre>
<ul>
<li>reduce.py</li>
</ul><pre class="hljs python" data-lang="python" data-unlink><span class="synComment">#!/usr/bin/env python</span>

<span class="synPreProc">from</span> operator <span class="synPreProc">import</span> itemgetter
<span class="synPreProc">import</span> sys
 
word2count = {}
<span class="synStatement">for</span> line <span class="synStatement">in</span> sys.stdin:
line = line.strip()
word, count = line.split(<span class="synConstant">'</span><span class="synSpecial">\t</span><span class="synConstant">'</span>, <span class="synConstant">1</span>)
<span class="synStatement">try</span>:
    count = <span class="synIdentifier">int</span>(count)
    word2count[word] = word2count.get(word, <span class="synConstant">0</span>) + count
<span class="synStatement">except</span> <span class="synType">ValueError</span>:
    <span class="synStatement">pass</span>
 
sorted_word2count = <span class="synIdentifier">sorted</span>(word2count.items(), key=itemgetter(<span class="synConstant">0</span>))
<span class="synStatement">for</span> word, count <span class="synStatement">in</span> sorted_word2count:
<span class="synIdentifier">print</span> <span class="synConstant">'%s</span><span class="synSpecial">\t</span><span class="synConstant">%s'</span>% (word, count)
</pre>
<ul>
<li>map.pyのテスト。keyに対するvalueが設定される。</li>
</ul><pre class="code" data-lang="" data-unlink>$ echo 'c c++ java python perl php javascript java python' | /home/yuta/work/dev/hadoop/map_reduce/map.py      
c	1
c++	1
java	1
python	1
perl	1
php	1
javascript	1
java	1
python	1</pre>
<ul>
<li>reduce.pyのテスト。keyに対してvalueの値が束ねられている。</li>
</ul><pre class="code" data-lang="" data-unlink>$ echo 'c c++ java python perl php javascript java python' | /home/yuta/work/dev/hadoop/map_reduce/map.py | sort | /home/yuta/work/dev/hadoop/map_reduce/reduce.py
c	1
c++	1
java	2
javascript	1
perl	1
php	1
python	2</pre>
</div>
<div class="section">
<h5>テキストデータを<a class="keyword" href="http://d.hatena.ne.jp/keyword/HDFS">HDFS</a>上にコピー</h5>

<ul>
<li>グーテンバーグ上のテキストをコピーして実行。
<ul>
<li><a href="http://www.gutenberg.org/cache/epub/20417/pg20417.txt">http://www.gutenberg.org/cache/epub/20417/pg20417.txt</a></li>
<li><a href="http://www.gutenberg.org/cache/epub/5000/pg5000.txt">http://www.gutenberg.org/cache/epub/5000/pg5000.txt</a></li>
<li><a href="http://www.gutenberg.org/cache/epub/4300/pg4300.txt">http://www.gutenberg.org/cache/epub/4300/pg4300.txt</a></li>
</ul></li>
</ul><pre class="code" data-lang="" data-unlink>$ mkdir gutenberg
$ cd gutenberg
$ wget http://www.gutenberg.org/cache/epub/20417/pg20417.txt
$ wget http://www.gutenberg.org/cache/epub/5000/pg5000.txt
$ wget http://www.gutenberg.org/cache/epub/4300/pg4300.txt
$ cd ..
$ hdfs -copyFromLocal gutenberg gutenberg
$ hdfs -ls gutenberg
Found 3 items
-rw-r--r--   1 yuta supergroup     674566 2011-12-04 23:56 /user/yuta/gutenberg/pg20417.txt
-rw-r--r--   1 yuta supergroup    1573150 2011-12-04 23:56 /user/yuta/gutenberg/pg4300.txt
-rw-r--r--   1 yuta supergroup    1423801 2011-12-04 23:56 /user/yuta/gutenberg/pg5000.txt</pre>
</div>
<div class="section">
<h5><a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>でmap.py/reduce.pyを実行</h5>
<pre class="code" data-lang="" data-unlink>$ hadoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2-cdh3u2.jar -mapper /home/yuta/work/dev/hadoop/map_reduce/map.py -reducer /home/yuta/work/dev/hadoop/map_reduce/reduce.py -input gutenberg/* -output gutenberg-output
packageJobJar: [/var/lib/hadoop-0.20/cache/yuta/hadoop-unjar3531972439212126610/] [] /tmp/streamjob4995151306966509636.jar tmpDir=null
11/12/04 23:57:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11/12/04 23:57:14 WARN snappy.LoadSnappy: Snappy native library not loaded
11/12/04 23:57:14 INFO mapred.FileInputFormat: Total input paths to process : 3
11/12/04 23:57:15 INFO streaming.StreamJob: getLocalDirs(): [/var/lib/hadoop-0.20/cache/yuta/mapred/local]
11/12/04 23:57:15 INFO streaming.StreamJob: Running job: job_201112042137_0003
11/12/04 23:57:15 INFO streaming.StreamJob: To kill this job, run:
11/12/04 23:57:15 INFO streaming.StreamJob: /usr/lib/hadoop-0.20/bin/hadoop job  -Dmapred.job.tracker=localhost:8021 -kill job_201112042137_0003
11/12/04 23:57:15 INFO streaming.StreamJob: Tracking URL: http://localhost.localdomain:50030/jobdetails.jsp?jobid=job_201112042137_0003
11/12/04 23:57:16 INFO streaming.StreamJob:  map 0%  reduce 0%
11/12/04 23:58:20 INFO streaming.StreamJob:  map 100%  reduce 100%
11/12/04 23:58:20 INFO streaming.StreamJob: To kill this job, run:
11/12/04 23:58:20 INFO streaming.StreamJob: /usr/lib/hadoop-0.20/bin/hadoop job  -Dmapred.job.tracker=localhost:8021 -kill job_201112042137_0003
11/12/04 23:58:20 INFO streaming.StreamJob: Tracking URL: http://localhost.localdomain:50030/jobdetails.jsp?jobid=job_201112042137_0003
11/12/04 23:58:20 ERROR streaming.StreamJob: Job not successful. Error: NA
11/12/04 23:58:20 INFO streaming.StreamJob: killJob...
Streaming Command Failed!</pre><p>処理に失敗している.... 調べてみたところfileオプションを付ける必要があるみたい。</p>

<ul>
<li>fileオプションを付けて再チャレンジ</li>
</ul><pre class="code" data-lang="" data-unlink>$ hdoop jar /usr/lib/hadoop-0.20/contrib/streaming/hadoop-streaming-0.20.2-cdh3u2.jar -mapper /home/yuta/work/dev/hadoop/map_reduce/map.py -reducer /home/yuta/work/dev/hadoop/map_reduce/reduce.py -input gutenberg/* -output gutenberg-output -file /home/yuta/work/dev/hadoop/map_reduce/map.py -file /home/yuta/work/dev/hadoop/map_reduce/reduce.py
packageJobJar: [/home/yuta/work/dev/hadoop/map_reduce/map.py, /home/yuta/work/dev/hadoop/map_reduce/reduce.py, /var/lib/hadoop-0.20/cache/yuta/hadoop-unjar8731079524882793743/] [] /tmp/streamjob400379298028508245.jar tmpDir=null
11/12/05 00:26:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
11/12/05 00:26:29 WARN snappy.LoadSnappy: Snappy native library not loaded
11/12/05 00:26:29 INFO mapred.FileInputFormat: Total input paths to process : 3
11/12/05 00:26:29 INFO streaming.StreamJob: getLocalDirs(): [/var/lib/hadoop-0.20/cache/yuta/mapred/local]
11/12/05 00:26:29 INFO streaming.StreamJob: Running job: job_201112042137_0007
11/12/05 00:26:29 INFO streaming.StreamJob: To kill this job, run:
11/12/05 00:26:29 INFO streaming.StreamJob: /usr/lib/hadoop-0.20/bin/hadoop job  -Dmapred.job.tracker=localhost:8021 -kill job_201112042137_0007
11/12/05 00:26:29 INFO streaming.StreamJob: Tracking URL: http://localhost.localdomain:50030/jobdetails.jsp?jobid=job_201112042137_0007
11/12/05 00:26:31 INFO streaming.StreamJob:  map 0%  reduce 0%
11/12/05 00:26:47 INFO streaming.StreamJob:  map 33%  reduce 0%
11/12/05 00:26:51 INFO streaming.StreamJob:  map 67%  reduce 0%
11/12/05 00:27:02 INFO streaming.StreamJob:  map 100%  reduce 0%
11/12/05 00:27:11 INFO streaming.StreamJob:  map 100%  reduce 100%
11/12/05 00:27:18 INFO streaming.StreamJob: Job complete: job_201112042137_0007
11/12/05 00:27:18 INFO streaming.StreamJob: Output: gutenberg-output</pre><p>Warningが出ているけれども取りあえずは成功。</p>

<ul>
<li>outputファイルを確認する。</li>
</ul><pre class="code" data-lang="" data-unlink>$ hdfs -cat gutenberg-output/part-00000  | head -n 20
"(Lo)cra"	1
"1490	1
"1498,"	1
"35"	1
"40,"	1
"A	2
"AS-IS".	1
"A_	1
"Absoluti	1
"Alack!	1
"Alack!"	1
"Alla	1
"Allegorical	1
"Alpha	1
"Alpha,"	1
"Alpine-glow"	1
"An	2
"And	3
"Antoni	1
"At	1</pre><p>Map/Reduceが成功して、wordの出現回数が記録されている。</p>

</div>
<div class="section">
<h5>Map/Reduceを最適化する</h5>

<ul>
<li>map.py</li>
</ul><pre class="hljs python" data-lang="python" data-unlink><span class="synComment">#!/usr/bin/env python</span>

<span class="synPreProc">import</span> sys
 
<span class="synStatement">def</span> <span class="synIdentifier">read_input</span>(<span class="synIdentifier">file</span>):
<span class="synStatement">for</span> line <span class="synStatement">in</span> <span class="synIdentifier">file</span>:
    <span class="synStatement">yield</span> line.split()
 
<span class="synStatement">def</span> <span class="synIdentifier">main</span>(separator=<span class="synConstant">'</span><span class="synSpecial">\t</span><span class="synConstant">'</span>):
data = read_input(sys.stdin)
<span class="synStatement">for</span> words <span class="synStatement">in</span> data:
    <span class="synStatement">for</span> word <span class="synStatement">in</span> words:
        <span class="synIdentifier">print</span> <span class="synConstant">'%s%s%d'</span> % (word, separator, <span class="synConstant">1</span>)
 
<span class="synStatement">if</span> __name__ == <span class="synConstant">"__main__"</span>:
main()
</pre>
<ul>
<li>reduce.py</li>
</ul><pre class="hljs python" data-lang="python" data-unlink><span class="synComment">#!/usr/bin/env python</span>

<span class="synPreProc">from</span> itertools <span class="synPreProc">import</span> groupby
<span class="synPreProc">from</span> operator <span class="synPreProc">import</span> itemgetter
<span class="synPreProc">import</span> sys
 
<span class="synStatement">def</span> <span class="synIdentifier">read_mapper_output</span>(<span class="synIdentifier">file</span>, separator=<span class="synConstant">'</span><span class="synSpecial">\t</span><span class="synConstant">'</span>):
<span class="synStatement">for</span> line <span class="synStatement">in</span> <span class="synIdentifier">file</span>:
    <span class="synStatement">yield</span> line.rstrip().split(separator, <span class="synConstant">1</span>)
 
<span class="synStatement">def</span> <span class="synIdentifier">main</span>(separator=<span class="synConstant">'</span><span class="synSpecial">\t</span><span class="synConstant">'</span>):
data = read_mapper_output(sys.stdin, separator=separator)
<span class="synStatement">for</span> current_word, group <span class="synStatement">in</span> groupby(data, itemgetter(<span class="synConstant">0</span>)):
    <span class="synStatement">try</span>:
        total_count = <span class="synIdentifier">sum</span>(<span class="synIdentifier">int</span>(count) <span class="synStatement">for</span> current_word, count <span class="synStatement">in</span> group)
        <span class="synIdentifier">print</span> <span class="synConstant">"%s%s%d"</span> % (current_word, separator, total_count)
    <span class="synStatement">except</span> <span class="synType">ValueError</span>:
        <span class="synComment"># count was not a number, so silently discard this item</span>
        <span class="synStatement">pass</span>
 
<span class="synStatement">if</span> __name__ == <span class="synConstant">"__main__"</span>:
main()
</pre>
</div>
</blockquote>

</div>
<div class="section">
<h4><a class="keyword" href="http://d.hatena.ne.jp/keyword/Hadoop">Hadoop</a>のエラー</h4>

<blockquote>
    
<div class="section">
<h5>Name node is in safe mode.Resources are low on NN. Safe mode must be turned off manually.</h5>
<p>何かしらのエラーで<a class="keyword" href="http://d.hatena.ne.jp/keyword/Hadoop">Hadoop</a>がSafe Modeに切り替わってしまっているみたい。以下のコマンドでSafe Modeが解除できる。</p>
<pre class="code" data-lang="" data-unlink>$ hadoop dfsadmin -safemode leave
Safe mode is OFF
$ hadoop dfsadmin -safemode get
Safe mode is OFF</pre>
</div>
<div class="section">
<h5>WARN <a class="keyword" href="http://d.hatena.ne.jp/keyword/hdfs">hdfs</a>.DFSClient: DataStreamer Exception: org.<a class="keyword" href="http://d.hatena.ne.jp/keyword/apache">apache</a>.<a class="keyword" href="http://d.hatena.ne.jp/keyword/hadoop">hadoop</a>.ipc.RemoteException: <a class="keyword" href="http://d.hatena.ne.jp/keyword/java">java</a>.io.IOException:could only be replicated to 0 nodes, instead of 1</h5>
<p>このエラーは<a href="http://localhost:50070/">http://localhost:50070/</a>にアクセスすると直る。</p>

</div>
</blockquote>

</div>
<div class="section">
<h4><span class="deco" style="font-size:large;">今後学習しないといけないこと </span></h4>

<blockquote>
    <p>時間がある時に次のことをまとめようと思う。</p>

<ul>
<li>分散<a class="keyword" href="http://d.hatena.ne.jp/keyword/grep">grep</a> </li>
<li>分散sort</li>
<li>分散キャッシュ</li>
</ul>
</blockquote>

</div>
<div class="section">
<h4><span class="deco" style="font-size:large;">リンク</span></h4>

<blockquote>
    
<ul>
<li><a href="http://blog.livedoor.jp/sasata299/archives/51461548.html">CentOS に Hadoop, Pig, Hive, HBase をインストール</a></li>
<li><a href="http://ja.wikipedia.org/wiki/Hadoop">Hadoop - Wikipedia</a></li>
<li><a href="http://oss.infoscience.co.jp/hadoop/">Welcome to Apache Hadoop!</a></li>
<li><a href="http://d.hatena.ne.jp/shiumachi/20100425/1272197037">Hadoopリンクまとめ(1)</a></li>
<li><a href="http://www.atmarkit.co.jp/fjava/rensai4/hadoop_tm01/01.html">いまさら聞けないHadoopとテキストマイニング入門</a></li>
<li><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">Writing An Hadoop MapReduce Program In Python</a></li>
</ul>
</blockquote>

</div>

